{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:38.619584Z",
     "start_time": "2019-08-19T17:09:37.539914Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:38.764093Z",
     "start_time": "2019-08-19T17:09:38.715052Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                          missing_keys, unexpected_keys, error_msgs):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
    "    this module, but not its descendants. This is called on every submodule\n",
    "    in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
    "    module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
    "    For state dicts without metadata, :attr:`local_metadata` is empty.\n",
    "    Subclasses can achieve class-specific backward compatible loading using\n",
    "    the version number at `local_metadata.get(\"version\", None)`.\n",
    "\n",
    "    .. note::\n",
    "        :attr:`state_dict` is not the same object as the input\n",
    "        :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
    "        it can be modified.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        prefix (str): the prefix for parameters and buffers used in this\n",
    "            module\n",
    "        local_metadata (dict): a dict containing the metadata for this module.\n",
    "            See\n",
    "        strict (bool): whether to strictly enforce that the keys in\n",
    "            :attr:`state_dict` with :attr:`prefix` match the names of\n",
    "            parameters and buffers in this module\n",
    "        missing_keys (list of str): if ``strict=True``, add missing keys to\n",
    "            this list\n",
    "        unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
    "            keys to this list\n",
    "        error_msgs (list of str): error messages should be added to this\n",
    "            list, and will be reported together in\n",
    "            :meth:`~torch.nn.Module.load_state_dict`\n",
    "    \"\"\"\n",
    "    for hook in self._load_state_dict_pre_hooks.values():\n",
    "        hook(state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs)\n",
    "\n",
    "    local_name_params = itertools.chain(self._parameters.items(),\n",
    "                                        self._buffers.items())\n",
    "    local_state = {k: v.data for k, v in local_name_params if v is not None}\n",
    "\n",
    "    for name, param in local_state.items():\n",
    "        key = prefix + name\n",
    "        if key in state_dict:\n",
    "            input_param = state_dict[key]\n",
    "\n",
    "            # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
    "            if len(param.shape) == 0 and len(input_param.shape) == 1:\n",
    "                input_param = input_param[0]\n",
    "\n",
    "            if input_param.shape != param.shape:\n",
    "                # local shape should match the one in checkpoint\n",
    "                error_msgs.append(\n",
    "                    'size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                    'the shape in current model is {}.'.format(\n",
    "                        key, input_param.shape, param.shape))\n",
    "                #if not strict:\n",
    "                #    continue\n",
    "\n",
    "            if isinstance(input_param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                input_param = input_param.data\n",
    "\n",
    "            try:\n",
    "                if False:\n",
    "                    input_param = input_param[torch.randperm(input_param.size()[0])]\n",
    "                param.copy_(input_param)\n",
    "            except Exception:\n",
    "                error_msgs.append(\n",
    "                    'While copying the parameter named \"{}\", '\n",
    "                    'whose dimensions in the model are {} and '\n",
    "                    'whose dimensions in the checkpoint are {}.'.format(\n",
    "                        key, param.size(), input_param.size()))\n",
    "                # PG load partially\n",
    "\n",
    "                if len(input_param.size()) == 3:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(\n",
    "                            key, param.size(), input_param.size(),\n",
    "                            param[:input_param.size()[0], :input_param.size(\n",
    "                            )[1], :input_param.size()[2]].shape))\n",
    "                else:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(key, param.size(), input_param.size(),\n",
    "                                param[:input_param.size()[0]].shape))\n",
    "\n",
    "                try:\n",
    "                    new_input_param = torch.empty_like(param)\n",
    "                    new_input_param = torch.nn.init.normal_(new_input_param,\n",
    "                                                            mean=input_param.mean(),\n",
    "                                                            std=input_param.std())\n",
    "\n",
    "                    if len(input_param.size()) == 3:\n",
    "                        new_input_param[:input_param.size()[0], :input_param.\n",
    "                                        size()[1], :input_param.size(\n",
    "                                        )[2]] = input_param\n",
    "                    else:\n",
    "                        new_input_param[:input_param.size()[0]] = input_param\n",
    "                    param.copy_(new_input_param)\n",
    "                except Exception as e:\n",
    "                    assert e\n",
    "                    error_msgs.append(\n",
    "                        'Failed to load weights partially {}'.format(e))\n",
    "        elif strict:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    if strict:\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(prefix):\n",
    "                input_name = key[len(prefix):]\n",
    "                input_name = input_name.split(\n",
    "                    '.', 1)[0]  # get the name of param/buffer/child\n",
    "                if input_name not in self._modules and input_name not in local_state:\n",
    "                    unexpected_keys.append(key)\n",
    "\n",
    "def load_state_dict(self, state_dict, strict=True):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
    "    this module and its descendants. If :attr:`strict` is ``True``, then\n",
    "    the keys of :attr:`state_dict` must exactly match the keys returned\n",
    "    by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        strict (bool, optional): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "\n",
    "    Returns:\n",
    "        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
    "            * **missing_keys** is a list of str containing the missing keys\n",
    "            * **unexpected_keys** is a list of str containing the unexpected keys\n",
    "    \"\"\"\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "                \n",
    "    load(self)\n",
    "\n",
    "    if strict:\n",
    "        if len(unexpected_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Unexpected key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in unexpected_keys)))\n",
    "        if len(missing_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Missing key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in missing_keys)))\n",
    "\n",
    "    if strict and len(error_msgs) > 0:\n",
    "        raise RuntimeError(\n",
    "            'Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:39.233169Z",
     "start_time": "2019-08-19T17:09:39.227819Z"
    }
   },
   "outputs": [],
   "source": [
    "nn.Module._load_from_state_dict = _load_from_state_dict\n",
    "nn.Module.load_state_dict = load_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:41.060744Z",
     "start_time": "2019-08-19T17:09:39.851184Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.distributed import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:41.101982Z",
     "start_time": "2019-08-19T17:09:41.063665Z"
    }
   },
   "outputs": [],
   "source": [
    "fname_ext = lambda fname,ext: f'{str(fname)[:-4]}{ext}{str(fname)[-4:]}'\n",
    "\n",
    "def preprocess(fname, type_index=None, ext=''):\n",
    "    t  = pd.read_csv(fname_ext(fname,ext))\n",
    "    s  = pd.read_csv('structures.csv')\n",
    "    \n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "\n",
    "    if has_y:\n",
    "        # atom-atom level\n",
    "        # molecule_name,atom_index_0,atom_index_1,type,fc,sd,pso,dso\n",
    "        scalar_couplings = pd.read_csv(f'scalar_coupling_contributions{ext}.csv') # fc,sd,pso,dso\n",
    "\n",
    "        # atom level\n",
    "        # molecule_name,atom_index,XX,YX,ZX,XY,YY,ZY,XZ,YZ,ZZ\n",
    "        magnetic_shielding = pd.read_csv('magnetic_shielding_tensors.csv')\n",
    "        # molecule_name,atom_index,mulliken_charge\n",
    "        mulliken_charges = pd.read_csv('mulliken_charges.csv')\n",
    "\n",
    "        # molecule level\n",
    "        # molecule_name,X,Y,Z\n",
    "        dipole_moments = pd.read_csv('dipole_moments.csv')\n",
    "        # molecule_name,potential_energy\n",
    "        potential_energy = pd.read_csv('potential_energy.csv')\n",
    "\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_idx'] = t['type'].apply(lambda x: type_index.index(x) ) # pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_idx'] = pd.factorize(t['type'])[0]\n",
    "\n",
    "    s['atom_idx'] = s['atom'].apply(lambda x: atoms.index(x) )\n",
    "\n",
    "    max_items = len(t.groupby(['molecule_name', 'atom_index_0']))# if has_y else 422550\n",
    "    max_atoms = int(s.atom_index.max() + 1)\n",
    "\n",
    "    if has_y:\n",
    "        contributions = ['fc','sd','pso','dso']\n",
    "        magnetic_tensors = ['XX','YX','ZX','XY','YY','ZY','XZ','YZ','ZZ']\n",
    "        XYZ = ['X','Y','Z']\n",
    "    xyz = ['x', 'y', 'z']\n",
    "    \n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_type  = np.zeros((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_ext   = np.zeros((max_items,1,         max_atoms), dtype=np.bool_)\n",
    "    x_atom  = np.empty((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_atom[:] = -1\n",
    "\n",
    "    if has_y:\n",
    "        y_scalar   = np.zeros((max_items,len(contributions)   ,max_atoms), dtype=np.float32)\n",
    "        y_magnetic = np.zeros((max_items,len(magnetic_tensors),max_atoms), dtype=np.float32)\n",
    "        y_mulliken = np.zeros((max_items,1                    ,max_atoms), dtype=np.float32)\n",
    "\n",
    "        y_dipole   = np.zeros((max_items,len(XYZ)), dtype=np.float32)\n",
    "        y_potential= np.zeros((max_items,1       ), dtype=np.float32)\n",
    "\n",
    "        y_magnetic[...] = np.nan\n",
    "        y_mulliken[...] = np.nan\n",
    "    else:\n",
    "        xt_ids = np.zeros((max_items, max_atoms), dtype=np.int32)\n",
    "\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    i = j = 0\n",
    "    \n",
    "    for (m_name, m_index) ,m_group in tqdm(t.groupby(['molecule_name', 'molecule_index'])):\n",
    "        ss = s[s.molecule_name==m_name]\n",
    "        n_atoms = len(ss)\n",
    "        if has_y:\n",
    "            magnetic = magnetic_shielding[\n",
    "                    (magnetic_shielding['molecule_name']==m_name)][magnetic_tensors].values.T\n",
    "\n",
    "            mulliken = mulliken_charges[\n",
    "                    (mulliken_charges['molecule_name']==m_name)]['mulliken_charge'].values.T\n",
    "\n",
    "            scs = scalar_couplings[scalar_couplings['molecule_name']==m_name]\n",
    "            \n",
    "            y_dipole[j,:]= dipole_moments[dipole_moments['molecule_name']==m_name][XYZ].values\n",
    "            y_potential[j,:]=potential_energy[\n",
    "                potential_energy['molecule_name']==m_name]['potential_energy'].values\n",
    "        \n",
    "        for a_name,a_group in m_group.groupby('atom_index_0'):\n",
    "            \n",
    "            ref_a = ss[ss['atom_index']==a_name]\n",
    "            \n",
    "            x_xyz[i] = 0.\n",
    "            x_type[i] = -1\n",
    "            x_ext[i] =  True\n",
    "            \n",
    "            x_xyz[i,:,:n_atoms] = (ss[xyz].values-ref_a[xyz].values).T  # xyz \n",
    "            x_type[i,0,a_group['atom_index_1']] = a_group['type_idx']  # type \n",
    "            x_ext[i,0,a_group['atom_index_1']] = a_group['ext']  # ext \n",
    "            x_atom[i,:,:n_atoms] = ss['atom_idx'].T                \n",
    "\n",
    "            if has_y:\n",
    "                y_scalar[i,:,a_group['atom_index_1']] = scs[scs['atom_index_0']==a_name][contributions]\n",
    "                y_magnetic[i,:,:n_atoms] = magnetic\n",
    "                y_mulliken[i,:,:n_atoms] = mulliken\n",
    "            else:\n",
    "                xt_ids[i,a_group['atom_index_1']] = a_group['id']  \n",
    "\n",
    "            m[i] = m_index\n",
    "            i+=1\n",
    "        j += 1\n",
    "    assert i == max_items\n",
    "    print(i,max_items)\n",
    "    if has_y:\n",
    "        return x_xyz,x_type,x_ext,x_atom, m, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential\n",
    "    else:\n",
    "        return x_xyz,x_type,x_ext,x_atom, m, xt_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where you want to use original training set '' or extended ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:41.692190Z",
     "start_time": "2019-08-19T17:09:41.688409Z"
    }
   },
   "outputs": [],
   "source": [
    "ext = '_ext' # or ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed or preprocess and save for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:48.324228Z",
     "start_time": "2019-08-19T17:09:42.786721Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fname = Path('train.npz')\n",
    "types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "\n",
    "try:\n",
    "    npzfile = np.load(fname_ext(train_fname, ext))\n",
    "    x_xyz   = npzfile['x_xyz']\n",
    "    x_type  = npzfile['x_type']\n",
    "    x_ext   = npzfile['x_ext']\n",
    "    x_atom  = npzfile['x_atom']\n",
    "\n",
    "    y_scalar    = npzfile['y_scalar']\n",
    "    y_magnetic  = npzfile['y_magnetic']\n",
    "    y_mulliken  = npzfile['y_mulliken']\n",
    "    y_dipole    = npzfile['y_dipole']\n",
    "    y_potential = npzfile['y_potential']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_type,x_ext,x_atom, m, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential = \\\n",
    "        preprocess(train_fname.with_suffix('.csv'), type_index=types, ext=ext)\n",
    "    np.savez(fname_ext(train_fname, ext), \n",
    "             x_xyz=x_xyz,\n",
    "             x_type=x_type,\n",
    "             x_ext=x_ext,\n",
    "             x_atom=x_atom,\n",
    "             y_scalar=y_scalar,\n",
    "             y_magnetic=y_magnetic,\n",
    "             y_mulliken=y_mulliken,\n",
    "             y_dipole=y_dipole,\n",
    "             y_potential=y_potential,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:48.332322Z",
     "start_time": "2019-08-19T17:09:48.326293Z"
    }
   },
   "outputs": [],
   "source": [
    "use_memmap = True\n",
    "try:\n",
    "    load_fn = np.load if not use_memmap else partial(np.lib.format.open_memmap, mode='r')\n",
    "    x_coulombmat = load_fn(f'x_coulombmat32{ext}.npy')\n",
    "except:\n",
    "    x_coulombmat = np.load(f'x_coulombmat{ext}.npy', allow_pickle=True)\n",
    "    x_coulombmat = np.array(x_coulombmat.tolist()).astype(np.float32)\n",
    "    np.save(f'x_coulombmat32{ext}.npy', x_coulombmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:48.338202Z",
     "start_time": "2019-08-19T17:09:48.334641Z"
    }
   },
   "outputs": [],
   "source": [
    "x_qm9_mulliken = load_fn(f'x_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:48.346751Z",
     "start_time": "2019-08-19T17:09:48.340324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1405126, 3, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 4, 29),\n",
       " (1405126, 9, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 3),\n",
       " (1405126, 1),\n",
       " (1405126,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_type,x_ext,x_atom,x_qm9_mulliken, \n",
    "                   y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:49.367867Z",
     "start_time": "2019-08-19T17:09:48.348694Z"
    }
   },
   "outputs": [],
   "source": [
    "x_xyz_mean, x_xyz_std = Tensor(x_xyz.mean(axis=(0,2),keepdims=True)), Tensor(x_xyz.std(axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:49.647878Z",
     "start_time": "2019-08-19T17:09:49.369847Z"
    }
   },
   "outputs": [],
   "source": [
    "x_qm9_mulliken_mean = Tensor(x_qm9_mulliken.mean(axis=(0,2),keepdims=True))\n",
    "x_qm9_mulliken_std  = Tensor(x_qm9_mulliken.std( axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:49.653815Z",
     "start_time": "2019-08-19T17:09:49.650096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai classes (this should should be done into its own `application` but who has time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:49.673551Z",
     "start_time": "2019-08-19T17:09:49.657553Z"
    }
   },
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,type,ext,atom,qm9_mulliken,coulomb): \n",
    "        self.i,self.xyz,self.type,self.ext, self.atom,self.qm9_mulliken,self.coulomb = \\\n",
    "            i,xyz,type,ext,atom,qm9_mulliken,coulomb\n",
    "        self.data = [Tensor(xyz), LongTensor((type)), \n",
    "                     Tensor(ext), LongTensor((atom)),Tensor(qm9_mulliken), Tensor(coulomb)]\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i,self.xyz,self.type,self.ext,self.atom,self.qm9_mulliken,self.coulomb)\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,magnetic,mulliken,dipole,potential,**kwargs): \n",
    "        self.scalar,self.magnetic,self.mulliken,self.dipole,self.potential = \\\n",
    "            scalar,magnetic,mulliken,dipole,potential\n",
    "        self.data = (Tensor(scalar), Tensor(magnetic), Tensor(dipole), Tensor(potential))\n",
    "    def __str__(self):\n",
    "        res, spacer, n_couplings = '', '', 0\n",
    "        for s in self.data[0].sum(dim=0):\n",
    "            if s==0.: spacer = ' * '\n",
    "            else: \n",
    "                res += f'{spacer}{s:.4f}'\n",
    "                spacer = ' '\n",
    "                n_couplings +=1\n",
    "        return f'{n_couplings}: {res}'\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:49.691659Z",
     "start_time": "2019-08-19T17:09:49.675862Z"
    }
   },
   "outputs": [],
   "source": [
    "class LMAEMaskedLoss(Module):\n",
    "    def __init__(self,\n",
    "                 contrib_w=0., magnetic_w=0., dipole_w=0., potential_w=0., \n",
    "                 types_w = [1]*n_types, return_all=False, proxy_log=torch.log, exclude_ext=False):\n",
    "        self.contrib_w,self.magnetic_w,self.dipole_w,self.potential_w = contrib_w,magnetic_w,dipole_w,potential_w\n",
    "        self.types_w = types_w\n",
    "        self.return_all = return_all\n",
    "        self.proxy_log = proxy_log\n",
    "        self.exclude_ext = exclude_ext\n",
    "    \n",
    "    def forward(self, input_outputs, t_scalar, t_magnetic, t_dipole, t_potential):    \n",
    "        type, ext, p_scalar, p_magnetic, p_dipole, p_potential = input_outputs\n",
    "        loss = 0.\n",
    "        n = 0\n",
    "        j_loss = [0] * n_types\n",
    "        for t in range(n_types):\n",
    "            mask = (type == t).squeeze(1) if not self.exclude_ext else ((type == t) & (ext == 0)).squeeze(1)\n",
    "            if mask.sum() > 0:\n",
    "                _output,_target = p_scalar.transpose(1,2)[mask], t_scalar.transpose(1,2)[mask] # scalars at the end\n",
    "                # LMAE scalar\n",
    "                s_loss = self.proxy_log((_output.sum(dim=-1) - _target.sum(dim=-1)).abs().mean()+1e-9)\n",
    "                loss += self.types_w[t] * s_loss\n",
    "                j_loss[t] += s_loss\n",
    "                # LMAE scalar contributions\n",
    "                #for i_contrib in range(_output.shape[-1]):\n",
    "                #    loss += self.contrib_w * \\\n",
    "                #        self.proxy_log((_output[...,i_contrib] - _target[...,i_contrib]).abs().mean()+1e-9)\n",
    "                n+=1\n",
    "        loss /= n\n",
    "        \n",
    "        if self.magnetic_w > 0:\n",
    "            mask = ~torch.isnan(t_magnetic)\n",
    "            loss += self.magnetic_w * MSELossFlat()(p_magnetic[mask], t_magnetic[mask])\n",
    "            \n",
    "        if self.dipole_w    > 0: loss += self.dipole_w    * MSELossFlat()(p_dipole,    t_dipole)\n",
    "        if self.potential_w > 0: loss += self.potential_w * MSELossFlat()(p_potential, t_potential)\n",
    "\n",
    "        return loss if not self.return_all else (loss, *j_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:49.698750Z",
     "start_time": "2019-08-19T17:09:49.693805Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return ScalarCouplingItem(*o)\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quaterions allow us to rotate 3d points randoming with a nice uniform distribution of 3 numbers hece we use them, however it's still to be seen if are useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:09:49.733919Z",
     "start_time": "2019-08-19T17:09:49.700715Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "    \n",
    "    original_shape = list(v.shape)\n",
    "    q = q.view(-1, 4)\n",
    "    v = v.view(-1, 3)\n",
    "    \n",
    "    qvec = q[:, 1:]\n",
    "    uv = torch.cross(qvec, v, dim=1)\n",
    "    uuv = torch.cross(qvec, uv, dim=1)\n",
    "    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)\n",
    "\n",
    "def random_rotation(data):\n",
    "    x_xyz = data[0].transpose(0,1)\n",
    "    r = torch.rand(3)\n",
    "    sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-r[:1]),torch.sqrt(r[:1]),2*math.pi*r[1:2],2*math.pi*r[2:3]\n",
    "    q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                   sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=0).unsqueeze(0)\n",
    "    x_xyz = qrot(q.expand(x_xyz.shape[0],-1), x_xyz).squeeze(0).transpose(0,1)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def normalize(data):\n",
    "    sq = False\n",
    "    if data[0].ndim < 3:\n",
    "        data[0].unsqueeze_(0)\n",
    "        data[4].unsqueeze_(0)\n",
    "        sq = True\n",
    "    x_xyz      = (data[0] - x_xyz_mean)          / x_xyz_std\n",
    "    x_mulliken = (data[4] - x_qm9_mulliken_mean) / x_qm9_mulliken_std\n",
    "    if sq:\n",
    "        x_xyz.squeeze_(0)\n",
    "        x_mulliken.squeeze_(0)\n",
    "    return (x_xyz, data[1],data[2],data[3],x_mulliken,data[5])\n",
    "\n",
    "def canonize_(data):\n",
    "    xyz,type,ext,atom,mulliken,coulomb = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    mask_atoms = ~mask.unsqueeze(0)\n",
    "    n_atoms = mask_atoms.sum()\n",
    "    i = torch.nonzero(type.squeeze(0) == -1)[0] # pick first one w/o j-coupling\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask],mulliken[:,mask] = 0,-1,1,-1,0\n",
    "    return (xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms)\n",
    "\n",
    "def canonize(data):\n",
    "    xyz,type,ext,atom,mulliken,coulomb = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    i_max_atom = torch.nonzero(atom != -1).max() + 1\n",
    "    mask_atoms = torch.ones ((max_atoms,  max_atoms), dtype=torch.uint8)\n",
    "    zeros      = torch.zeros((i_max_atom,i_max_atom), dtype=torch.uint8)\n",
    "    mask_atoms[:zeros.shape[0],:zeros.shape[1]] = zeros\n",
    "    n_atoms = i_max_atom\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask],mulliken[:,mask] = 0,-1,1,-1,0\n",
    "    return (xyz,type,ext,atom,mulliken, coulomb, mask_atoms, n_atoms)\n",
    "\n",
    "def knockout(data):\n",
    "    xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms = data\n",
    "    atom_to_knockout = random.randint(0, n_atoms-1)\n",
    "    mask_atoms[atom_to_knockout,:] = 1\n",
    "    return xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms\n",
    "\n",
    "def canonize2(data):\n",
    "    xyz,type,ext,atom,mulliken = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    i_max_atom = torch.nonzero(atom != -1).max() + 1\n",
    "    #mask_atoms = torch.ones((max_atoms,   max_atoms), dtype=torch.uint8)\n",
    "    #zeros      = torch.zeros((i_max_atom, i_max_atom), dtype=torch.uint8)\n",
    "    #mask_atoms[:zeros.shape[0],:zeros.shape[1]] = zeros\n",
    "\n",
    "    mask_atoms = torch.ones ((max_atoms, ), dtype=torch.uint8)\n",
    "    zeros      = torch.zeros((i_max_atom,), dtype=torch.uint8)\n",
    "    mask_atoms[:zeros.shape[0],] = zeros\n",
    "    #mask_atoms[(type==-1).squeeze(0),:] = 1\n",
    "    n_atoms = i_max_atom\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask],mulliken[:,mask] = 0,-1,1,-1,0\n",
    "    return (xyz,type,ext,atom,mulliken, mask_atoms, n_atoms)\n",
    "\n",
    "def canonize_and_shuffle(data):\n",
    "    xyz,type,ext,atom,mulliken,coulomb = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    mask_atoms = ~mask.unsqueeze(0)\n",
    "    n_atoms = mask_atoms.sum()\n",
    "    i = torch.nonzero(type.squeeze(0) == -1)[0] # pick first one w/o j-coupling\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask],mulliken[:,mask] = \\\n",
    "        0,-1,1,-1,0\n",
    "    \n",
    "    perm = torch.randperm(n_atoms)\n",
    "    xyz[:,~mask] = xyz[:,~mask][:,perm]\n",
    "    type[:,~mask] = type[:,~mask][:,perm]\n",
    "    ext[:,~mask] = ext[:,~mask][:,perm]\n",
    "    atom[:,~mask] = atom[:,~mask][:,perm]\n",
    "    mulliken[:,~mask] = mulliken[:,~mask][:,perm]\n",
    "    \n",
    "    return (xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0, 29-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build `data` bunch etc. for fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:18.654986Z",
     "start_time": "2019-08-19T17:09:49.735995Z"
    }
   },
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                       enumerate(zip(x_xyz,x_type,x_ext,x_atom,x_qm9_mulliken,x_coulombmat))),\n",
    "                label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:19.035949Z",
     "start_time": "2019-08-19T17:11:18.657069Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:19.162729Z",
     "start_time": "2019-08-19T17:11:19.038706Z"
    },
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = data.split_by_idx(idx_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:22.994242Z",
     "start_time": "2019-08-19T17:11:19.164755Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.label_from_func(\n",
    "    func=lambda o: (y_scalar[o.i], y_magnetic[o.i], y_mulliken[o.i], y_dipole[o.i], y_potential[o.i]),\n",
    "    label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:23.000636Z",
     "start_time": "2019-08-19T17:11:22.996375Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    try:\n",
    "        xt_coulombmat = load_fn(f'xt_coulombmat32{ext}.npy')\n",
    "    except:\n",
    "        xt_coulombmat = np.load(f'xt_coulombmat{ext}.npy', allow_pickle=True)\n",
    "        xt_coulombmat = np.array(xt_coulombmat.tolist()).astype(np.float32)\n",
    "        np.save(f'xt_coulombmat32{ext}.npy', xt_coulombmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:23.010259Z",
     "start_time": "2019-08-19T17:11:23.002572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function normalize at 0x7f0199711c80>, <function canonize at 0x7f01994b8f28>] [<function normalize at 0x7f0199711c80>, <function canonize at 0x7f01994b8f28>]\n"
     ]
    }
   ],
   "source": [
    "tfms = [normalize, canonize]\n",
    "tta_tfms = list(tfms)\n",
    "#tta_tfms.insert(0,random_rotation)\n",
    "#tta_tfms.append(knockout)\n",
    "#tta_tfms[-1] = canonize_and_shuffle\n",
    "print(tta_tfms, tfms)\n",
    "data = data.transform((tta_tfms, tfms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:23.143873Z",
     "start_time": "2019-08-19T17:11:23.012470Z"
    }
   },
   "outputs": [],
   "source": [
    "data=data.databunch()#num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelList (78217 items)\n",
       "x: ItemList\n",
       "16 8 atoms 7 couplings,17 8 atoms 7 couplings,18 8 atoms 7 couplings,19 8 atoms 7 couplings,20 8 atoms 7 couplings\n",
       "y: ScalarCouplingList\n",
       "7: 83.5430 -2.3783 * -11.7004 -11.6979 3.2528 13.6913 3.2521,7: 83.5417 -2.3786 -11.7004 * -11.6996 13.6924 3.2525 3.2527,7: 83.5484 -2.3772 -11.6979 -11.6996 * 3.2524 3.2524 13.6921,7: -2.3788 83.5418 3.2528 13.6924 3.2524 * -11.7004 -11.6993,7: -2.3785 83.5430 13.6913 3.2525 3.2524 -11.7004 * -11.6976\n",
       "Path: ."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds.filter_by_func(lambda item, _: len(item.data[2].cpu().numpy()[item.data[2].cpu().numpy()==0]) == 0)\n",
    "data.valid_ds.filter_by_func(lambda item, _: len(item.data[2].cpu().numpy()[item.data[2].cpu().numpy()==0]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Whole model here, self-contained (needs some cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:23.162815Z",
     "start_time": "2019-08-19T17:11:23.146818Z"
    }
   },
   "outputs": [],
   "source": [
    "class LMAEMetric(LearnerCallback):\n",
    "    _order=-20 # Needs to run before the recorder\n",
    "    def __init__(self, learn, val_only=True):\n",
    "        super().__init__(learn)\n",
    "        self.val_only=val_only\n",
    "        self.metric = LMAEMaskedLoss(return_all=True, exclude_ext=True)\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.val_only: self.learn.recorder.add_metric_names(['tLMAE'])\n",
    "        self.learn.recorder.add_metric_names(['ðŸ‘‰ðŸ»LMAEðŸ‘ˆðŸ»'] + [f'lmae{i}' for i in range(n_types)])\n",
    "            \n",
    "    def on_batch_end(self, train, last_output, last_target, **kwargs):\n",
    "        if self.val_only and train: return \n",
    "        preds,targs = self.preds[int(train)], self.targs[int(train)] # 0 val 1 train\n",
    "        if preds is None:\n",
    "            targs, preds = listify(last_target), listify(last_output)\n",
    "            targs,preds = [t.detach() for t in targs],[t.detach() for t in preds]\n",
    "        else:\n",
    "            for i,(o,t) in enumerate(zip(last_output, last_target)):\n",
    "                preds[i] = torch.cat([preds[i], o.detach()], dim=0)\n",
    "                targs[i] = torch.cat([targs[i], t.detach()], dim=0)\n",
    "        self.preds[int(train)], self.targs[int(train)] = preds,targs\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.targs, self.preds = [None, None], [None, None]\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        mets = []\n",
    "        if self.preds[1]: mets.append(self.metric.forward(self.preds[1], *self.targs[1])[0]) # just tLMAE\n",
    "        if self.preds[0]: mets.extend(self.metric.forward(self.preds[0], *self.targs[0]))\n",
    "        return add_metrics(last_metrics, mets) if mets else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:23.240391Z",
     "start_time": "2019-08-19T17:11:23.166832Z"
    }
   },
   "outputs": [],
   "source": [
    "Activation = Enum('Activation', 'ReLU Swish GeLU')\n",
    "\n",
    "class PositionalEncoding(Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:, :-1]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        print(pe.size())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2, 1)\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x).transpose(2, 1)\n",
    "    \n",
    "class GeLU(Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class Swish(Module):\n",
    "    def forward(self, x): return x * torch.sigmoid(x)\n",
    "\n",
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish()}\n",
    "\n",
    "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=False,\n",
    "                 scale:bool=True):\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "\n",
    "        self.attention1 = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.attention2 = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.attention3 = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.attention4 = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs):\n",
    "        return self.ln(x + self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "\n",
    "    def _apply_attention(self, x:Tensor, mask:Tensor=None):\n",
    "        if False:\n",
    "            if mask is not None:\n",
    "                x= x.masked_fill(mask[:,:,0].squeeze(1).unsqueeze(-1).repeat(1, 1, x.size(2)),  0.)\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = self.attention1(x), self.attention2(x), self.attention3(x)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask,  -1.0E9).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return self.attention4(attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1))\n",
    "\n",
    "    def _attention_einsum(self, x, mask=None):\n",
    "        # Permute and matmul is a little bit faster but this implementation is more readable\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(x), 3, dim=-1)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        attn_score = torch.einsum('bind,bjnd->bijn', (wq, wk))\n",
    "        if self.scale: attn_score.mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('-inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=2))\n",
    "        attn_vec = torch.einsum('bijn,bjnd->bind', (attn_prob, wv))\n",
    "        return attn_vec.contiguous().view(bs, x_len, -1)\n",
    "\n",
    "\n",
    "class DecoderLayer(Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, act:Activation=Activation.ReLU, double_drop:bool=True,\n",
    "                 attn_cls:Callable=MultiHeadAttention):\n",
    "        self.mhra = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): return self.ff(self.mhra(x, mask=mask, **kwargs))\n",
    "\n",
    "class Transformer(Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int,\n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention,\n",
    "                 learned_pos_enc:bool=True, mask:bool=True):\n",
    "        self.mask = mask\n",
    "        #self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        #self.pos_enc = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        #self.pos_enc = PositionalEncoding(29, 0.1)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop,\n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "\n",
    "    def reset(self): pass\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        #bs, x_len = x.size()\n",
    "        #pos = torch.arange(0, x_len, device=x.device, dtype=x.dtype)\n",
    "        inp = self.drop_emb(x)# + self.pos_enc(pos)[None]) #.mul_(self.d_model ** 0.5)\n",
    "        #if False:\n",
    "        #    inp += self.pos_enc(x)[None]\n",
    "        #mask = None #torch.triu(x.new_ones(x_len, x_len), diagonal=1).byte()[None,None] if self.mask else None\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "        for layer in self.layers: inp = layer(inp, mask=mask)\n",
    "        return inp #For the LinearDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:23.280402Z",
     "start_time": "2019-08-19T17:11:23.242589Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionOld(Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.attention = nn.Linear(d_model, 3 * n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs):\n",
    "        return self.ln(x + self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "\n",
    "    def _apply_attention(self, x:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(x), 3, dim=-1)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask,  -1.0E9).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)\n",
    "    \n",
    "class AtomTransformerOld(Module):\n",
    "    def __init__(self,  n_heads,d_model, d_head=None, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.transformer = Transformer(n_heads=n_heads,d_model=d_model, d_head=d_head, **kwargs)\n",
    "        \n",
    "        #self.scalar    = nn.Conv1d(d_model+ n_types + 1, 4, 1)\n",
    "        self.scalar    = nn.Conv1d(d_model, 4, 1)\n",
    "        self.magnetic  = nn.Conv1d(d_model, 9, 1)\n",
    "        self.dipole    = nn.Linear(d_model*max_atoms, 3)\n",
    "        self.potential = nn.Linear(d_model*max_atoms, 1)\n",
    "        \n",
    "        self.n_atom_embedding = d_model//2\n",
    "        self.n_type_embedding = d_model - self.n_atom_embedding - 6\n",
    "        self.type_embedding = nn.Embedding(len(types)+1,self.n_type_embedding)\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,self.n_atom_embedding)\n",
    "    \n",
    "        #n_pos_encoder = d_model - n_type_embedding - n_atom_embedding\n",
    "        #self.pos_encoder = nn.Sequential(\n",
    "        #    nn.Conv1d(3+1+1,n_pos_encoder, 1), nn.ReLU(), nn.BatchNorm1d(n_pos_encoder),\n",
    "        #)\n",
    "        \n",
    "    def forward(self,xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms):\n",
    "        bs, _, n_pts = xyz.shape        \n",
    "        t = self.type_embedding((type+1).squeeze(1)) #* math.sqrt(self.n_atom_embedding) #.transpose(1,2)\n",
    "        a = self.atom_embedding((atom+1).squeeze(1)) #* math.sqrt(self.n_type_embedding) #.transpose(1,2)\n",
    "        \n",
    "        x = torch.cat([xyz, mulliken, ext, mask_atoms.float()], dim=1) #* math.sqrt(self.d_model)        \n",
    "        #x = self.pos_encoder(x).transpose(1,2)\n",
    "\n",
    "        x = torch.cat([x.transpose(1,2), t, a], dim=-1) \n",
    "\n",
    "        mask = (coulomb == 0).unsqueeze(1)\n",
    "        #mask = torch.triu(x.new_ones(max_atoms, max_atoms), diagonal=1).byte()[None,:,:,None]#[None,None] \n",
    "        #print(mask.shape, mask)\n",
    "        x = self.transformer(x, mask).transpose(1,2).contiguous()\n",
    "        \n",
    "        #t_one_hot = torch.zeros(bs,n_types+1,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1, 1.)\n",
    "        #scalar    = self.scalar(torch.cat([x, t_one_hot], dim=1))\n",
    "        scalar    = self.scalar(x)\n",
    "        \n",
    "        magnetic  = self.magnetic(x) \n",
    "        dipole    = self.dipole(x.view(bs,-1))\n",
    "        potential = self.potential(x.view(bs,-1))\n",
    "                \n",
    "        return type,ext,scalar,magnetic,dipole,potential\n",
    "    \n",
    "    def reset(self): pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:23.352590Z",
     "start_time": "2019-08-19T17:11:23.282680Z"
    }
   },
   "outputs": [],
   "source": [
    "class AtomTransformer(Module):\n",
    "    def __init__(self,  n_heads,d_model, d_head=None,dropout=0, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.transformer = Transformer(n_heads=n_heads,d_model=d_model, d_head=d_head, **kwargs)\n",
    "        \n",
    "        self.scalar    = nn.Conv1d(d_model+ n_types + 1, 1, 1)\n",
    "        self.magnetic  = nn.Conv1d(d_model, 9, 1)\n",
    "        self.dipole    = nn.Linear(d_model*max_atoms, 3)\n",
    "        self.potential = nn.Linear(d_model*max_atoms, 1)\n",
    "        \n",
    "        self.n_atom_embedding = d_model//2\n",
    "        self.n_type_embedding = d_model - self.n_atom_embedding - 3 # -3 best model\n",
    "        self.type_embedding = nn.Embedding(len(types)+1,self.n_type_embedding)\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,self.n_atom_embedding)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "        #n_pos_encoder = d_model - n_type_embedding - n_atom_embedding\n",
    "        #self.pos_encoder = nn.Sequential(\n",
    "        #    nn.Conv1d(3+1+1,n_pos_encoder, 1), nn.ReLU(), nn.BatchNorm1d(n_pos_encoder),\n",
    "        #)\n",
    "        \n",
    "    def forward(self,xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms):\n",
    "        bs, _, n_pts = xyz.shape        \n",
    "        t = self.type_embedding((type+1).squeeze(1)) #* math.sqrt(self.n_atom_embedding) #.transpose(1,2)\n",
    "        a = self.atom_embedding((atom+1).squeeze(1)) #* math.sqrt(self.n_type_embedding) #.transpose(1,2)\n",
    "        \n",
    "        #x = torch.cat([xyz, mulliken, ext, mask_atoms.float()], dim=1) #* math.sqrt(self.d_model)               \n",
    "        #x = torch.cat([xyz, ext, mask_atoms.float()], dim=1) #* math.sqrt(self.d_model)               \n",
    "        x = xyz \n",
    "        #x = self.pos_encoder(x).transpose(1,2)\n",
    "\n",
    "        x = torch.cat([x.transpose(1,2), t, a], dim=-1) \n",
    "\n",
    "        mask = mask_atoms.to(dtype=torch.bool).unsqueeze(1)\n",
    "        #mask = (coulomb == 0).unsqueeze(1)\n",
    "        #mask = torch.triu(x.new_ones(max_atoms, max_atoms), diagonal=1).byte()[None,:,:,None]#[None,None] \n",
    "        #print(mask.shape, mask)\n",
    "        x = self.transformer(x, mask).transpose(1,2).contiguous()\n",
    "        x = self.dropout(x)\n",
    "        t_one_hot = torch.zeros(bs,n_types+1,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1, 1.)\n",
    "        \n",
    "        scalar    = self.scalar(torch.cat([x, t_one_hot], dim=1))\n",
    "        magnetic  = self.magnetic(x) \n",
    "        dipole    = self.dipole(x.view(bs,-1))\n",
    "        potential = self.potential(x.view(bs,-1))\n",
    "                \n",
    "        return type,ext,scalar,magnetic,dipole,potential\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    \n",
    "class AtomTransformer2(Module):\n",
    "    def __init__(self,  n_heads,d_model, d_head=None, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.transformer = Transformer(n_heads=n_heads,d_model=d_model, d_head=d_head, **kwargs)\n",
    "        \n",
    "        self.scalar    = nn.Conv1d(d_model+ n_types + 1, 4, 1)\n",
    "        self.magnetic  = nn.Conv1d(d_model, 9, 1)\n",
    "        self.dipole    = nn.Linear(d_model*max_atoms, 3)\n",
    "        self.potential = nn.Linear(d_model*max_atoms, 1)\n",
    "        \n",
    "        self.n_atom_embedding = d_model//2\n",
    "        self.n_type_embedding = d_model - self.n_atom_embedding - 6 # -3 best model\n",
    "        self.type_embedding = nn.Embedding(len(types)+1,self.n_type_embedding)\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,self.n_atom_embedding)\n",
    "    \n",
    "        #n_pos_encoder = d_model - n_type_embedding - n_atom_embedding\n",
    "        #self.pos_encoder = nn.Sequential(\n",
    "        #    nn.Conv1d(3+1+1,n_pos_encoder, 1), nn.ReLU(), nn.BatchNorm1d(n_pos_encoder),\n",
    "        #)\n",
    "        \n",
    "    def forward(self,xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms):\n",
    "        bs, _, n_pts = xyz.shape        \n",
    "        t = self.type_embedding((type+1).squeeze(1)) #* math.sqrt(self.n_atom_embedding) #.transpose(1,2)\n",
    "        a = self.atom_embedding((atom+1).squeeze(1)) #* math.sqrt(self.n_type_embedding) #.transpose(1,2)\n",
    "        \n",
    "        x = torch.cat([xyz, mulliken, ext, mask_atoms.float()], dim=1) #* math.sqrt(self.d_model)               \n",
    "        #x = torch.cat([xyz, ext, mask_atoms.float()], dim=1) #* math.sqrt(self.d_model)               \n",
    "        #x = xyz \n",
    "        #x = self.pos_encoder(x).transpose(1,2)\n",
    "\n",
    "        x = torch.cat([x.transpose(1,2), t, a], dim=-1) \n",
    "\n",
    "        mask = (coulomb == 0).unsqueeze(1)\n",
    "        #mask = torch.triu(x.new_ones(max_atoms, max_atoms), diagonal=1).byte()[None,:,:,None]#[None,None] \n",
    "        #print(mask.shape, mask)\n",
    "        x = self.transformer(x, mask).transpose(1,2).contiguous()\n",
    "        t_one_hot = torch.zeros(bs,n_types+1,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1, 1.)\n",
    "        \n",
    "        scalar    = self.scalar(torch.cat([x, t_one_hot], dim=1))\n",
    "        magnetic  = self.magnetic(x) \n",
    "        dipole    = self.dipole(x.view(bs,-1))\n",
    "        potential = self.potential(x.view(bs,-1))\n",
    "                \n",
    "        return type,ext,scalar,magnetic,dipole,potential\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "\n",
    "class MultiHeadAttention3(Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=False,\n",
    "                 scale:bool=False):\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "\n",
    "        self.attention1 = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.attention2 = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.attention3 = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.attention4 = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.scale=False\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs):\n",
    "        return self.ln(x + self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "\n",
    "    def _apply_attention(self, x:Tensor, mask:Tensor=None):\n",
    "        if False:\n",
    "            if mask is not None:\n",
    "                x= x.masked_fill(mask[:,:,0].squeeze(1).unsqueeze(-1).repeat(1, 1, x.size(2)),  0.)\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = self.attention1(x), self.attention2(x), self.attention3(x)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask,  -1.0E9).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return self.attention4(attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1))\n",
    "\n",
    "    def _attention_einsum(self, x, mask=None):\n",
    "        # Permute and matmul is a little bit faster but this implementation is more readable\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(x), 3, dim=-1)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        attn_score = torch.einsum('bind,bjnd->bijn', (wq, wk))\n",
    "        if self.scale: attn_score.mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('-inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=2))\n",
    "        attn_vec = torch.einsum('bijn,bjnd->bind', (attn_prob, wv))\n",
    "        return attn_vec.contiguous().view(bs, x_len, -1)\n",
    "\n",
    "    \n",
    "class AtomTransformer3(Module):\n",
    "    def __init__(self,  n_heads,d_model, d_head=None, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.transformer = Transformer(n_heads=n_heads,d_model=d_model, d_head=d_head, **kwargs)\n",
    "        \n",
    "        self.scalar    = nn.Conv1d(d_model+ n_types + 1, 4, 1)\n",
    "        self.magnetic  = nn.Conv1d(d_model, 9, 1)\n",
    "        self.dipole    = nn.Linear(d_model*max_atoms, 3)\n",
    "        self.potential = nn.Linear(d_model*max_atoms, 1)\n",
    "        \n",
    "        self.n_atom_embedding = d_model//2\n",
    "        self.n_type_embedding = d_model - self.n_atom_embedding - 3 # -3 best model\n",
    "        self.type_embedding = nn.Embedding(len(types)+1,self.n_type_embedding)\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,self.n_atom_embedding)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "    \n",
    "        #n_pos_encoder = d_model - n_type_embedding - n_atom_embedding\n",
    "        #self.pos_encoder = nn.Sequential(\n",
    "        #    nn.Conv1d(3+1+1,n_pos_encoder, 1), nn.ReLU(), nn.BatchNorm1d(n_pos_encoder),\n",
    "        #)\n",
    "        \n",
    "    def forward(self,xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms):\n",
    "        bs, _, n_pts = xyz.shape        \n",
    "        t = self.type_embedding((type+1).squeeze(1)) #* math.sqrt(self.n_atom_embedding) #.transpose(1,2)\n",
    "        a = self.atom_embedding((atom+1).squeeze(1)) #* math.sqrt(self.n_type_embedding) #.transpose(1,2)\n",
    "        \n",
    "        #x = torch.cat([xyz, mulliken, ext, mask_atoms.float()], dim=1) #* math.sqrt(self.d_model)               \n",
    "        #x = torch.cat([xyz, ext, mask_atoms.float()], dim=1) #* math.sqrt(self.d_model)               \n",
    "        x = xyz \n",
    "        #x = self.pos_encoder(x).transpose(1,2)\n",
    "\n",
    "        x = torch.cat([x.transpose(1,2), t, a], dim=-1) \n",
    "\n",
    "        mask = (coulomb == 0).unsqueeze(1)\n",
    "        #mask = torch.triu(x.new_ones(max_atoms, max_atoms), diagonal=1).byte()[None,:,:,None]#[None,None] \n",
    "        #print(mask.shape, mask)\n",
    "        x = self.transformer(x, mask).transpose(1,2).contiguous()\n",
    "        x = self.dropout(x)\n",
    "        t_one_hot = torch.zeros(bs,n_types+1,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1, 1.)\n",
    "        \n",
    "        scalar    = self.scalar(torch.cat([x, t_one_hot], dim=1))\n",
    "        magnetic  = self.magnetic(x) \n",
    "        dipole    = self.dipole(x.view(bs,-1))\n",
    "        potential = self.potential(x.view(bs,-1))\n",
    "                \n",
    "        return type,ext,scalar,magnetic,dipole,potential\n",
    "    \n",
    "    def reset(self): pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback allows to insert multiple stateful (not averaged) metrics in one pass. Addditionally we could add metrics for train if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model instantiation: where's all your TPUs/GPUs when you need a decent hyperparam sweep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:23.367362Z",
     "start_time": "2019-08-19T17:11:23.354688Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.callbacks import *\n",
    "#from fastai.callbacks import SaveModelCallback\n",
    "class SaveModelCustomCallback(TrackerCallback):\n",
    "    \"A `TrackerCallback` that saves the model when monitored quantity is best.\"\n",
    "    def __init__(self, learn:Learner, monitor:str='valid_loss', mode:str='auto', every:str='improvement', name:str='bestmodel'):\n",
    "        super().__init__(learn, monitor=monitor, mode=mode)\n",
    "        self.every,self.name = every,name\n",
    "        if self.every not in ['improvement', 'epoch']:\n",
    "            warn(f'SaveModel every {self.every} is invalid, falling back to \"improvement\".')\n",
    "            self.every = 'improvement'\n",
    "                 \n",
    "    def jump_to_epoch(self, epoch:int)->None:\n",
    "        try: \n",
    "            self.learn.load(f'{self.name}_{epoch-1}', purge=False)\n",
    "            print(f\"Loaded {self.name}_{epoch-1}\")\n",
    "        except: print(f'Model {self.name}_{epoch-1} not found.')\n",
    "\n",
    "    def on_epoch_end(self, epoch:int, **kwargs:Any)->None:\n",
    "        \"Compare the value monitored to its best score and maybe save the model.\"\n",
    "        if self.every==\"epoch\": self.learn.save(f'{self.name}_{epoch}')\n",
    "        else: #every=\"improvement\"\n",
    "            current = self.get_monitor_value()\n",
    "            if current is not None and self.operator(current, self.best):\n",
    "                print(f'Better model found at epoch {epoch} with {self.monitor} value: {current}.')\n",
    "                self.best = current\n",
    "                self.learn.save(f'{self.name}_{epoch}_{self.best}')\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        \"Load the best model.\"\n",
    "        if False and self.every==\"improvement\" and (self.learn.path/f'{self.learn.model_dir}/{self.name}.pth').is_file():\n",
    "            self.learn.load(f'{self.name}', purge=False)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:31.712227Z",
     "start_time": "2019-08-19T17:11:23.369116Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "net = AtomTransformer(n_layers=6, n_heads=16,d_model=1024,d_inner=4096, act=Activation.Swish, dropout=0)\n",
    "#net = AtomTransformer3(n_layers=6, n_heads=16,d_model=1024,d_inner=4096, attn_cls=MultiHeadAttention3)\n",
    "#net = AtomTransformer(n_layers=6, n_heads=16,d_model=2048,d_inner=2048)\n",
    "#net = AtomTransformer(n_layers=6, n_heads=8, d_model=512,d_inner=2048)\n",
    "\n",
    "#net = AtomTransformerOld(n_layers=6, n_heads=8,d_model=512,d_inner=2048, attn_cls=MultiHeadAttentionOld)\n",
    "\n",
    "#net = AtomTransformer2(n_layers=6, n_heads=12,d_model=768,d_inner=3072)\n",
    "\n",
    "\n",
    "#net = AtomTransformer(n_layers=6, n_heads=12,d_model=768,d_inner=3072)\n",
    "\n",
    "#net = AtomTransformer(n_layers=6, n_heads=16,d_model=1024,d_inner=4096, attn_cls=MultiHeadAttentionOld)\n",
    "\n",
    "learner = Learner(data,net, loss_func=LMAEMaskedLoss(),)\n",
    "\n",
    "learner.callbacks.append(LMAEMetric(learner))\n",
    "\n",
    "for p in learner.model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform(p)\n",
    "        \n",
    "\n",
    "learner.callbacks.append(SaveModelCustomCallback(learner, monitor='ðŸ‘‰ðŸ»LMAEðŸ‘ˆðŸ»', mode='min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:53:26.675699Z",
     "start_time": "2019-08-19T13:53:24.807346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomTransformer\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [16, 29, 29]         0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           4,198,400  True      \n",
       "______________________________________________________________________\n",
       "Swish                [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           4,195,328  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [16, 29, 29]         0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           4,198,400  True      \n",
       "______________________________________________________________________\n",
       "Swish                [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           4,195,328  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [16, 29, 29]         0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           4,198,400  True      \n",
       "______________________________________________________________________\n",
       "Swish                [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           4,195,328  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [16, 29, 29]         0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           4,198,400  True      \n",
       "______________________________________________________________________\n",
       "Swish                [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           4,195,328  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [16, 29, 29]         0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           4,198,400  True      \n",
       "______________________________________________________________________\n",
       "Swish                [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           4,195,328  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           1,049,600  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [16, 29, 29]         0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           4,198,400  True      \n",
       "______________________________________________________________________\n",
       "Swish                [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1024]           4,195,328  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 1024]           0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 1024]           2,048      True      \n",
       "______________________________________________________________________\n",
       "Conv1d               [1, 29]              1,034      True      \n",
       "______________________________________________________________________\n",
       "Conv1d               [9, 29]              9,225      True      \n",
       "______________________________________________________________________\n",
       "Linear               [3]                  89,091     True      \n",
       "______________________________________________________________________\n",
       "Linear               [1]                  29,697     True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 509]            4,581      True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 512]            3,072      True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1024, 29]           0          False     \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 82,011,644\n",
       "Total trainable params: 82,011,644\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : LMAEMaskedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied \n",
       "    LMAEMetric\n",
       "    SaveModelCustomCallback"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:31.717606Z",
     "start_time": "2019-08-19T17:11:31.714392Z"
    }
   },
   "outputs": [],
   "source": [
    "#sub_fname = \"loss-5.9943val-2.7766\" # uncomment or set None to skip loading trained net\n",
    "#sub_fname = \"loss-5.7552val-2.9950\"\n",
    "#sub_fname = \"loss-5.9943val-2.7766\"\n",
    "#sub_fname = \"loss-4.9516val-3.0042\"\n",
    "#sub_fname = \"loss-4.0414val-2.7287\"\n",
    "#sub_fname = \"loss-4.9044val-2.5880\"\n",
    "\n",
    "#sub_fname = \"loss-5.0862val-3.0047\"\n",
    "\n",
    "#sub_fname = 'loss-4.9516val-2.8229'\n",
    "#sub_fname = 'bestmodel_62_-2.2235493659973145'\n",
    "#sub_fname = 'loss-5.6540val-3.0131'\n",
    "#sub_fname = 'bestmodel_6_-0.6834144592285156'\n",
    "sub_fname = 'loss-5.1336val-3.0759'\n",
    "sub_fname = 'loss-3.5933val-3.0637'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:41.657025Z",
     "start_time": "2019-08-19T17:11:31.721110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: loss-3.5933val-3.0637... Loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Attempting to load: {sub_fname}... \", end=\"\")\n",
    "    learner.load(sub_fname, strict=False,with_opt=False)\n",
    "    print(\"Loaded\")\n",
    "except Exception as e:\n",
    "    print(\"NOT loaded! \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T06:40:14.614754Z",
     "start_time": "2019-08-19T06:40:14.609905Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for name, param in learner.model.named_parameters():\n",
    "\n",
    "        if 'scalar' not in name:\n",
    "            param.requires_grad = False\n",
    "            pass\n",
    "        else:\n",
    "            print(name)\n",
    "            try:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:41.662778Z",
     "start_time": "2019-08-19T17:11:41.658966Z"
    }
   },
   "outputs": [],
   "source": [
    "learner = learner.to_parallel() #b/c it NaNs loss (probably would need to change fp16 settings)\n",
    "data.batch_size = int(4096//2)\n",
    "data.batch_size = data.batch_size // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real loss func. Need to test different auxiliary tasks weights: `magnetic_w`, `dipole_w`, `potential`, weights of indivial `lmae`s: `types_w` and maybe `input_transform_w` and `feature_transform_w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:11:41.668633Z",
     "start_time": "2019-08-19T17:11:41.664619Z"
    }
   },
   "outputs": [],
   "source": [
    "type_to_tune = 0\n",
    "types_w = [0] * 8\n",
    "types_w[type_to_tune] = 1\n",
    "\n",
    "learner.loss_func = LMAEMaskedLoss(magnetic_w=0, dipole_w=0, potential_w=0, types_w = types_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:55:31.203434Z",
     "start_time": "2019-08-19T13:53:36.687673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.module.module.module.transformer.layers.0.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.0.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.0.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.0.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.0.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.0.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.0.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.0.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.0.mhra.out.weight\", \"module.module.module.module.transformer.layers.0.mhra.out.bias\", \"module.module.module.module.transformer.layers.0.mhra.ln.weight\", \"module.module.module.module.transformer.layers.0.mhra.ln.bias\", \"module.module.module.module.transformer.layers.0.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.0.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.0.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.0.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.0.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.0.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.1.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.1.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.1.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.1.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.1.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.1.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.1.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.1.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.1.mhra.out.weight\", \"module.module.module.module.transformer.layers.1.mhra.out.bias\", \"module.module.module.module.transformer.layers.1.mhra.ln.weight\", \"module.module.module.module.transformer.layers.1.mhra.ln.bias\", \"module.module.module.module.transformer.layers.1.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.1.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.1.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.1.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.1.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.1.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.2.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.2.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.2.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.2.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.2.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.2.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.2.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.2.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.2.mhra.out.weight\", \"module.module.module.module.transformer.layers.2.mhra.out.bias\", \"module.module.module.module.transformer.layers.2.mhra.ln.weight\", \"module.module.module.module.transformer.layers.2.mhra.ln.bias\", \"module.module.module.module.transformer.layers.2.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.2.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.2.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.2.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.2.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.2.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.3.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.3.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.3.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.3.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.3.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.3.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.3.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.3.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.3.mhra.out.weight\", \"module.module.module.module.transformer.layers.3.mhra.out.bias\", \"module.module.module.module.transformer.layers.3.mhra.ln.weight\", \"module.module.module.module.transformer.layers.3.mhra.ln.bias\", \"module.module.module.module.transformer.layers.3.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.3.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.3.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.3.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.3.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.3.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.4.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.4.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.4.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.4.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.4.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.4.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.4.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.4.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.4.mhra.out.weight\", \"module.module.module.module.transformer.layers.4.mhra.out.bias\", \"module.module.module.module.transformer.layers.4.mhra.ln.weight\", \"module.module.module.module.transformer.layers.4.mhra.ln.bias\", \"module.module.module.module.transformer.layers.4.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.4.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.4.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.4.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.4.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.4.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.5.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.5.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.5.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.5.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.5.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.5.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.5.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.5.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.5.mhra.out.weight\", \"module.module.module.module.transformer.layers.5.mhra.out.bias\", \"module.module.module.module.transformer.layers.5.mhra.ln.weight\", \"module.module.module.module.transformer.layers.5.mhra.ln.bias\", \"module.module.module.module.transformer.layers.5.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.5.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.5.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.5.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.5.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.5.ff.layers.6.bias\", \"module.module.module.module.scalar.weight\", \"module.module.module.module.scalar.bias\", \"module.module.module.module.magnetic.weight\", \"module.module.module.module.magnetic.bias\", \"module.module.module.module.dipole.weight\", \"module.module.module.module.dipole.bias\", \"module.module.module.module.potential.weight\", \"module.module.module.module.potential.bias\", \"module.module.module.module.type_embedding.weight\", \"module.module.module.module.atom_embedding.weight\". \n\tUnexpected key(s) in state_dict: \"module.module.module.module.module.module.transformer.layers.0.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.6.bias\", \"module.module.module.module.module.module.scalar.weight\", \"module.module.module.module.module.module.scalar.bias\", \"module.module.module.module.module.module.magnetic.weight\", \"module.module.module.module.module.module.magnetic.bias\", \"module.module.module.module.module.module.dipole.weight\", \"module.module.module.module.module.module.dipole.bias\", \"module.module.module.module.module.module.potential.weight\", \"module.module.module.module.module.module.potential.bias\", \"module.module.module.module.module.module.type_embedding.weight\", \"module.module.module.module.module.module.atom_embedding.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 152, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 162, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in parallel_apply\n    output.reraise()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 369, in reraise\n    raise self.exc_type(msg)\nRuntimeError: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 152, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 162, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in parallel_apply\n    output.reraise()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 369, in reraise\n    raise self.exc_type(msg)\nRuntimeError: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 152, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 162, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in parallel_apply\n    output.reraise()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 369, in reraise\n    raise self.exc_type(msg)\nRuntimeError: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 152, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 162, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in parallel_apply\n    output.reraise()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 369, in reraise\n    raise self.exc_type(msg)\nRuntimeError: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 547, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 146, in forward\n    \"them on device: {}\".format(self.src_device_obj, t.device))\nRuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:1\n\n\n\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-508b4339e832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#learner.data.batch_size = 512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#learner.opt_func = RAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuggestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m loss_func_name2activ = {'cross_entropy_loss': F.softmax, 'nll_loss': torch.exp, 'poisson_nll_loss': torch.exp,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, exception)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;34m\"Handle end of training, `exception` is an `Exception` or False if no exceptions during training.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name, call_mets, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36m_call_and_update\u001b[0;34m(self, cb, cb_name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"Call `cb_name` on `cb` and update the inner state.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/callbacks/lr_finder.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"Cleanup learn model weights disturbed during LRFinder exploration.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, file, device, strict, with_opt, purge, remove_module)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremove_module\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_module_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b855ac69440d>\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    174\u001b[0m         raise RuntimeError(\n\u001b[1;32m    175\u001b[0m             'Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 176\u001b[0;31m                 self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.module.module.module.transformer.layers.0.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.0.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.0.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.0.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.0.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.0.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.0.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.0.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.0.mhra.out.weight\", \"module.module.module.module.transformer.layers.0.mhra.out.bias\", \"module.module.module.module.transformer.layers.0.mhra.ln.weight\", \"module.module.module.module.transformer.layers.0.mhra.ln.bias\", \"module.module.module.module.transformer.layers.0.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.0.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.0.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.0.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.0.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.0.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.1.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.1.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.1.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.1.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.1.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.1.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.1.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.1.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.1.mhra.out.weight\", \"module.module.module.module.transformer.layers.1.mhra.out.bias\", \"module.module.module.module.transformer.layers.1.mhra.ln.weight\", \"module.module.module.module.transformer.layers.1.mhra.ln.bias\", \"module.module.module.module.transformer.layers.1.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.1.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.1.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.1.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.1.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.1.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.2.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.2.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.2.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.2.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.2.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.2.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.2.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.2.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.2.mhra.out.weight\", \"module.module.module.module.transformer.layers.2.mhra.out.bias\", \"module.module.module.module.transformer.layers.2.mhra.ln.weight\", \"module.module.module.module.transformer.layers.2.mhra.ln.bias\", \"module.module.module.module.transformer.layers.2.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.2.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.2.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.2.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.2.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.2.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.3.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.3.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.3.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.3.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.3.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.3.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.3.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.3.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.3.mhra.out.weight\", \"module.module.module.module.transformer.layers.3.mhra.out.bias\", \"module.module.module.module.transformer.layers.3.mhra.ln.weight\", \"module.module.module.module.transformer.layers.3.mhra.ln.bias\", \"module.module.module.module.transformer.layers.3.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.3.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.3.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.3.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.3.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.3.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.4.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.4.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.4.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.4.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.4.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.4.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.4.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.4.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.4.mhra.out.weight\", \"module.module.module.module.transformer.layers.4.mhra.out.bias\", \"module.module.module.module.transformer.layers.4.mhra.ln.weight\", \"module.module.module.module.transformer.layers.4.mhra.ln.bias\", \"module.module.module.module.transformer.layers.4.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.4.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.4.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.4.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.4.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.4.ff.layers.6.bias\", \"module.module.module.module.transformer.layers.5.mhra.attention1.weight\", \"module.module.module.module.transformer.layers.5.mhra.attention1.bias\", \"module.module.module.module.transformer.layers.5.mhra.attention2.weight\", \"module.module.module.module.transformer.layers.5.mhra.attention2.bias\", \"module.module.module.module.transformer.layers.5.mhra.attention3.weight\", \"module.module.module.module.transformer.layers.5.mhra.attention3.bias\", \"module.module.module.module.transformer.layers.5.mhra.attention4.weight\", \"module.module.module.module.transformer.layers.5.mhra.attention4.bias\", \"module.module.module.module.transformer.layers.5.mhra.out.weight\", \"module.module.module.module.transformer.layers.5.mhra.out.bias\", \"module.module.module.module.transformer.layers.5.mhra.ln.weight\", \"module.module.module.module.transformer.layers.5.mhra.ln.bias\", \"module.module.module.module.transformer.layers.5.ff.layers.0.weight\", \"module.module.module.module.transformer.layers.5.ff.layers.0.bias\", \"module.module.module.module.transformer.layers.5.ff.layers.3.weight\", \"module.module.module.module.transformer.layers.5.ff.layers.3.bias\", \"module.module.module.module.transformer.layers.5.ff.layers.6.weight\", \"module.module.module.module.transformer.layers.5.ff.layers.6.bias\", \"module.module.module.module.scalar.weight\", \"module.module.module.module.scalar.bias\", \"module.module.module.module.magnetic.weight\", \"module.module.module.module.magnetic.bias\", \"module.module.module.module.dipole.weight\", \"module.module.module.module.dipole.bias\", \"module.module.module.module.potential.weight\", \"module.module.module.module.potential.bias\", \"module.module.module.module.type_embedding.weight\", \"module.module.module.module.atom_embedding.weight\". \n\tUnexpected key(s) in state_dict: \"module.module.module.module.module.module.transformer.layers.0.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.0.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.0.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.0.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.1.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.1.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.1.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.2.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.2.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.2.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.3.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.3.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.3.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.4.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.4.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.4.ff.layers.6.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention1.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention1.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention2.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention2.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention3.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention3.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention4.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.attention4.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.out.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.out.bias\", \"module.module.module.module.module.module.transformer.layers.5.mhra.ln.weight\", \"module.module.module.module.module.module.transformer.layers.5.mhra.ln.bias\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.0.weight\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.0.bias\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.3.weight\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.3.bias\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.6.weight\", \"module.module.module.module.module.module.transformer.layers.5.ff.layers.6.bias\", \"module.module.module.module.module.module.scalar.weight\", \"module.module.module.module.module.module.scalar.bias\", \"module.module.module.module.module.module.magnetic.weight\", \"module.module.module.module.module.module.magnetic.bias\", \"module.module.module.module.module.module.dipole.weight\", \"module.module.module.module.module.module.dipole.bias\", \"module.module.module.module.module.module.potential.weight\", \"module.module.module.module.module.module.potential.bias\", \"module.module.module.module.module.module.type_embedding.weight\", \"module.module.module.module.module.module.atom_embedding.weight\". "
     ]
    }
   ],
   "source": [
    "#learner.data.batch_size = 512\n",
    "#learner.opt_func = RAdam\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowGraph(LearnerCallback):\n",
    "    \"Update a graph of learner stats and metrics after each epoch.\"\n",
    "    def on_epoch_end(self, n_epochs:int, last_metrics:MetricsList, **kwargs)->bool:\n",
    "        \"If we have `last_metrics` plot them in our pbar graph\"\n",
    "        if last_metrics is not None and last_metrics[0] is not None:\n",
    "            rec = self.learn.recorder\n",
    "            iters = range_of(rec.losses)\n",
    "            val_iter = np.array(rec.nb_batches).cumsum()\n",
    "            x_bounds = (0, (n_epochs - len(rec.nb_batches)) * rec.nb_batches[-1] + len(rec.losses))\n",
    "            y_bounds = (min((min(Tensor(rec.losses)), min(Tensor(rec.val_losses)))), \n",
    "                        max((max(Tensor(rec.losses)), max(Tensor(rec.val_losses)))))\n",
    "            rec.pbar.update_graph([(iters, rec.losses), (val_iter, rec.val_losses)], x_bounds, y_bounds)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T17:12:05.721975Z",
     "start_time": "2019-08-19T17:12:05.714116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False),\n",
       " __main__.ShowGraph]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.callback_fns.append(ShowGraph)\n",
    "learner.callback_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:21:04.766711Z",
     "start_time": "2019-08-19T17:12:17.245500Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>ðŸ‘‰ðŸ»LMAEðŸ‘ˆðŸ»</th>\n",
       "      <th>lmae0</th>\n",
       "      <th>lmae1</th>\n",
       "      <th>lmae2</th>\n",
       "      <th>lmae3</th>\n",
       "      <th>lmae4</th>\n",
       "      <th>lmae5</th>\n",
       "      <th>lmae6</th>\n",
       "      <th>lmae7</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.368205</td>\n",
       "      <td>-0.265009</td>\n",
       "      <td>-3.070257</td>\n",
       "      <td>-2.059542</td>\n",
       "      <td>-3.525609</td>\n",
       "      <td>-2.194268</td>\n",
       "      <td>-3.496864</td>\n",
       "      <td>-3.074673</td>\n",
       "      <td>-3.598289</td>\n",
       "      <td>-2.936985</td>\n",
       "      <td>-3.675822</td>\n",
       "      <td>10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.370098</td>\n",
       "      <td>-0.265422</td>\n",
       "      <td>-3.067103</td>\n",
       "      <td>-2.062604</td>\n",
       "      <td>-3.523842</td>\n",
       "      <td>-2.192281</td>\n",
       "      <td>-3.493525</td>\n",
       "      <td>-3.072083</td>\n",
       "      <td>-3.588431</td>\n",
       "      <td>-2.935217</td>\n",
       "      <td>-3.668843</td>\n",
       "      <td>10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.371664</td>\n",
       "      <td>-0.265457</td>\n",
       "      <td>-3.061227</td>\n",
       "      <td>-2.062894</td>\n",
       "      <td>-3.520267</td>\n",
       "      <td>-2.191175</td>\n",
       "      <td>-3.486282</td>\n",
       "      <td>-3.067727</td>\n",
       "      <td>-3.574609</td>\n",
       "      <td>-2.929765</td>\n",
       "      <td>-3.657095</td>\n",
       "      <td>10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.371654</td>\n",
       "      <td>-0.265423</td>\n",
       "      <td>-3.050684</td>\n",
       "      <td>-2.062681</td>\n",
       "      <td>-3.506853</td>\n",
       "      <td>-2.187631</td>\n",
       "      <td>-3.475091</td>\n",
       "      <td>-3.059050</td>\n",
       "      <td>-3.546837</td>\n",
       "      <td>-2.922425</td>\n",
       "      <td>-3.644907</td>\n",
       "      <td>10:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.371896</td>\n",
       "      <td>-0.265658</td>\n",
       "      <td>-3.041956</td>\n",
       "      <td>-2.064696</td>\n",
       "      <td>-3.499897</td>\n",
       "      <td>-2.181445</td>\n",
       "      <td>-3.463601</td>\n",
       "      <td>-3.055296</td>\n",
       "      <td>-3.522943</td>\n",
       "      <td>-2.914712</td>\n",
       "      <td>-3.633056</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.371614</td>\n",
       "      <td>-0.265547</td>\n",
       "      <td>-3.026036</td>\n",
       "      <td>-2.063804</td>\n",
       "      <td>-3.492857</td>\n",
       "      <td>-2.172650</td>\n",
       "      <td>-3.443316</td>\n",
       "      <td>-3.036859</td>\n",
       "      <td>-3.494789</td>\n",
       "      <td>-2.903424</td>\n",
       "      <td>-3.600594</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-0.371825</td>\n",
       "      <td>-0.265491</td>\n",
       "      <td>-3.008316</td>\n",
       "      <td>-2.064359</td>\n",
       "      <td>-3.481717</td>\n",
       "      <td>-2.166218</td>\n",
       "      <td>-3.416761</td>\n",
       "      <td>-3.019091</td>\n",
       "      <td>-3.474164</td>\n",
       "      <td>-2.892154</td>\n",
       "      <td>-3.552061</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-0.371760</td>\n",
       "      <td>-0.265767</td>\n",
       "      <td>-2.992669</td>\n",
       "      <td>-2.065162</td>\n",
       "      <td>-3.473461</td>\n",
       "      <td>-2.162849</td>\n",
       "      <td>-3.396500</td>\n",
       "      <td>-3.012052</td>\n",
       "      <td>-3.431575</td>\n",
       "      <td>-2.874861</td>\n",
       "      <td>-3.524893</td>\n",
       "      <td>11:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-0.372175</td>\n",
       "      <td>-0.265991</td>\n",
       "      <td>-2.966217</td>\n",
       "      <td>-2.066994</td>\n",
       "      <td>-3.467800</td>\n",
       "      <td>-2.154428</td>\n",
       "      <td>-3.347142</td>\n",
       "      <td>-2.984940</td>\n",
       "      <td>-3.379923</td>\n",
       "      <td>-2.861067</td>\n",
       "      <td>-3.467441</td>\n",
       "      <td>10:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-0.372567</td>\n",
       "      <td>-0.265654</td>\n",
       "      <td>-2.941584</td>\n",
       "      <td>-2.064903</td>\n",
       "      <td>-3.455447</td>\n",
       "      <td>-2.146606</td>\n",
       "      <td>-3.308198</td>\n",
       "      <td>-2.951147</td>\n",
       "      <td>-3.346131</td>\n",
       "      <td>-2.833051</td>\n",
       "      <td>-3.427184</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>-0.372734</td>\n",
       "      <td>-0.264349</td>\n",
       "      <td>-2.893712</td>\n",
       "      <td>-2.057306</td>\n",
       "      <td>-3.432577</td>\n",
       "      <td>-2.118068</td>\n",
       "      <td>-3.259034</td>\n",
       "      <td>-2.915148</td>\n",
       "      <td>-3.242242</td>\n",
       "      <td>-2.793381</td>\n",
       "      <td>-3.331939</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>-0.371993</td>\n",
       "      <td>-0.265373</td>\n",
       "      <td>-2.881733</td>\n",
       "      <td>-2.064271</td>\n",
       "      <td>-3.420724</td>\n",
       "      <td>-2.125378</td>\n",
       "      <td>-3.235524</td>\n",
       "      <td>-2.891002</td>\n",
       "      <td>-3.215152</td>\n",
       "      <td>-2.788142</td>\n",
       "      <td>-3.313671</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-0.371456</td>\n",
       "      <td>-0.263112</td>\n",
       "      <td>-2.833621</td>\n",
       "      <td>-2.048687</td>\n",
       "      <td>-3.412986</td>\n",
       "      <td>-2.085052</td>\n",
       "      <td>-3.161645</td>\n",
       "      <td>-2.846588</td>\n",
       "      <td>-3.140884</td>\n",
       "      <td>-2.762703</td>\n",
       "      <td>-3.210418</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-0.375498</td>\n",
       "      <td>-0.266297</td>\n",
       "      <td>-2.817510</td>\n",
       "      <td>-2.070059</td>\n",
       "      <td>-3.395048</td>\n",
       "      <td>-2.080632</td>\n",
       "      <td>-3.116020</td>\n",
       "      <td>-2.806620</td>\n",
       "      <td>-3.132774</td>\n",
       "      <td>-2.752046</td>\n",
       "      <td>-3.186883</td>\n",
       "      <td>10:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>-0.378338</td>\n",
       "      <td>-0.266188</td>\n",
       "      <td>-2.790624</td>\n",
       "      <td>-2.069860</td>\n",
       "      <td>-3.390692</td>\n",
       "      <td>-2.063243</td>\n",
       "      <td>-3.091764</td>\n",
       "      <td>-2.783228</td>\n",
       "      <td>-3.058028</td>\n",
       "      <td>-2.726763</td>\n",
       "      <td>-3.141416</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-0.381333</td>\n",
       "      <td>-0.266210</td>\n",
       "      <td>-2.756670</td>\n",
       "      <td>-2.069641</td>\n",
       "      <td>-3.374017</td>\n",
       "      <td>-2.059467</td>\n",
       "      <td>-3.039838</td>\n",
       "      <td>-2.740160</td>\n",
       "      <td>-3.013819</td>\n",
       "      <td>-2.688238</td>\n",
       "      <td>-3.068182</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>-0.384826</td>\n",
       "      <td>-0.267130</td>\n",
       "      <td>-2.733960</td>\n",
       "      <td>-2.076203</td>\n",
       "      <td>-3.364602</td>\n",
       "      <td>-2.038746</td>\n",
       "      <td>-2.991597</td>\n",
       "      <td>-2.701174</td>\n",
       "      <td>-3.007575</td>\n",
       "      <td>-2.665655</td>\n",
       "      <td>-3.026130</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-0.387353</td>\n",
       "      <td>-0.267719</td>\n",
       "      <td>-2.705751</td>\n",
       "      <td>-2.080638</td>\n",
       "      <td>-3.360557</td>\n",
       "      <td>-2.031781</td>\n",
       "      <td>-2.948220</td>\n",
       "      <td>-2.671653</td>\n",
       "      <td>-2.930502</td>\n",
       "      <td>-2.647448</td>\n",
       "      <td>-2.975206</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>-0.388501</td>\n",
       "      <td>-0.268890</td>\n",
       "      <td>-2.689384</td>\n",
       "      <td>-2.088769</td>\n",
       "      <td>-3.353468</td>\n",
       "      <td>-2.013618</td>\n",
       "      <td>-2.931528</td>\n",
       "      <td>-2.656177</td>\n",
       "      <td>-2.902777</td>\n",
       "      <td>-2.626622</td>\n",
       "      <td>-2.942109</td>\n",
       "      <td>10:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>-0.392743</td>\n",
       "      <td>-0.267699</td>\n",
       "      <td>-2.649170</td>\n",
       "      <td>-2.080834</td>\n",
       "      <td>-3.335644</td>\n",
       "      <td>-1.990246</td>\n",
       "      <td>-2.870818</td>\n",
       "      <td>-2.599560</td>\n",
       "      <td>-2.840316</td>\n",
       "      <td>-2.595216</td>\n",
       "      <td>-2.880724</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-0.394294</td>\n",
       "      <td>-0.269136</td>\n",
       "      <td>-2.637183</td>\n",
       "      <td>-2.090688</td>\n",
       "      <td>-3.333668</td>\n",
       "      <td>-2.008828</td>\n",
       "      <td>-2.843610</td>\n",
       "      <td>-2.581531</td>\n",
       "      <td>-2.825768</td>\n",
       "      <td>-2.573986</td>\n",
       "      <td>-2.839389</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>-0.398576</td>\n",
       "      <td>-0.267898</td>\n",
       "      <td>-2.603328</td>\n",
       "      <td>-2.082288</td>\n",
       "      <td>-3.320744</td>\n",
       "      <td>-1.992305</td>\n",
       "      <td>-2.801839</td>\n",
       "      <td>-2.527305</td>\n",
       "      <td>-2.766346</td>\n",
       "      <td>-2.548265</td>\n",
       "      <td>-2.787527</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>-0.401679</td>\n",
       "      <td>-0.269770</td>\n",
       "      <td>-2.585443</td>\n",
       "      <td>-2.095527</td>\n",
       "      <td>-3.317733</td>\n",
       "      <td>-1.962282</td>\n",
       "      <td>-2.757658</td>\n",
       "      <td>-2.505686</td>\n",
       "      <td>-2.753899</td>\n",
       "      <td>-2.530736</td>\n",
       "      <td>-2.760026</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>-0.405242</td>\n",
       "      <td>-0.269977</td>\n",
       "      <td>-2.569508</td>\n",
       "      <td>-2.096531</td>\n",
       "      <td>-3.304975</td>\n",
       "      <td>-1.959563</td>\n",
       "      <td>-2.761567</td>\n",
       "      <td>-2.477494</td>\n",
       "      <td>-2.715636</td>\n",
       "      <td>-2.502079</td>\n",
       "      <td>-2.738221</td>\n",
       "      <td>10:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>-0.408047</td>\n",
       "      <td>-0.270522</td>\n",
       "      <td>-2.550794</td>\n",
       "      <td>-2.100207</td>\n",
       "      <td>-3.296892</td>\n",
       "      <td>-1.937498</td>\n",
       "      <td>-2.716947</td>\n",
       "      <td>-2.446567</td>\n",
       "      <td>-2.706871</td>\n",
       "      <td>-2.504452</td>\n",
       "      <td>-2.696919</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>-0.411391</td>\n",
       "      <td>-0.271063</td>\n",
       "      <td>-2.546096</td>\n",
       "      <td>-2.103774</td>\n",
       "      <td>-3.300256</td>\n",
       "      <td>-1.946489</td>\n",
       "      <td>-2.712092</td>\n",
       "      <td>-2.459535</td>\n",
       "      <td>-2.683095</td>\n",
       "      <td>-2.479912</td>\n",
       "      <td>-2.683613</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>-0.415578</td>\n",
       "      <td>-0.270477</td>\n",
       "      <td>-2.512628</td>\n",
       "      <td>-2.100878</td>\n",
       "      <td>-3.290040</td>\n",
       "      <td>-1.925277</td>\n",
       "      <td>-2.657824</td>\n",
       "      <td>-2.419552</td>\n",
       "      <td>-2.629958</td>\n",
       "      <td>-2.442281</td>\n",
       "      <td>-2.635216</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>-0.418766</td>\n",
       "      <td>-0.271857</td>\n",
       "      <td>-2.513408</td>\n",
       "      <td>-2.109485</td>\n",
       "      <td>-3.286416</td>\n",
       "      <td>-1.942059</td>\n",
       "      <td>-2.653107</td>\n",
       "      <td>-2.391367</td>\n",
       "      <td>-2.640345</td>\n",
       "      <td>-2.456140</td>\n",
       "      <td>-2.628343</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>-0.424398</td>\n",
       "      <td>-0.272516</td>\n",
       "      <td>-2.497135</td>\n",
       "      <td>-2.113851</td>\n",
       "      <td>-3.272410</td>\n",
       "      <td>-1.939016</td>\n",
       "      <td>-2.633145</td>\n",
       "      <td>-2.389922</td>\n",
       "      <td>-2.605267</td>\n",
       "      <td>-2.425072</td>\n",
       "      <td>-2.598394</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>-0.425735</td>\n",
       "      <td>-0.271322</td>\n",
       "      <td>-2.476510</td>\n",
       "      <td>-2.107031</td>\n",
       "      <td>-3.272484</td>\n",
       "      <td>-1.922689</td>\n",
       "      <td>-2.606190</td>\n",
       "      <td>-2.359721</td>\n",
       "      <td>-2.576778</td>\n",
       "      <td>-2.403232</td>\n",
       "      <td>-2.563956</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>-0.431090</td>\n",
       "      <td>-0.273038</td>\n",
       "      <td>-2.469249</td>\n",
       "      <td>-2.117459</td>\n",
       "      <td>-3.265134</td>\n",
       "      <td>-1.936470</td>\n",
       "      <td>-2.584580</td>\n",
       "      <td>-2.336683</td>\n",
       "      <td>-2.566170</td>\n",
       "      <td>-2.397360</td>\n",
       "      <td>-2.550138</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>-0.435238</td>\n",
       "      <td>-0.273189</td>\n",
       "      <td>-2.461111</td>\n",
       "      <td>-2.118674</td>\n",
       "      <td>-3.260444</td>\n",
       "      <td>-1.937697</td>\n",
       "      <td>-2.571096</td>\n",
       "      <td>-2.317363</td>\n",
       "      <td>-2.559641</td>\n",
       "      <td>-2.389924</td>\n",
       "      <td>-2.534045</td>\n",
       "      <td>10:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>-0.440820</td>\n",
       "      <td>-0.273509</td>\n",
       "      <td>-2.446296</td>\n",
       "      <td>-2.120710</td>\n",
       "      <td>-3.248840</td>\n",
       "      <td>-1.937520</td>\n",
       "      <td>-2.556147</td>\n",
       "      <td>-2.295588</td>\n",
       "      <td>-2.539020</td>\n",
       "      <td>-2.366861</td>\n",
       "      <td>-2.505686</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>-0.443714</td>\n",
       "      <td>-0.273712</td>\n",
       "      <td>-2.440122</td>\n",
       "      <td>-2.122214</td>\n",
       "      <td>-3.244734</td>\n",
       "      <td>-1.945649</td>\n",
       "      <td>-2.540812</td>\n",
       "      <td>-2.282323</td>\n",
       "      <td>-2.529567</td>\n",
       "      <td>-2.361850</td>\n",
       "      <td>-2.493825</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>-0.448762</td>\n",
       "      <td>-0.273762</td>\n",
       "      <td>-2.436435</td>\n",
       "      <td>-2.122238</td>\n",
       "      <td>-3.231731</td>\n",
       "      <td>-1.956522</td>\n",
       "      <td>-2.534285</td>\n",
       "      <td>-2.278210</td>\n",
       "      <td>-2.527309</td>\n",
       "      <td>-2.358145</td>\n",
       "      <td>-2.483039</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>-0.453459</td>\n",
       "      <td>-0.274266</td>\n",
       "      <td>-2.424106</td>\n",
       "      <td>-2.125947</td>\n",
       "      <td>-3.229135</td>\n",
       "      <td>-1.959420</td>\n",
       "      <td>-2.513107</td>\n",
       "      <td>-2.255847</td>\n",
       "      <td>-2.504875</td>\n",
       "      <td>-2.343356</td>\n",
       "      <td>-2.461161</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>-0.456624</td>\n",
       "      <td>-0.274423</td>\n",
       "      <td>-2.416775</td>\n",
       "      <td>-2.126658</td>\n",
       "      <td>-3.214153</td>\n",
       "      <td>-1.963030</td>\n",
       "      <td>-2.503460</td>\n",
       "      <td>-2.243876</td>\n",
       "      <td>-2.497874</td>\n",
       "      <td>-2.337560</td>\n",
       "      <td>-2.447596</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>-0.463068</td>\n",
       "      <td>-0.274569</td>\n",
       "      <td>-2.413081</td>\n",
       "      <td>-2.127980</td>\n",
       "      <td>-3.212604</td>\n",
       "      <td>-1.963162</td>\n",
       "      <td>-2.499900</td>\n",
       "      <td>-2.238721</td>\n",
       "      <td>-2.491298</td>\n",
       "      <td>-2.329233</td>\n",
       "      <td>-2.441749</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>-0.466390</td>\n",
       "      <td>-0.274783</td>\n",
       "      <td>-2.409973</td>\n",
       "      <td>-2.129044</td>\n",
       "      <td>-3.210397</td>\n",
       "      <td>-1.961923</td>\n",
       "      <td>-2.495990</td>\n",
       "      <td>-2.235699</td>\n",
       "      <td>-2.486553</td>\n",
       "      <td>-2.325589</td>\n",
       "      <td>-2.434585</td>\n",
       "      <td>10:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>-0.471212</td>\n",
       "      <td>-0.274733</td>\n",
       "      <td>-2.407081</td>\n",
       "      <td>-2.128584</td>\n",
       "      <td>-3.207680</td>\n",
       "      <td>-1.961871</td>\n",
       "      <td>-2.489956</td>\n",
       "      <td>-2.227124</td>\n",
       "      <td>-2.482580</td>\n",
       "      <td>-2.325496</td>\n",
       "      <td>-2.433352</td>\n",
       "      <td>10:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>-0.475921</td>\n",
       "      <td>-0.275029</td>\n",
       "      <td>-2.402423</td>\n",
       "      <td>-2.130903</td>\n",
       "      <td>-3.207183</td>\n",
       "      <td>-1.953284</td>\n",
       "      <td>-2.482904</td>\n",
       "      <td>-2.227207</td>\n",
       "      <td>-2.474726</td>\n",
       "      <td>-2.316013</td>\n",
       "      <td>-2.427160</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>-0.478055</td>\n",
       "      <td>-0.275119</td>\n",
       "      <td>-2.399058</td>\n",
       "      <td>-2.131453</td>\n",
       "      <td>-3.205356</td>\n",
       "      <td>-1.947924</td>\n",
       "      <td>-2.478171</td>\n",
       "      <td>-2.223406</td>\n",
       "      <td>-2.468833</td>\n",
       "      <td>-2.316670</td>\n",
       "      <td>-2.420653</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>-0.482729</td>\n",
       "      <td>-0.275170</td>\n",
       "      <td>-2.395883</td>\n",
       "      <td>-2.131843</td>\n",
       "      <td>-3.201841</td>\n",
       "      <td>-1.947563</td>\n",
       "      <td>-2.474594</td>\n",
       "      <td>-2.217848</td>\n",
       "      <td>-2.463302</td>\n",
       "      <td>-2.312492</td>\n",
       "      <td>-2.417579</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>-0.485494</td>\n",
       "      <td>-0.275197</td>\n",
       "      <td>-2.395002</td>\n",
       "      <td>-2.132138</td>\n",
       "      <td>-3.200511</td>\n",
       "      <td>-1.945982</td>\n",
       "      <td>-2.472385</td>\n",
       "      <td>-2.219411</td>\n",
       "      <td>-2.463152</td>\n",
       "      <td>-2.310789</td>\n",
       "      <td>-2.415648</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>-0.488964</td>\n",
       "      <td>-0.275300</td>\n",
       "      <td>-2.395382</td>\n",
       "      <td>-2.132662</td>\n",
       "      <td>-3.200737</td>\n",
       "      <td>-1.946995</td>\n",
       "      <td>-2.473796</td>\n",
       "      <td>-2.219400</td>\n",
       "      <td>-2.463004</td>\n",
       "      <td>-2.310723</td>\n",
       "      <td>-2.415741</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>-0.490858</td>\n",
       "      <td>-0.275316</td>\n",
       "      <td>-2.393965</td>\n",
       "      <td>-2.132856</td>\n",
       "      <td>-3.199714</td>\n",
       "      <td>-1.945510</td>\n",
       "      <td>-2.470078</td>\n",
       "      <td>-2.216721</td>\n",
       "      <td>-2.462373</td>\n",
       "      <td>-2.310090</td>\n",
       "      <td>-2.414376</td>\n",
       "      <td>10:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>-0.491335</td>\n",
       "      <td>-0.275351</td>\n",
       "      <td>-2.393462</td>\n",
       "      <td>-2.132999</td>\n",
       "      <td>-3.198863</td>\n",
       "      <td>-1.945565</td>\n",
       "      <td>-2.470622</td>\n",
       "      <td>-2.217896</td>\n",
       "      <td>-2.459595</td>\n",
       "      <td>-2.308568</td>\n",
       "      <td>-2.413589</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>-0.494819</td>\n",
       "      <td>-0.275354</td>\n",
       "      <td>-2.392784</td>\n",
       "      <td>-2.133081</td>\n",
       "      <td>-3.198380</td>\n",
       "      <td>-1.944618</td>\n",
       "      <td>-2.469775</td>\n",
       "      <td>-2.216777</td>\n",
       "      <td>-2.458715</td>\n",
       "      <td>-2.308281</td>\n",
       "      <td>-2.412642</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>-0.496539</td>\n",
       "      <td>-0.275355</td>\n",
       "      <td>-2.392400</td>\n",
       "      <td>-2.133063</td>\n",
       "      <td>-3.197966</td>\n",
       "      <td>-1.944401</td>\n",
       "      <td>-2.469108</td>\n",
       "      <td>-2.216050</td>\n",
       "      <td>-2.458655</td>\n",
       "      <td>-2.307873</td>\n",
       "      <td>-2.412083</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>-0.497732</td>\n",
       "      <td>-0.275354</td>\n",
       "      <td>-2.392468</td>\n",
       "      <td>-2.133079</td>\n",
       "      <td>-3.197708</td>\n",
       "      <td>-1.944379</td>\n",
       "      <td>-2.469366</td>\n",
       "      <td>-2.216159</td>\n",
       "      <td>-2.458899</td>\n",
       "      <td>-2.307936</td>\n",
       "      <td>-2.412221</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with ðŸ‘‰ðŸ»LMAEðŸ‘ˆðŸ» value: -3.0702567100524902.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#learner.data.batch_size = 512*2\n",
    "#learner.data.batch_size = 512\n",
    "#learner.opt_func = RAdam\n",
    "#learner.opt_func = partial(torch.optim.Adam, betas=(0.9,0.99), eps=1e-2)\n",
    "learner.fit_one_cycle(50, 1e-5, start_epoch=0)#, moms=(0.75,0.70))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:21:05.284490Z",
     "start_time": "2019-08-21T02:21:04.781955Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:21:06.711105Z",
     "start_time": "2019-08-21T02:21:05.287164Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:21:08.400771Z",
     "start_time": "2019-08-21T02:21:06.713776Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(500, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T11:15:02.866857Z",
     "start_time": "2019-08-15T11:15:02.810887Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Fine tune regular fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:26:43.041291Z",
     "start_time": "2019-08-21T03:26:43.034173Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.callbacks.append(\n",
    "    ReduceLROnPlateauCallback(learner, monitor='train_loss', mode='min', factor=0.2, patience=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T06:19:15.311297Z",
     "start_time": "2019-08-21T03:27:35.491980Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#learner.opt_func = RAdam\n",
    "learner.fit(50, 1e-3)# , moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(300, 5e-4)#, moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-10T11:24:45.530739Z",
     "start_time": "2019-08-10T11:24:44.737186Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T16:57:30.084191Z",
     "start_time": "2019-08-16T16:57:29.686756Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T16:57:43.396559Z",
     "start_time": "2019-08-16T16:57:41.386234Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(10,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:22:55.624674Z",
     "start_time": "2019-08-21T02:21:08.404032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2753544, tensor(-2.3925, device='cuda:0'), tensor(-2.1331, device='cuda:0'), tensor(-3.1977, device='cuda:0'), tensor(-1.9444, device='cuda:0'), tensor(-2.4694, device='cuda:0'), tensor(-2.2162, device='cuda:0'), tensor(-2.4589, device='cuda:0'), tensor(-2.3079, device='cuda:0'), tensor(-2.4122, device='cuda:0')] tensor(-2.1331, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "learner.to_fp32()\n",
    "\n",
    "val = learner.validate()\n",
    "val_lmae = val[2+type_to_tune]\n",
    "print(val, val_lmae)\n",
    "val = val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:22:56.164688Z",
     "start_time": "2019-08-21T02:22:55.627089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val-2.3925_lmae0-2.1331\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sub_fname = f'loss{learner.recorder.losses[-1]:.04f}val{val:.04f}_lmae{type_to_tune}{val_lmae:.04f}'\n",
    "except Exception as e:\n",
    "    sub_fname = f'val{val:.04f}_lmae{type_to_tune}{val_lmae:.04f}'\n",
    "learner.save(sub_fname)\n",
    "print(sub_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:22:59.667874Z",
     "start_time": "2019-08-21T02:22:56.166810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2941"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Make sure `tranforms` are activated to test set otherwise TTA > 1 will be as TTA =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:23:02.472345Z",
     "start_time": "2019-08-21T02:22:59.671773Z"
    }
   },
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile  = np.load(fname_ext(test_fname, ext))\n",
    "    xt_xyz   = npzfile['x_xyz']\n",
    "    xt_type  = npzfile['x_type']\n",
    "    xt_ext   = npzfile['x_ext']\n",
    "    xt_atom  = npzfile['x_atom']\n",
    "    mt = npzfile['m']\n",
    "    xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    xt_xyz,xt_type,xt_ext,xt_atom,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index=types,ext=ext)\n",
    "    np.savez(fname_ext('_'+test_fname, ext), \n",
    "             x_xyz  = xt_xyz,\n",
    "             x_type = xt_type,\n",
    "             x_ext  = xt_ext,\n",
    "             x_atom = xt_atom,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:23:02.480589Z",
     "start_time": "2019-08-21T02:23:02.474709Z"
    }
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    try:\n",
    "        xt_coulombmat = load_fn(f'xt_coulombmat32{ext}.npy')\n",
    "    except:\n",
    "        xt_coulombmat = np.load(f'xt_coulombmat{ext}.npy', allow_pickle=True)\n",
    "        xt_coulombmat = np.array(xt_coulombmat.tolist()).astype(np.float32)\n",
    "        np.save(f'xt_coulombmat32{ext}.npy', xt_coulombmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:23:02.487954Z",
     "start_time": "2019-08-21T02:23:02.482416Z"
    }
   },
   "outputs": [],
   "source": [
    "xt_qm9_mulliken = load_fn(f'xt_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:23:02.493831Z",
     "start_time": "2019-08-21T02:23:02.490149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xt_qm9_mulliken_ext.npy'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'xt_qm9_mulliken{ext}.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:23:02.501444Z",
     "start_time": "2019-08-21T02:23:02.495993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(756113, 3, 29),\n",
       " (756113, 1, 29),\n",
       " (756113, 1, 29),\n",
       " (756113, 1, 29),\n",
       " (756113, 1, 29),\n",
       " (756113, 29),\n",
       " (756113,)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [xt_xyz,xt_type,xt_ext,xt_atom, xt_qm9_mulliken,xt_ids, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data.add_test(ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                              enumerate(zip(xt_xyz,xt_type,xt_ext,xt_atom,xt_qm9_mulliken,xt_coulombmat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:23:50.865687Z",
     "start_time": "2019-08-21T02:23:46.400798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del xt_xyz,xt_type,xt_ext,xt_atom, xt_qm9_mulliken, mt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTA_N = 1\n",
    "learner.data.test_ds.tfms = tta_tfms if TTA_N > 1 else tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:33:49.692070Z",
     "start_time": "2019-08-21T02:23:50.867913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4658147</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4658148</td>\n",
       "      <td>198.874146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4658149</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4658150</td>\n",
       "      <td>198.831512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4658151</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147                  0.000000\n",
       "1  4658148                198.874146\n",
       "2  4658149                  0.000000\n",
       "3  4658150                198.831512\n",
       "4  4658151                  0.000000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = defaultdict(int)\n",
    "xt_ids_not_extended = (xt_ids!=0) & (xt_ids<=7163688) # TODO\n",
    "ids = xt_ids[xt_ids_not_extended]\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n",
    "        types_, _, preds_,_,_,_ = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "        preds_ = preds_.sum(dim=1)\n",
    "        preds_[types_.squeeze(1)!=type_to_tune] = 0.\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "    preds = test_preds[xt_ids_not_extended]\n",
    "    for k in range(len(ids)):\n",
    "        sub[int(ids[k])] += preds[k]\n",
    "    \n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = sub[int(ids[k])]/TTA_N\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(sub_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'val-2.3925_lmae0-2.1331_val'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.data.valid_dl = data.valid_dl.new(shuffle=False)\n",
    "TTA_N =1                                          \n",
    "sub = defaultdict(int)\n",
    "targets = defaultdict(int)\n",
    "targets_types = defaultdict(int)\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_exts = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_types = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_targets = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Valid)), total=len(learner.dl(DatasetType.Valid)), parent=mb):\n",
    "\n",
    "        types_, ext_, preds_,_,_,_ = learner.pred_batch(ds_type=DatasetType.Valid, batch=batch)\n",
    "        preds_ = preds_.sum(dim=1)\n",
    "        preds_[types_.squeeze(1)!=type_to_tune] = 0.\n",
    "\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "        targets_ = batch[1][0].sum(dim=1)\n",
    "        test_targets = np.concatenate([test_targets, targets_.data.cpu().numpy()], axis = 0)\n",
    "        \n",
    "        test_types = np.concatenate([test_types, types_.data.cpu().numpy().squeeze(1)], axis = 0)\n",
    "        test_exts = np.concatenate([test_exts, ext_.data.cpu().numpy().squeeze(1)], axis = 0)\n",
    "    \n",
    "    test_preds = test_preds.flatten()\n",
    "    test_types = test_types.flatten()\n",
    "    test_targets = test_targets.flatten()\n",
    "    test_exts = test_exts.flatten()\n",
    "    \n",
    "    mask = (test_types != -1) & (test_exts == 0) #.squeeze(1)\n",
    "    preds = test_preds[mask]\n",
    "    test_targets = test_targets[mask]\n",
    "    test_types = test_types[mask]\n",
    "    for k in range(len(preds)):\n",
    "        sub[k] += preds[k]\n",
    "        targets[k] = test_targets[k]\n",
    "        targets_types[k] = test_types[k]\n",
    "    \n",
    "for k in range(len(sub.keys())):\n",
    "    sub[k] = sub[k]/TTA_N\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()\n",
    "sub_df.to_csv(sub_fname + '_val', index=False)\n",
    "sub_fname + '_val'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:33:49.698278Z",
     "start_time": "2019-08-21T02:33:49.694339Z"
    }
   },
   "outputs": [],
   "source": [
    "#sub_fname = 'loss-4.9044val-2.5880'\n",
    "#sub_fname='loss-4.9516val-2.8229'\n",
    "sub_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:34:01.489116Z",
     "start_time": "2019-08-21T02:33:49.700328Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(sub_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:34:01.493376Z",
     "start_time": "2019-08-21T02:34:01.490900Z"
    }
   },
   "outputs": [],
   "source": [
    "comp = 'champs-scalar-coupling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:34:01.503188Z",
     "start_time": "2019-08-21T02:34:01.495166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T02:34:01.507189Z",
     "start_time": "2019-08-21T02:34:01.504925Z"
    }
   },
   "outputs": [],
   "source": [
    "ext=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:20:28.337421Z",
     "start_time": "2019-08-21T03:20:22.710973Z"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c {comp} -f {sub_fname} -m 'knockout no QM9 no ext no max_atoms tta {TTA_N} {ext} transformer + attn no scale + 1024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:21:29.884652Z",
     "start_time": "2019-08-21T03:20:28.342603Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "!kaggle competitions submissions -c {comp} -v > submissions-{comp}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:21:29.911396Z",
     "start_time": "2019-08-21T03:21:29.890254Z"
    }
   },
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(f'submissions-{comp}.csv')\n",
    "submissions.iloc[0].publicScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:24:09.681791Z",
     "start_time": "2019-08-21T03:22:20.434837Z"
    }
   },
   "outputs": [],
   "source": [
    "learner.data.valid_dl = data.valid_dl.new(shuffle=False)\n",
    "TTA_N =1                                          \n",
    "sub = defaultdict(int)\n",
    "targets = defaultdict(int)\n",
    "targets_types = defaultdict(int)\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_exts = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_types = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_targets = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Valid)), total=len(learner.dl(DatasetType.Valid)), parent=mb):\n",
    "\n",
    "        types_, ext_, preds_,_,_,_ = learner.pred_batch(ds_type=DatasetType.Valid, batch=batch)\n",
    "        preds_ = preds_.sum(dim=1)\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "        targets_ = batch[1][0].sum(dim=1)\n",
    "        test_targets = np.concatenate([test_targets, targets_.data.cpu().numpy()], axis = 0)\n",
    "        \n",
    "        test_types = np.concatenate([test_types, types_.data.cpu().numpy().squeeze(1)], axis = 0)\n",
    "        test_exts = np.concatenate([test_exts, ext_.data.cpu().numpy().squeeze(1)], axis = 0)\n",
    "    \n",
    "    test_preds = test_preds.flatten()\n",
    "    test_types = test_types.flatten()\n",
    "    test_targets = test_targets.flatten()\n",
    "    test_exts = test_exts.flatten()\n",
    "    \n",
    "    mask = (test_types != -1) & (test_exts == 0) #.squeeze(1)\n",
    "    preds = test_preds[mask]\n",
    "    test_targets = test_targets[mask]\n",
    "    test_types = test_types[mask]\n",
    "    for k in range(len(preds)):\n",
    "        sub[k] += preds[k]\n",
    "        targets[k] = test_targets[k]\n",
    "        targets_types[k] = test_types[k]\n",
    "    \n",
    "for k in range(len(sub.keys())):\n",
    "    sub[k] = sub[k]/TTA_N\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:24:10.021713Z",
     "start_time": "2019-08-21T03:24:09.684318Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_targets = pd.DataFrame(targets.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:24:10.360866Z",
     "start_time": "2019-08-21T03:24:10.024618Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_targets = pd.DataFrame(targets.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:24:10.685807Z",
     "start_time": "2019-08-21T03:24:10.363096Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_targets_types = pd.DataFrame(targets_types.items(), columns=['id', 'type'])\n",
    "sub_targets_types.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T21:42:00.805651Z",
     "start_time": "2019-08-05T21:41:59.063328Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_targets_types.to_csv('validation_types', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:24:10.695479Z",
     "start_time": "2019-08-21T03:24:10.687958Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:24:10.704696Z",
     "start_time": "2019-08-21T03:24:10.697572Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_targets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T20:07:20.048256Z",
     "start_time": "2019-08-03T20:07:20.041023Z"
    }
   },
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T20:09:32.533038Z",
     "start_time": "2019-08-03T20:09:30.294063Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_targets.to_csv('validation_targets', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:24:12.898543Z",
     "start_time": "2019-08-21T03:24:10.706332Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(sub_fname + '_val', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T03:24:12.906168Z",
     "start_time": "2019-08-21T03:24:12.901116Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_fname + '_val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T21:00:08.361139Z",
     "start_time": "2019-08-05T21:00:07.258433Z"
    }
   },
   "outputs": [],
   "source": [
    "!head loss-4.9516val-3.0042_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T20:11:05.245077Z",
     "start_time": "2019-08-03T20:11:04.340447Z"
    }
   },
   "outputs": [],
   "source": [
    "!head loss-5.7552val-2.9950_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T20:11:18.206277Z",
     "start_time": "2019-08-03T20:11:17.307404Z"
    }
   },
   "outputs": [],
   "source": [
    "!head validation_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
