{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.distributed import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(fname, type_index=None):\n",
    "    t  = pd.read_csv(fname)\n",
    "    s  = pd.read_csv('structures.csv')\n",
    "    \n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "\n",
    "    if has_y:\n",
    "        # atom-atom level\n",
    "        # molecule_name,atom_index_0,atom_index_1,type,fc,sd,pso,dso\n",
    "        scalar_couplings = pd.read_csv('scalar_coupling_contributions.csv') # fc,sd,pso,dso\n",
    "\n",
    "        # atom level\n",
    "        # molecule_name,atom_index,XX,YX,ZX,XY,YY,ZY,XZ,YZ,ZZ\n",
    "        magnetic_shielding = pd.read_csv('magnetic_shielding_tensors.csv')\n",
    "        # molecule_name,atom_index,mulliken_charge\n",
    "        mulliken_charges = pd.read_csv('mulliken_charges.csv')\n",
    "\n",
    "        # molecule level\n",
    "        # molecule_name,X,Y,Z\n",
    "        dipole_moments = pd.read_csv('dipole_moments.csv')\n",
    "        # molecule_name,potential_energy\n",
    "        potential_energy = pd.read_csv('potential_energy.csv')\n",
    "\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_index'] = pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_index'] = pd.factorize(t['type'])[0]\n",
    "    s = pd.concat([s,pd.get_dummies(s['atom'])], axis=1)\n",
    "\n",
    "    max_items = 785836 if has_y else 422550\n",
    "    max_atoms = int(s.atom_index.max() + 1)\n",
    "\n",
    "    if has_y:\n",
    "        contributions = ['fc','sd','pso','dso']\n",
    "        magnetic_tensors = ['XX','YX','ZX','XY','YY','ZY','XZ','YZ','ZZ']\n",
    "        XYZ = ['X','Y','Z']\n",
    "    xyz = ['x', 'y', 'z']\n",
    "    a_hot = ['C','F','H','N','O']\n",
    "    \n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_a_hot = np.zeros((max_items,len(a_hot),max_atoms), dtype=np.float32)\n",
    "    x_type  = np.zeros((max_items,1,         max_atoms), dtype=np.float32)\n",
    "\n",
    "    if has_y:\n",
    "        y_scalar   = np.zeros((max_items,len(contributions)   ,max_atoms), dtype=np.float32)\n",
    "        y_magnetic = np.zeros((max_items,len(magnetic_tensors),max_atoms), dtype=np.float32)\n",
    "        y_mulliken = np.zeros((max_items,1                    ,max_atoms), dtype=np.float32)\n",
    "\n",
    "        y_dipole   = np.zeros((max_items,len(XYZ)), dtype=np.float32)\n",
    "        y_potential= np.zeros((max_items,1              ), dtype=np.float32)\n",
    "\n",
    "        y_magnetic[...] = np.nan\n",
    "        y_mulliken[...] = np.nan\n",
    "    else:\n",
    "        xt_ids = np.zeros((max_items, max_atoms), dtype=np.int32)\n",
    "\n",
    "\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    i = j = 0\n",
    "    \n",
    "    for (m_name, m_index) ,m_group in tqdm(t.groupby(['molecule_name', 'molecule_index'])):\n",
    "        ss = s[s.molecule_name==m_name]\n",
    "        n_atoms = len(ss)\n",
    "        if has_y:\n",
    "            magnetic = magnetic_shielding[\n",
    "                    (magnetic_shielding['molecule_name']==m_name)][magnetic_tensors].values.T\n",
    "\n",
    "            mulliken = mulliken_charges[\n",
    "                    (mulliken_charges['molecule_name']==m_name)]['mulliken_charge'].values.T\n",
    "\n",
    "            scs = scalar_couplings[scalar_couplings['molecule_name']==m_name]\n",
    "            \n",
    "            y_dipole[j,:]= dipole_moments[dipole_moments['molecule_name']==m_name][XYZ].values\n",
    "            y_potential[j,:]=potential_energy[\n",
    "                potential_energy['molecule_name']==m_name]['potential_energy'].values\n",
    "        \n",
    "        for a_name,a_group in m_group.groupby('atom_index_0'):\n",
    "            \n",
    "            ref_a = ss[ss['atom_index']==a_name]\n",
    "            \n",
    "            x_xyz[i] = 0.\n",
    "            x_a_hot[i] = ref_a[a_hot].values.T\n",
    "            x_type[i] = -1\n",
    "\n",
    "            x_xyz[i,:,:n_atoms] = (ss[xyz].values-ref_a[xyz].values).T  # xyz \n",
    "            x_a_hot[i,:,:n_atoms] = ss[a_hot].T                  # a_hot\n",
    "            x_type[i,0,a_group['atom_index_1']] = a_group['type_index']  # type \n",
    "            \n",
    "            if has_y:\n",
    "                y_scalar[i,:,a_group['atom_index_1']] = scs[scs['atom_index_0']==a_name][contributions]\n",
    "                y_magnetic[i,:,:n_atoms] = magnetic\n",
    "                y_mulliken[i,:,:n_atoms] = mulliken\n",
    "            else:\n",
    "                xt_ids[i,a_group['atom_index_1']] = a_group['id']  \n",
    "\n",
    "            m[i] = m_index\n",
    "            i+=1\n",
    "        j += 1\n",
    "    assert i == max_items\n",
    "    print(i,max_items)\n",
    "    if has_y:\n",
    "        return x_xyz,x_a_hot,x_type, m , y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential\n",
    "    else:\n",
    "        return x_xyz,x_a_hot,x_type, m, xt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = Path('train.npz')\n",
    "try:\n",
    "    npzfile = np.load(train_fname)\n",
    "    x_xyz = npzfile['x_xyz']\n",
    "    x_a_hot = npzfile['x_a_hot']\n",
    "    x_type = npzfile['x_type']\n",
    "    y_scalar = npzfile['y_scalar']\n",
    "    y_magnetic = npzfile['y_magnetic']\n",
    "    y_mulliken = npzfile['y_mulliken']\n",
    "    y_dipole = npzfile['y_dipole']\n",
    "    y_potential = npzfile['y_potential']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_a_hot,x_type, m , y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential = \\\n",
    "        preprocess(train_fname.with_suffix('.csv'))\n",
    "    np.savez(train_fname, \n",
    "             x_xyz=x_xyz,\n",
    "             x_a_hot=x_a_hot,\n",
    "             x_type=x_type,\n",
    "             y_scalar=y_scalar,\n",
    "             y_magnetic=y_magnetic,\n",
    "             y_mulliken=y_mulliken,\n",
    "             y_dipole=y_dipole,\n",
    "             y_potential=y_potential,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coulombmat = np.load('x_coulombmat.npy', allow_pickle=True)\n",
    "x_coulombmat = np.array(x_coulombmat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile = np.load(test_fname)\n",
    "    xt_xyz = npzfile['x_xyz']\n",
    "    xt_a_hot = npzfile['x_a_hot']\n",
    "    xt_type = npzfile['x_type']\n",
    "    mt = npzfile['m']\n",
    "    #xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    train_csv = pd.read_csv(train_fname.with_suffix('.csv'))\n",
    "    xt_xyz,xt_a_hot,xt_type,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index = pd.factorize(train_csv['type'])[1])\n",
    "    np.savez(test_fname, \n",
    "             x_xyz=xt_xyz,\n",
    "             x_a_hot=xt_a_hot,\n",
    "             x_type=xt_type,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)\n",
    "    \n",
    "try:\n",
    "    xt_ids = np.load('xt_ids.npy')\n",
    "except:\n",
    "    print(\"ASK PAVEL\")\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(785836, 3, 29),\n",
       " (785836, 5, 29),\n",
       " (785836, 1, 29),\n",
       " (785836, 4, 29),\n",
       " (785836, 9, 29),\n",
       " (785836, 1, 29),\n",
       " (785836, 3),\n",
       " (785836, 1),\n",
       " (785836,)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_a_hot,x_type, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do norm in FASTAI instead of here\n",
    "# TOCHECK: Filter only valid atoms (otherwise repeated atoms may skew stats)\n",
    "#xyz_mean, xyz_std = x_xyz.mean(axis=(0,2), keepdims=True),  x_xyz.std(axis=(0,2), keepdims=True)\n",
    "#x_xyz  = (x_xyz  - xyz_mean) / xyz_std\n",
    "#xt_xyz = (xt_xyz - xyz_mean) / xyz_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(422550, 3, 29), (422550, 5, 29), (422550, 1, 29), (422550,)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [xt_xyz,xt_a_hot,xt_type, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/filipradenovic/cnnimageretrieval-pytorch/blob/master/cirtorch/layers/functional.py\n",
    "    \n",
    "def gem(x, kernel_size, p=3, eps=1e-6):\n",
    "    return F.avg_pool1d(x.clamp(min=eps).pow(p), kernel_size).pow(1./p)\n",
    "\n",
    "class GeM(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, self.kernel_size, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,a_hot,type,coulomb): \n",
    "        self.i,self.xyz,self.a_ahot,self.type,self.coulomb = i,xyz,a_hot,type,coulomb\n",
    "        self.data = [torch.cat([Tensor(xyz), Tensor(self.a_ahot),Tensor(self.type)], dim=0), \n",
    "                     Tensor(coulomb)]\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        #print(tfms)\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i,self.xyz,self.a_ahot,self.type,self.coulomb)\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,magnetic,mulliken,dipole,potential,**kwargs): \n",
    "        self.scalar,self.magnetic,self.mulliken,self.dipole,self.potential = \\\n",
    "            scalar,magnetic,mulliken,dipole,potential\n",
    "        self.data = Tensor(np.sum(scalar, axis=0))\n",
    "    def __str__(self):\n",
    "        res, spacer, n_couplings = '', '', 0\n",
    "        for s in self.data:\n",
    "            if s==0.: spacer = ' * '\n",
    "            else: \n",
    "                res += f'{spacer}{s:.4f}'\n",
    "                spacer = ' '\n",
    "                n_couplings +=1\n",
    "        return f'{n_couplings}: {res}'\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMAEMaskedLoss(input_outputs,target,input_transform_weight=0, feature_transform_weight=0.):\n",
    "    input, output, trans, trans_feat = input_outputs\n",
    "    loss = 0.\n",
    "    n = 0\n",
    "    for type in range(n_types):\n",
    "        mask = (input[:,8,:] == type)\n",
    "        if mask.sum() > 0:\n",
    "            _output,_target = output[:,0,:], target\n",
    "            loss += torch.log((_output[mask] - _target[mask]).abs().mean()+1e-9)\n",
    "            n+=1\n",
    "    loss /= n\n",
    "    if input_transform_weight   > 0: loss += feature_transform_regularizer(trans)*input_transform_weight\n",
    "    if feature_transform_weight > 0: loss += feature_transform_regularizer(trans_feat)*feature_transform_weight\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.c = 1\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return ScalarCouplingItem(*o)\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "    \n",
    "    \n",
    "    original_shape = list(v.shape)\n",
    "    q = q.view(-1, 4)\n",
    "    v = v.view(-1, 3)\n",
    "    \n",
    "    qvec = q[:, 1:]\n",
    "    uv = torch.cross(qvec, v, dim=1)\n",
    "    uuv = torch.cross(qvec, uv, dim=1)\n",
    "    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)\n",
    "\n",
    "def random_rotation(data):\n",
    "    x_xyz = data[0][:3].transpose(0,1)\n",
    "    r = torch.rand(3)\n",
    "    sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-r[:1]),torch.sqrt(r[:1]),2*math.pi*r[1:2],2*math.pi*r[2:3]\n",
    "    q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                   sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=0).unsqueeze(0)\n",
    "    x_xyz = qrot(q.expand(x_xyz.shape[0],-1), x_xyz).squeeze(0).transpose(0,1)\n",
    "    return (torch.cat((x_xyz, data[0][3:]), dim=0), data[1])\n",
    "\n",
    "def smooth_atom_one_hot(data):\n",
    "    r = torch.rand(5,5)\n",
    "    r[:,0] = r[:,0]*.2 + 0.8\n",
    "    s = 1 - r[:,0]\n",
    "    rs = r[:,1:].sum(dim=1,keepdim=True)\n",
    "    r[:,1:] *= s.unsqueeze(-1)/rs\n",
    "    for i in range(5):\n",
    "        r[i] = r[i].roll(i,dims=0)\n",
    "    a_hot = data[0][3:-1].argmax(dim=0)\n",
    "    data[0][3:-1] = r[a_hot].transpose(0,1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in enumerate(zip(x_xyz,x_a_hot,x_type, x_coulombmat))),\n",
    "                label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.split_by_idx(idx_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.label_from_func(\n",
    "    func=lambda o: (y_scalar[o.i], y_magnetic[o.i], y_mulliken[o.i], y_dipole[o.i], y_potential[o.i]),\n",
    "    label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelLists;\n",
       "\n",
       "Train: LabelList (707619 items)\n",
       "x: ItemList\n",
       "0 5 atoms 4 couplings,1 5 atoms 3 couplings,2 5 atoms 2 couplings,3 5 atoms 1 couplings,4 4 atoms 3 couplings\n",
       "y: ScalarCouplingList\n",
       "4: 84.8076 * -11.2569 -11.2549 -11.2543,3: 84.8074 * -11.2542 -11.2548,2: 84.8093 * -11.2543,1: 84.8095,3: 32.6888 * -11.1867 -11.1757\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (78217 items)\n",
       "x: ItemList\n",
       "9 8 atoms 7 couplings,10 8 atoms 6 couplings,11 8 atoms 5 couplings,12 8 atoms 4 couplings,13 8 atoms 3 couplings\n",
       "y: ScalarCouplingList\n",
       "7: 83.5430 -2.3783 * -11.7004 -11.6979 3.2528 13.6913 3.2521,6: 83.5417 -2.3786 * -11.6996 13.6924 3.2525 3.2527,5: 83.5484 -2.3772 * 3.2524 3.2524 13.6921,4: -2.3788 83.5418 * -11.7004 -11.6993,3: -2.3785 83.5430 * -11.6976\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (422550 items)\n",
       "x: ItemList\n",
       "0 4 atoms 3 couplings,1 4 atoms 2 couplings,2 9 atoms 4 couplings,3 9 atoms 3 couplings,4 9 atoms 2 couplings\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PUT REAL STUFF HERE WHEN/IF WE USE IT\n",
    "xt_coulombmat = np.empty((xt_xyz.shape[0], 29,29))\n",
    "data.add_test(\n",
    "    ItemList(items=(MoleculeItem(i,*v) for i,v in enumerate(zip(xt_xyz,xt_a_hot,xt_type,xt_coulombmat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.transform(([random_rotation, smooth_atom_one_hot], [random_rotation, smooth_atom_one_hot]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelLists;\n",
       "\n",
       "Train: LabelList (707619 items)\n",
       "x: ItemList\n",
       "0 5 atoms 4 couplings,1 5 atoms 3 couplings,2 5 atoms 2 couplings,3 5 atoms 1 couplings,4 4 atoms 3 couplings\n",
       "y: ScalarCouplingList\n",
       "4: 84.8076 * -11.2569 -11.2549 -11.2543,3: 84.8074 * -11.2542 -11.2548,2: 84.8093 * -11.2543,1: 84.8095,3: 32.6888 * -11.1867 -11.1757\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (78217 items)\n",
       "x: ItemList\n",
       "9 8 atoms 7 couplings,10 8 atoms 6 couplings,11 8 atoms 5 couplings,12 8 atoms 4 couplings,13 8 atoms 3 couplings\n",
       "y: ScalarCouplingList\n",
       "7: 83.5430 -2.3783 * -11.7004 -11.6979 3.2528 13.6913 3.2521,6: 83.5417 -2.3786 * -11.6996 13.6924 3.2525 3.2527,5: 83.5484 -2.3772 * 3.2524 3.2524 13.6921,4: -2.3788 83.5418 * -11.7004 -11.6993,3: -2.3785 83.5430 * -11.6976\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (422550 items)\n",
       "x: ItemList\n",
       "0 4 atoms 3 couplings,1 4 atoms 2 couplings,2 9 atoms 4 couplings,3 9 atoms 3 couplings,4 9 atoms 2 couplings\n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: ."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4139,  0.6641,  0.0000,  2.0780,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.6861,  0.7920,  0.0000,  2.4780,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.5206, -0.2446,  0.0000, -0.7652,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this a few times to see rotations at work (atom xyz should change)\n",
    "data.test_ds[0][0].data[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0920, 0.0000, 1.7831, 1.7831, 1.7832, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000])\n",
      "tensor([1.0920, 0.0000, 1.7831, 1.7831, 1.7832, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# but norm of the atoms shouldn't change\n",
    "print(torch.norm(data.train_ds[0][0].data[0][:3], dim=0))\n",
    "for _ in range(1000):\n",
    "    _ = data.train_ds[0][0].data[0][:3]\n",
    "print(torch.norm(data.train_ds[0][0].data[0][:3], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemCallback(Callback):\n",
    "    def __init__(self): super().__init__()\n",
    "    def on_batch_begin(self,**kwargs):\n",
    "        \"Save the last_input (i.e. current input) for on_loss_begin_callback\"\n",
    "        self.last_input = kwargs['last_input']\n",
    "    def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "        \"Add last_input to last_output, i.e. input to loss function b/c we need it\"\n",
    "        return {'last_output': (self.last_input[0], *last_output)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO REFACTOR\n",
    "class Quaternion(Module):\n",
    "    def __init__(self, in_channels = 3):\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        #http://planning.cs.uiuc.edu/node198.html\n",
    "        sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-x[:,:1]),torch.sqrt(x[:,:1]),2*math.pi*x[:,1:2],2*math.pi*x[:,2:3]\n",
    "        q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                       sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=1)\n",
    "        return q\n",
    "    \n",
    "# https://github.com/eladhoffer/fix_your_classifier/blob/master/fixed_proj.py\n",
    "class LinearProjection(Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, bias=True, init_scale=10):\n",
    "        if init_scale is not None:\n",
    "            self.weight = nn.Parameter(torch.Tensor(1).fill_(init_scale))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_size).fill_(0))\n",
    "        self.proj = Variable(torch.Tensor(\n",
    "            output_size, input_size), requires_grad=False)\n",
    "        torch.manual_seed(123)\n",
    "        nn.init.orthogonal_(self.proj)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.proj.type_as(x)\n",
    "        x = x / x.norm(2, -1, keepdim=True)\n",
    "        out = nn.functional.linear(x, w)\n",
    "        if hasattr(self, 'weight'): out = out * self.weight\n",
    "        if hasattr(self, 'bias'):   out = out + self.bias.view(1, -1)\n",
    "        return out\n",
    "\n",
    "# from https://github.com/fxia22/pointnet.pytorch/blob/master/pointnet/model.py\n",
    "class STNkd(Module):\n",
    "    def __init__(self, pools, channels, add_eye, last_fc=nn.Linear):\n",
    "        k, c_filters, fc_filters = channels[0][0], channels[0][1:], channels[1]\n",
    "        self.conv1 = torch.nn.Conv1d(k, c_filters[0], 1)\n",
    "        self.conv2 = torch.nn.Conv1d(c_filters[0], c_filters[1], 1)\n",
    "        self.conv3 = torch.nn.Conv1d(c_filters[1], c_filters[2], 1)\n",
    "        self.fc1 = nn.Linear(c_filters[2]*len(pools), fc_filters[0])\n",
    "        self.fc2 = nn.Linear(fc_filters[0], fc_filters[1])\n",
    "        self.fc3 = last_fc(fc_filters[1], k*k)\n",
    "        # TODO REFACTOR\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(c_filters[0])\n",
    "        self.bn2 = nn.BatchNorm1d(c_filters[1])\n",
    "        self.bn3 = nn.BatchNorm1d(c_filters[2])\n",
    "        self.bn4 = nn.BatchNorm1d(fc_filters[0])\n",
    "        self.bn5 = nn.BatchNorm1d(fc_filters[1])\n",
    "        self.pools = [pool(1) for pool in pools]\n",
    "\n",
    "        self.k = k\n",
    "        self.add_eye = add_eye\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.cat([pool(x).squeeze(-1) for pool in self.pools], dim=-1)    \n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x).view(-1, self.k, self.k)\n",
    "        if self.add_eye: x += torch.eye(self.k, dtype=x.dtype, device=x.device).unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "class PointNetfeat(Module):\n",
    "    def __init__(self, pools, C, quaternion_transform,\n",
    "                 input_transform, feature_transform, add_eye=True):\n",
    "        self.qrot = Quaternion() if quaternion_transform else None\n",
    "        self.stn = STNkd(pools, channels = input_transform, add_eye=add_eye) if input_transform else None        \n",
    "        self.convs = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv1d(C[i], C[i+1], 1), nn.BatchNorm1d(C[i+1]), nn.ReLU(), ) for i in range(len(C)-1)])\n",
    "        self.fstn = STNkd(pools, channels = feature_transform, add_eye=add_eye, \n",
    "                         last_fc = nn.Linear) if feature_transform else None\n",
    "        self.pools = [pool(1) for pool in pools]\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize, _, n_pts = x.shape\n",
    "        if self.qrot is not None:\n",
    "            x_xyz = x[:,:3,...]\n",
    "            q = self.qrot(x_xyz).unsqueeze(1).expand(-1,n_pts,-1).contiguous()\n",
    "            x_xyz = x_xyz.transpose(1,2).contiguous()\n",
    "            x[:,:3,:] = qrot(q,x_xyz).transpose(2,1)\n",
    "        if self.stn is not None: # TODO add xyz_\n",
    "            x_xyz = x[:,:3,...].clone()\n",
    "            trans = self.stn(x_xyz)\n",
    "            x[:,:3,:] = torch.bmm(trans, x_xyz)\n",
    "        else:\n",
    "            trans = torch.eye(1, dtype=x.dtype, device=x.device).view(1,1).expand((batchsize,-1))\n",
    "        for conv in self.convs[:2]:\n",
    "            x = conv(x)\n",
    "\n",
    "        if self.fstn is not None:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = torch.bmm(trans_feat, x)\n",
    "        else:\n",
    "            trans_feat = torch.eye(1, dtype=x.dtype, device=x.device).view(1,1).expand((batchsize,-1))\n",
    "\n",
    "        pointfeat = x\n",
    "        for conv in self.convs[2:]:\n",
    "            x = conv(x)\n",
    "        x = torch.cat([pool(x).squeeze(-1) for pool in self.pools], dim=-1)    \n",
    "        return pointfeat, x, trans, trans_feat\n",
    "    \n",
    "class PointNetDenseReg(Module):\n",
    "    def __init__(self, feat_channels, seg_channels, \n",
    "                 quaternion_transform, input_transform, feature_transform, add_eye, \n",
    "                 pools = [nn.AdaptiveMaxPool1d], dropout_rate = 0.):\n",
    "        self.feat = PointNetfeat(pools,\n",
    "                                 C = feat_channels,\n",
    "                                 quaternion_transform = quaternion_transform,\n",
    "                                 input_transform = input_transform, \n",
    "                                 feature_transform=feature_transform,\n",
    "                                 add_eye=add_eye)\n",
    "        #self.feat_c = PointNetfeat(pools,\n",
    "        #                         C = feat_channels,\n",
    "        #                         quaternion_transform = quaternion_transform,\n",
    "        #                         input_transform = input_transform, \n",
    "        #                         feature_transform=feature_transform,\n",
    "        #                         add_eye=add_eye)\n",
    "        C = seg_channels\n",
    "        \n",
    "        C.insert(0, feat_channels[-1]*len(pools) + feat_channels[1])        \n",
    "        \n",
    "        self.seg_convs = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv1d(C[i], C[i+1], 1), nn.BatchNorm1d(C[i+1]), nn.ReLU(),) for i in range(len(C)-2)])\n",
    "        self.seg_convs.append(nn.Conv1d(C[-2], C[-1], 1))\n",
    "            \n",
    "        #coulomb_C = [feat_channels[2],128,1024]\n",
    "        \n",
    "        #self.coulomb_convs = nn.ModuleList([nn.Sequential(\n",
    "        #    nn.Conv1d(coulomb_C[i], coulomb_C[i+1], 1), nn.BatchNorm1d(coulomb_C[i+1]), nn.ReLU(),) \n",
    "        #                                  for i in range(len(coulomb_C)-1)])\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_rate) if dropout_rate > 0. else None\n",
    "        self.pools = [pool(1) for pool in pools]\n",
    "        \n",
    "    def forward(self, x, coulomb):\n",
    "        batchsize, _, n_pts = x.shape\n",
    "        pointfeat, globalfeat, trans, trans_feat = self.feat(x)\n",
    "#        x_c = torch.matmul(pointfeat, coulomb)\n",
    "#        for coulomb_conv in self.coulomb_convs[:3]:\n",
    "#            x_c = coulomb_conv(x_c)\n",
    "#        x_c = torch.cat([pool(x_c) for pool in self.pools], dim=-1)   \n",
    "#        for coulomb_conv in self.coulomb_convs[3:]:\n",
    "#            x_c = coulomb_conv(x_c)\n",
    "\n",
    "        #print(globalfeat.shape, pointfeat.shape, x_c.shape)\n",
    "        x = torch.cat([globalfeat.unsqueeze(-1).expand(-1, -1, n_pts), \n",
    "                       pointfeat,\n",
    "                      ], 1)\n",
    "                      # x_c.expand(-1, -1, n_pts)], 1)\n",
    "        #print(x.shape)\n",
    "        for seg_conv in self.seg_convs[:-1]:\n",
    "            x = seg_conv(x)\n",
    "        if self.dropout is not None: x = self.dropout(x)\n",
    "        x = self.seg_convs[-1](x)\n",
    "        return x, trans, trans_feat\n",
    "\n",
    "def feature_transform_regularizer(trans): \n",
    "    batchsize,d  = trans.shape[0], trans.shape[1]\n",
    "    I = torch.eye(d, dtype=trans.dtype, device=trans.device).unsqueeze(0)\n",
    "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "net = PointNetDenseReg(feat_channels = [9,64,64,128,1024],\n",
    "                       seg_channels  = [512,256,128,1],\n",
    "                       quaternion_transform=False, \n",
    "                       input_transform=[[3, 64,128,1024], [512,256]], \n",
    "                       feature_transform=[[64, 64,128,1024], [512,256]],\n",
    "                       add_eye=True)\n",
    "learner = Learner(data,net, callbacks=[ChemCallback()],\n",
    "                  loss_func=partial(LMAEMaskedLoss,input_transform_weight=0., feature_transform_weight=1e-3),\n",
    "                  metrics=[ partial(LMAEMaskedLoss,input_transform_weight=0., feature_transform_weight=0.)])\n",
    "data.batch_size = 4096*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fname = \"loss-2.6995val-1.8588\" # '600k' #'val-1.7450' # 'loss-1.6714val-1.5721'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: loss-2.6995val-1.8588... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antor/miniconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type PointNetDenseReg. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/antor/miniconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type PointNetfeat. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/antor/miniconda3/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type STNkd. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Attempting to load: {sub_fname}... \", end=\"\")\n",
    "    learner.load(sub_fname, strict=False)\n",
    "    print(\"Loaded\")\n",
    "except:\n",
    "    print(\"NOT loaded!\")\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learner.to_parallel()#.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.batch_size = int(4096*2)\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(720, 6e-3)#, moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8622718\n"
     ]
    }
   ],
   "source": [
    "data.batch_size = 4096\n",
    "learner.to_fp32()\n",
    "val = learner.validate()[0]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fname = f'loss{learner.recorder.losses[-1]:.04f}val{val:.04f}'\n",
    "#sub_fname = f'val{val:.04f}'\n",
    "learner.save(sub_fname)\n",
    "print(sub_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='100', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.00% [1/100 00:27<45:22]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='104', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d96b6362f4fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     for batch_idx, batch in progress_bar(\n\u001b[0;32m---> 12\u001b[0;31m         enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;31m# prime the prefetch loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_put_indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TTA_N = 100\n",
    "\n",
    "sub = dict()\n",
    "\n",
    "ids = xt_ids[xt_ids!=0]\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 1, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n",
    "        _, preds_, _, _ = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "    test_preds = np.squeeze(test_preds, 1)\n",
    "\n",
    "    preds = test_preds[xt_ids!=0]\n",
    "\n",
    "    for k in range(len(ids)):\n",
    "        if tta > 0:\n",
    "            sub[int(ids[k])] += preds[k]\n",
    "        else:\n",
    "            sub[int(ids[k])] = preds[k]\n",
    "\n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = sub[int(ids[k])]/TTA_N\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(sub_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 'champs-scalar-coupling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 62.5M/62.5M [00:06<00:00, 10.0MB/s]\n",
      "Successfully submitted to Predicting Molecular Properties"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c {comp} -f {sub_fname} -m 'rot 1-hot aug feat_reg 1e-3 tta 100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "!kaggle competitions submissions -c {comp} -v > submissions-{comp}.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-1.882'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions = pd.read_csv(f'submissions-{comp}.csv')\n",
    "submissions.iloc[0].publicScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fname"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
