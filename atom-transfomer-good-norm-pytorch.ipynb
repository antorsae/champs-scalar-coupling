{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from RAdam.radam import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                          missing_keys, unexpected_keys, error_msgs):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
    "    this module, but not its descendants. This is called on every submodule\n",
    "    in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
    "    module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
    "    For state dicts without metadata, :attr:`local_metadata` is empty.\n",
    "    Subclasses can achieve class-specific backward compatible loading using\n",
    "    the version number at `local_metadata.get(\"version\", None)`.\n",
    "\n",
    "    .. note::\n",
    "        :attr:`state_dict` is not the same object as the input\n",
    "        :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
    "        it can be modified.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        prefix (str): the prefix for parameters and buffers used in this\n",
    "            module\n",
    "        local_metadata (dict): a dict containing the metadata for this module.\n",
    "            See\n",
    "        strict (bool): whether to strictly enforce that the keys in\n",
    "            :attr:`state_dict` with :attr:`prefix` match the names of\n",
    "            parameters and buffers in this module\n",
    "        missing_keys (list of str): if ``strict=True``, add missing keys to\n",
    "            this list\n",
    "        unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
    "            keys to this list\n",
    "        error_msgs (list of str): error messages should be added to this\n",
    "            list, and will be reported together in\n",
    "            :meth:`~torch.nn.Module.load_state_dict`\n",
    "    \"\"\"\n",
    "    for hook in self._load_state_dict_pre_hooks.values():\n",
    "        hook(state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs)\n",
    "\n",
    "    local_name_params = itertools.chain(self._parameters.items(),\n",
    "                                        self._buffers.items())\n",
    "    local_state = {k: v.data for k, v in local_name_params if v is not None}\n",
    "\n",
    "    for name, param in local_state.items():\n",
    "        key = prefix + name\n",
    "        if key in state_dict:\n",
    "            input_param = state_dict[key]\n",
    "\n",
    "            # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
    "            if len(param.shape) == 0 and len(input_param.shape) == 1:\n",
    "                input_param = input_param[0]\n",
    "\n",
    "            if input_param.shape != param.shape:\n",
    "                # local shape should match the one in checkpoint\n",
    "                error_msgs.append(\n",
    "                    'size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                    'the shape in current model is {}.'.format(\n",
    "                        key, input_param.shape, param.shape))\n",
    "                #if not strict:\n",
    "                #    continue\n",
    "\n",
    "            if isinstance(input_param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                input_param = input_param.data\n",
    "\n",
    "            try:\n",
    "                param.copy_(input_param)\n",
    "            except Exception:\n",
    "                error_msgs.append(\n",
    "                    'While copying the parameter named \"{}\", '\n",
    "                    'whose dimensions in the model are {} and '\n",
    "                    'whose dimensions in the checkpoint are {}.'.format(\n",
    "                        key, param.size(), input_param.size()))\n",
    "                # PG load partially\n",
    "\n",
    "                if len(input_param.size()) == 3:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(\n",
    "                            key, param.size(), input_param.size(),\n",
    "                            param[:input_param.size()[0], :input_param.size(\n",
    "                            )[1], :input_param.size()[2]].shape))\n",
    "                else:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(key, param.size(), input_param.size(),\n",
    "                                param[:input_param.size()[0]].shape))\n",
    "\n",
    "                try:\n",
    "                    new_input_param = torch.empty_like(param)\n",
    "                    new_input_param = torch.nn.init.normal_(new_input_param,\n",
    "                                                            mean=input_param.mean(),\n",
    "                                                            std=input_param.std())\n",
    "\n",
    "                    if len(input_param.size()) == 3:\n",
    "                        new_input_param[:input_param.size()[0], :input_param.\n",
    "                                        size()[1], :input_param.size(\n",
    "                                        )[2]] = input_param\n",
    "                    else:\n",
    "                        new_input_param[:input_param.size()[0]] = input_param\n",
    "                    param.copy_(new_input_param)\n",
    "                except Exception as e:\n",
    "                    assert e\n",
    "                    error_msgs.append(\n",
    "                        'Failed to load weights partially {}'.format(e))\n",
    "        elif strict:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    if strict:\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(prefix):\n",
    "                input_name = key[len(prefix):]\n",
    "                input_name = input_name.split(\n",
    "                    '.', 1)[0]  # get the name of param/buffer/child\n",
    "                if input_name not in self._modules and input_name not in local_state:\n",
    "                    unexpected_keys.append(key)\n",
    "\n",
    "def load_state_dict(self, state_dict, strict=True):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
    "    this module and its descendants. If :attr:`strict` is ``True``, then\n",
    "    the keys of :attr:`state_dict` must exactly match the keys returned\n",
    "    by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        strict (bool, optional): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "\n",
    "    Returns:\n",
    "        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
    "            * **missing_keys** is a list of str containing the missing keys\n",
    "            * **unexpected_keys** is a list of str containing the unexpected keys\n",
    "    \"\"\"\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "                \n",
    "    load(self)\n",
    "\n",
    "    if strict:\n",
    "        if len(unexpected_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Unexpected key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in unexpected_keys)))\n",
    "        if len(missing_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Missing key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in missing_keys)))\n",
    "\n",
    "    if strict and len(error_msgs) > 0:\n",
    "        raise RuntimeError(\n",
    "            'Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.Module._load_from_state_dict = _load_from_state_dict\n",
    "nn.Module.load_state_dict = load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.distributed import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deterministic\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname_ext = lambda fname,ext: f'{str(fname)[:-4]}{ext}{str(fname)[-4:]}'\n",
    "\n",
    "def preprocess(fname, type_index=None, ext=''):\n",
    "    t  = pd.read_csv(fname_ext(fname,ext))\n",
    "    s  = pd.read_csv('structures.csv')\n",
    "    \n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "\n",
    "    if has_y:\n",
    "        # atom-atom level\n",
    "        # molecule_name,atom_index_0,atom_index_1,type,fc,sd,pso,dso\n",
    "        scalar_couplings = pd.read_csv(f'scalar_coupling_contributions{ext}.csv') # fc,sd,pso,dso\n",
    "\n",
    "        # atom level\n",
    "        # molecule_name,atom_index,XX,YX,ZX,XY,YY,ZY,XZ,YZ,ZZ\n",
    "        magnetic_shielding = pd.read_csv('magnetic_shielding_tensors.csv')\n",
    "        # molecule_name,atom_index,mulliken_charge\n",
    "        mulliken_charges = pd.read_csv('mulliken_charges.csv')\n",
    "\n",
    "        # molecule level\n",
    "        # molecule_name,X,Y,Z\n",
    "        dipole_moments = pd.read_csv('dipole_moments.csv')\n",
    "        # molecule_name,potential_energy\n",
    "        potential_energy = pd.read_csv('potential_energy.csv')\n",
    "\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_idx'] = t['type'].apply(lambda x: type_index.index(x) ) # pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_idx'] = pd.factorize(t['type'])[0]\n",
    "\n",
    "    s['atom_idx'] = s['atom'].apply(lambda x: atoms.index(x) )\n",
    "\n",
    "    max_items = len(t.groupby(['molecule_name', 'atom_index_0']))# if has_y else 422550\n",
    "    max_atoms = int(s.atom_index.max() + 1)\n",
    "\n",
    "    if has_y:\n",
    "        contributions = ['fc','sd','pso','dso']\n",
    "        magnetic_tensors = ['XX','YX','ZX','XY','YY','ZY','XZ','YZ','ZZ']\n",
    "        XYZ = ['X','Y','Z']\n",
    "    xyz = ['x', 'y', 'z']\n",
    "    \n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_type  = np.zeros((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_ext   = np.zeros((max_items,1,         max_atoms), dtype=np.bool_)\n",
    "    x_atom  = np.empty((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_atom[:] = -1\n",
    "\n",
    "    if has_y:\n",
    "        y_scalar   = np.zeros((max_items,len(contributions)   ,max_atoms), dtype=np.float32)\n",
    "        y_magnetic = np.zeros((max_items,len(magnetic_tensors),max_atoms), dtype=np.float32)\n",
    "        y_mulliken = np.zeros((max_items,1                    ,max_atoms), dtype=np.float32)\n",
    "\n",
    "        y_dipole   = np.zeros((max_items,len(XYZ)), dtype=np.float32)\n",
    "        y_potential= np.zeros((max_items,1       ), dtype=np.float32)\n",
    "\n",
    "        y_magnetic[...] = np.nan\n",
    "        y_mulliken[...] = np.nan\n",
    "    else:\n",
    "        xt_ids = np.zeros((max_items, max_atoms), dtype=np.int32)\n",
    "\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    i = j = 0\n",
    "    \n",
    "    for (m_name, m_index) ,m_group in tqdm(t.groupby(['molecule_name', 'molecule_index'])):\n",
    "        ss = s[s.molecule_name==m_name]\n",
    "        n_atoms = len(ss)\n",
    "        if has_y:\n",
    "            magnetic = magnetic_shielding[\n",
    "                    (magnetic_shielding['molecule_name']==m_name)][magnetic_tensors].values.T\n",
    "\n",
    "            mulliken = mulliken_charges[\n",
    "                    (mulliken_charges['molecule_name']==m_name)]['mulliken_charge'].values.T\n",
    "\n",
    "            scs = scalar_couplings[scalar_couplings['molecule_name']==m_name]\n",
    "            \n",
    "            y_dipole[j,:]= dipole_moments[dipole_moments['molecule_name']==m_name][XYZ].values\n",
    "            y_potential[j,:]=potential_energy[\n",
    "                potential_energy['molecule_name']==m_name]['potential_energy'].values\n",
    "        \n",
    "        for a_name,a_group in m_group.groupby('atom_index_0'):\n",
    "            \n",
    "            ref_a = ss[ss['atom_index']==a_name]\n",
    "            \n",
    "            x_xyz[i] = 0.\n",
    "            x_type[i] = -1\n",
    "            x_ext[i] =  True\n",
    "            \n",
    "            x_xyz[i,:,:n_atoms] = (ss[xyz].values-ref_a[xyz].values).T  # xyz \n",
    "            x_type[i,0,a_group['atom_index_1']] = a_group['type_idx']  # type \n",
    "            x_ext[i,0,a_group['atom_index_1']] = a_group['ext']  # ext \n",
    "            x_atom[i,:,:n_atoms] = ss['atom_idx'].T                \n",
    "\n",
    "            if has_y:\n",
    "                y_scalar[i,:,a_group['atom_index_1']] = scs[scs['atom_index_0']==a_name][contributions]\n",
    "                y_magnetic[i,:,:n_atoms] = magnetic\n",
    "                y_mulliken[i,:,:n_atoms] = mulliken\n",
    "            else:\n",
    "                xt_ids[i,a_group['atom_index_1']] = a_group['id']  \n",
    "\n",
    "            m[i] = m_index\n",
    "            i+=1\n",
    "        j += 1\n",
    "    assert i == max_items\n",
    "    print(i,max_items)\n",
    "    if has_y:\n",
    "        return x_xyz,x_type,x_ext,x_atom, m, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential\n",
    "    else:\n",
    "        return x_xyz,x_type,x_ext,x_atom, m, xt_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where you want to use original training set '' or extended ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext = '_ext' # or ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed or preprocess and save for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fname = Path('train.npz')\n",
    "types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "\n",
    "try:\n",
    "    npzfile = np.load(fname_ext(train_fname, ext))\n",
    "    x_xyz   = npzfile['x_xyz']\n",
    "    x_type  = npzfile['x_type']\n",
    "    x_ext   = npzfile['x_ext']\n",
    "    x_atom  = npzfile['x_atom']\n",
    "\n",
    "    y_scalar    = npzfile['y_scalar']\n",
    "    y_magnetic  = npzfile['y_magnetic']\n",
    "    y_mulliken  = npzfile['y_mulliken']\n",
    "    y_dipole    = npzfile['y_dipole']\n",
    "    y_potential = npzfile['y_potential']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_type,x_ext,x_atom, m, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential = \\\n",
    "        preprocess(train_fname.with_suffix('.csv'), type_index=types, ext=ext)\n",
    "    np.savez(fname_ext(train_fname, ext), \n",
    "             x_xyz=x_xyz,\n",
    "             x_type=x_type,\n",
    "             x_ext=x_ext,\n",
    "             x_atom=x_atom,\n",
    "             y_scalar=y_scalar,\n",
    "             y_magnetic=y_magnetic,\n",
    "             y_mulliken=y_mulliken,\n",
    "             y_dipole=y_dipole,\n",
    "             y_potential=y_potential,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_memmap = True\n",
    "load_fn = np.load if not use_memmap else partial(np.lib.format.open_memmap, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_qm9_mulliken = load_fn(f'x_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1405126, 3, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 4, 29),\n",
       " (1405126, 9, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 3),\n",
       " (1405126, 1),\n",
       " (1405126,)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_type,x_ext,x_atom,x_qm9_mulliken, \n",
    "                   y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405126, 29)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_atom_mask = (x_atom != -1).swapaxes(1,2).squeeze(-1)\n",
    "real_atom_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_xyz_masked = x_xyz.swapaxes(1,2)[real_atom_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26154396, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BAD mean/std\n",
    "#x_xyz_mean, x_xyz_std = Tensor(x_xyz.mean(axis=(0,2),keepdims=True)), Tensor(x_xyz.std(axis=(0,2),keepdims=True))\n",
    "#x_xyz_mean = Tensor(x_xyz).mean(dim=(0,2),keepdim=True)\n",
    "#x_xyz_std  = Tensor(x_xyz).std(dim=(0,2), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GOOD mean/std\n",
    "x_xyz_mean = Tensor(x_xyz_masked).mean(dim=(0),keepdim=True).unsqueeze(-1)\n",
    "x_xyz_std  = Tensor(x_xyz_masked).std(dim=(0), keepdim=True).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0094],\n",
       "          [-0.0328],\n",
       "          [ 0.0032]]]), tensor([[[1.8703],\n",
       "          [2.2832],\n",
       "          [1.7753]]]), torch.Size([1, 3, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_mean, x_xyz_std, x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_xyz = (x_xyz - x_xyz_mean.numpy() ) / x_xyz_std.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_qm9_mulliken_mean = Tensor(x_qm9_mulliken.mean(axis=(0,2),keepdims=True))\n",
    "x_qm9_mulliken_std  = Tensor(x_qm9_mulliken.std( axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai classes (this should should be done into its own `application` but who has time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,type,ext,atom,qm9_mulliken): \n",
    "        self.i,self.xyz,self.type,self.ext, self.atom,self.qm9_mulliken = \\\n",
    "            i,xyz,type,ext,atom,qm9_mulliken\n",
    "        self.data = [Tensor(xyz), LongTensor((type)), \n",
    "                     Tensor(ext), LongTensor((atom)),Tensor(qm9_mulliken)]\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i,self.xyz,self.type,self.ext,self.atom,self.qm9_mulliken)\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,**kwargs): \n",
    "        self.scalar = scalar#,magnetic,mulliken,dipole,potential\n",
    "        self.data = Tensor(scalar) #, Tensor(magnetic), Tensor(dipole), Tensor(potential))\n",
    "    def __str__(self):\n",
    "        #res, spacer, n_couplings = '', '', 0\n",
    "        return f'{self.data}'\n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        y = self.clone()\n",
    "        for t in tfms:\n",
    "            if 'label_smoothing' == t.__name__:\n",
    "                if t: y.data = t(y.data)\n",
    "        return y\n",
    "    def clone(self): return self.__class__(self.scalar)#,self.magnetic,self.mulliken,self.dipole,self.potential)\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_scalar.mean(axis=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMaskedLoss(Module):\n",
    "    def __init__(self, contrib_w=0, types_w = [1]*n_types, return_all=False, proxy_log=torch.log, exclude_ext=False):\n",
    "        self.contrib_w = contrib_w\n",
    "        self.types_w = types_w\n",
    "        self.return_all = return_all\n",
    "        self.proxy_log = proxy_log\n",
    "        self.exclude_ext = exclude_ext\n",
    "        self.loss = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, from_forward, t_scalar):\n",
    "        valid_couplings_mask,type_masked,ext_masked,p_scalar_masked = from_forward\n",
    "        loss, n, j_loss = 0, 0, [0] * n_types\n",
    "        #p_scalar_masked = p_scalar_masked#.squeeze(-1)\n",
    "        t_scalar_masked = t_scalar[valid_couplings_mask.to(dtype=torch.bool)]\n",
    "        for t in range(n_types):\n",
    "            type_mask = (type_masked == t) if not self.exclude_ext else ((type_masked == t) & (ext_masked == 0))\n",
    "            if type_mask.sum() > 0:\n",
    "                _output,_target = p_scalar_masked[type_mask], t_scalar_masked[type_mask]\n",
    "                s_loss = self.proxy_log(torch.nn.L1Loss()(_output,_target)+1e-9)\n",
    "                loss += self.types_w[t] * s_loss\n",
    "                j_loss[t] += s_loss\n",
    "                n+=1\n",
    "        loss /= n\n",
    "        \n",
    "        return loss if not self.return_all else (loss, *j_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return ScalarCouplingItem(np.array(o, dtype=np.float32))\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quaterions allow us to rotate 3d points randoming with a nice uniform distribution of 3 numbers hece we use them, however it's still to be seen if are useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "    \n",
    "    original_shape = list(v.shape)\n",
    "    q = q.view(-1, 4)\n",
    "    v = v.view(-1, 3)\n",
    "    \n",
    "    qvec = q[:, 1:]\n",
    "    uv = torch.cross(qvec, v, dim=1)\n",
    "    uuv = torch.cross(qvec, uv, dim=1)\n",
    "    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)\n",
    "\n",
    "def random_rotation(data):\n",
    "    x_xyz = data[0].transpose(0,1)\n",
    "    r = torch.rand(3)\n",
    "    sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-r[:1]),torch.sqrt(r[:1]),2*math.pi*r[1:2],2*math.pi*r[2:3]\n",
    "    q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                   sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=0).unsqueeze(0)\n",
    "    x_xyz = qrot(q.expand(x_xyz.shape[0],-1), x_xyz).squeeze(0).transpose(0,1)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def normalize(data):\n",
    "    sq = False\n",
    "    if data[0].ndim < 3:\n",
    "        data[0].unsqueeze_(0)\n",
    "        data[4].unsqueeze_(0)\n",
    "        sq = True\n",
    "    x_xyz      = (data[0] - x_xyz_mean)          / x_xyz_std\n",
    "    x_mulliken = (data[4] - x_qm9_mulliken_mean) / x_qm9_mulliken_std\n",
    "    if sq:\n",
    "        x_xyz.squeeze_(0)\n",
    "        x_mulliken.squeeze_(0)\n",
    "    return (x_xyz, data[1],data[2],data[3],x_mulliken)\n",
    "\n",
    "def canonize(data):\n",
    "    xyz,type,ext,atom,mulliken = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    i_max_atom = torch.nonzero(atom != -1).max() + 1\n",
    "    mask_atoms = torch.ones ((max_atoms, ), dtype=torch.uint8)\n",
    "    zeros      = torch.zeros((i_max_atom,), dtype=torch.uint8)\n",
    "    mask_atoms[:zeros.shape[0],] = zeros\n",
    "    n_atoms = i_max_atom\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask],mulliken[:,mask] = 0,-1,1,-1,0\n",
    "    return (xyz,type,ext,atom,mulliken, mask_atoms, n_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD=0.05\n",
    "def label_smoothing(y):\n",
    "    r = y[0] + torch.zeros_like(y[0]).uniform_(-THRESHOLD, THRESHOLD).masked_fill(y[0]==0, 0.)\n",
    "    return (r, y[1], y[2], y[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build `data` bunch etc. for fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                       enumerate(zip(x_xyz,x_type,x_ext,x_atom,x_qm9_mulliken))),label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = data.split_by_idx(idx_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scalar[0].sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l(o): return np.array(y_scalar[o.i].sum(axis=0), dtype=np.float32)\n",
    "data = data.label_from_func(func=l,label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfms = [normalize, canonize]\n",
    "tta_tfms = list(tfms)\n",
    "#tta_tfms.insert(0,random_rotation)\n",
    "data = data.transform((tta_tfms, tfms))#.transform_y(([label_smoothing], None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelList (78217 items)\n",
       "x: ItemList\n",
       "16 8 atoms 7 couplings,17 8 atoms 7 couplings,18 8 atoms 7 couplings,19 8 atoms 7 couplings,20 8 atoms 7 couplings\n",
       "y: ScalarCouplingList\n",
       "tensor([ 83.5430,  -2.3783,   0.0000, -11.7004, -11.6979,   3.2528,  13.6913,\n",
       "          3.2521,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000]),tensor([ 83.5417,  -2.3786, -11.7004,   0.0000, -11.6996,  13.6924,   3.2525,\n",
       "          3.2527,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000]),tensor([ 83.5484,  -2.3772, -11.6979, -11.6996,   0.0000,   3.2524,   3.2524,\n",
       "         13.6921,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000]),tensor([ -2.3788,  83.5418,   3.2528,  13.6924,   3.2524,   0.0000, -11.7004,\n",
       "        -11.6993,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000]),tensor([ -2.3785,  83.5430,  13.6913,   3.2525,   3.2524, -11.7004,   0.0000,\n",
       "        -11.6976,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000])\n",
       "Path: ."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove ext\n",
    "data.train_ds.filter_by_func(lambda item, _: len(item.data[2].cpu().numpy()[item.data[2].cpu().numpy()==0]) == 0)\n",
    "data.valid_ds.filter_by_func(lambda item, _: len(item.data[2].cpu().numpy()[item.data[2].cpu().numpy()==0]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Whole model here, self-contained (needs some cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMetric(LearnerCallback):\n",
    "    _order=-20 # Needs to run before the recorder\n",
    "    def __init__(self, learn, val_only=True):\n",
    "        super().__init__(learn)\n",
    "        self.val_only=val_only\n",
    "        self.metric = LMAEMaskedLoss(return_all=True, exclude_ext=True)\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.val_only: self.learn.recorder.add_metric_names(['tLMAE'])\n",
    "        self.learn.recorder.add_metric_names(['üëâüèªLMAEüëàüèª'] + [f'{i} {types[i]}' for i in range(n_types)])\n",
    "            \n",
    "    def on_batch_end(self, train, last_output, last_target, **kwargs):\n",
    "        if self.val_only and train: return \n",
    "        preds,targs = self.preds[int(train)], self.targs[int(train)] # 0 val 1 train\n",
    "        \n",
    "        if preds is None:\n",
    "            targs, preds = last_target.detach().cpu(),[t.detach().cpu() for t in listify(last_output)]\n",
    "        else:\n",
    "            targs = torch.cat([targs,last_target.detach().cpu()], dim=0)\n",
    "            for i,(o,t) in enumerate(zip(last_output, last_target)):\n",
    "                preds[i] = torch.cat([preds[i], o.detach().cpu()], dim=0)\n",
    "        self.preds[int(train)], self.targs[int(train)] = preds,targs\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.targs, self.preds = [None, None], [None, None]\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if True: #(kwargs['epoch'] % max_atoms) == 0:\n",
    "            mets = []\n",
    "            if self.preds[1]: mets.append(self.metric.forward(self.preds[1], self.targs[1])[0]) # just tLMAE\n",
    "            if self.preds[0]: \n",
    "                mets.extend(self.metric.forward(self.preds[0], self.targs[0]))\n",
    "            return add_metrics(last_metrics, mets) if mets else None\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyDecoder(Module):\n",
    "    def __init__(self,dropout:float=0):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,tgt, memory, tgt_mask=None, memory_mask=None, \n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        return self.dropout(memory)\n",
    "\n",
    "class AtomTorchTransformer(Module):\n",
    "    def __init__(self,n_layers,n_heads,d_model,d_inner,embed_p:float=0,\n",
    "                 encoder_dropout:float=0,decoder_dropout:float=0,\n",
    "                 d_head=None,deep_decoder=False,dense_out=False, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)        \n",
    "        \n",
    "        self.transformer = nn.Transformer(num_encoder_layers=n_layers,\n",
    "                                          nhead=n_heads,d_model=d_model,dim_feedforward=d_inner,\n",
    "                                          dropout = encoder_dropout, \n",
    "                                          custom_decoder=DummyDecoder(dropout=decoder_dropout))\n",
    "        \n",
    "        channels_out = d_model*n_layers if dense_out else d_model\n",
    "        channels_out_scalar = channels_out + n_types + 1\n",
    "        if deep_decoder:\n",
    "            sl = [int(channels_out_scalar/(2**d)) for d in range(int(math.ceil(np.log2(channels_out_scalar/4)-1)))]\n",
    "            self.scalar = nn.Sequential(*(list(itertools.chain.from_iterable(\n",
    "                [[nn.Conv1d(sl[i],sl[i+1],1),nn.ReLU(),nn.BatchNorm1d(sl[i+1])] for i in range(len(sl)-1)])) + \n",
    "                [nn.Conv1d(sl[-1], 1, 1)]))\n",
    "        else:\n",
    "            self.scalar = nn.Conv1d(channels_out_scalar, 1, 1)\n",
    "        \n",
    "        n_atom_embedding = d_model//2\n",
    "        n_type_embedding = d_model - n_atom_embedding - 3 #- 1 - 1 -1 \n",
    "        self.type_embedding = nn.Embedding(len(types)+1,n_type_embedding)\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,n_atom_embedding)\n",
    "        self.drop_type, self.drop_atom = nn.Dropout(embed_p), nn.Dropout(embed_p)\n",
    "            \n",
    "    def forward(self,xyz,type,ext,atom,mulliken,mask_atoms,n_atoms):\n",
    "        bs, _, n_pts = xyz.shape        \n",
    "        t = self.drop_type(self.type_embedding((type+1).squeeze(1)))\n",
    "        a = self.drop_atom(self.atom_embedding((atom+1).squeeze(1)))\n",
    "        \n",
    "        x = xyz\n",
    "        x = torch.cat([x.transpose(1,2), t, a], dim=-1) * math.sqrt(self.d_model) # B,N(29),d_model\n",
    "\n",
    "        mask = mask_atoms.to(dtype=torch.bool) \n",
    "        x = x.transpose(0,1)\n",
    "        x = self.transformer(x, x, src_key_padding_mask=mask).permute(1,2,0)\n",
    "            \n",
    "        t_one_hot = torch.zeros(bs,n_types+1,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1, 1.)\n",
    "        scalar    = self.scalar(torch.cat([x, t_one_hot], dim=1)).squeeze(1)\n",
    "        \n",
    "        type = type.squeeze(1)\n",
    "        ext  = ext.squeeze(1)\n",
    "        valid_couplings_mask = (type != -1)\n",
    "        return valid_couplings_mask,type[valid_couplings_mask],ext[valid_couplings_mask],scalar[valid_couplings_mask]\n",
    "                    \n",
    "    def reset(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback allows to insert multiple stateful (not averaged) metrics in one pass. Addditionally we could add metrics for train if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model instantiation: where's all your TPUs/GPUs when you need a decent hyperparam sweep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShowGraph(LearnerCallback):\n",
    "    \"Update a graph of learner stats and metrics after each epoch.\"\n",
    "    def on_epoch_end(self, n_epochs:int, last_metrics:MetricsList, **kwargs)->bool:\n",
    "        \"If we have `last_metrics` plot them in our pbar graph\"\n",
    "        if last_metrics is not None and last_metrics[0] is not None:\n",
    "            rec = self.learn.recorder\n",
    "            iters = range_of(rec.losses)\n",
    "            val_iter = np.array(rec.nb_batches).cumsum()\n",
    "            x_bounds = (0, (n_epochs - len(rec.nb_batches)) * rec.nb_batches[-1] + len(rec.losses))\n",
    "            y_bounds = (min((min(Tensor(rec.losses)), min(Tensor(rec.val_losses)))), \n",
    "                        max((max(Tensor(rec.losses)), max(Tensor(rec.val_losses)))))\n",
    "            rec.pbar.update_graph([(iters, rec.losses), (val_iter, rec.val_losses)], x_bounds, y_bounds)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "n_layers=24 #6\n",
    "n_heads=16 #4\n",
    "d_model=256 #2048\n",
    "d_inner=2048*4 # 2048*2\n",
    "decoder_dropout=0.0\n",
    "\n",
    "deep_decoder = False\n",
    "dense_out    = False\n",
    "\n",
    "net = AtomTorchTransformer(n_layers=n_layers, n_heads=n_heads,d_model=d_model,d_inner=d_inner,\n",
    "                      embed_p=0, encoder_dropout=0., decoder_dropout=decoder_dropout,\n",
    "                      deep_decoder=deep_decoder, dense_out=dense_out)\n",
    "\n",
    "learner = Learner(data,net,loss_func=LMAEMaskedLoss(),callback_fns=ShowGraph).to_fp32()\n",
    "learner.callbacks.extend([\n",
    "    SaveModelCallback(learner, monitor='üëâüèªLMAEüëàüèª', mode='min'),\n",
    "    LMAEMetric(learner)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomTorchTransformer\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Conv1d               [1, 29]              266        True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 125]            1,125      True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 128]            768        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 125]            0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 100,893,295\n",
       "Total trainable params: 100,893,295\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : LMAEMaskedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied \n",
       "    SaveModelCallback\n",
       "    LMAEMetric"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub_fname = \"d_model2048decoder_dropout0.0n_layers6n_heads4d_inner4096loss-4.1465val-2.6932\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT loaded!  name 'sub_fname' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Attempting to load: {sub_fname}... \", end=\"\")\n",
    "    learner.load(sub_fname, strict=False,with_opt=False)\n",
    "    print(\"Loaded\")\n",
    "except Exception as e:\n",
    "    print(\"NOT loaded! \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural weight manual transplant \n",
    "if False:\n",
    "    for name,child in learner.model.named_children():\n",
    "        print(\"CHILD: \",name)\n",
    "        if not (name in ['scalar', 'atom_embedding', 'type_embedding']):\n",
    "            print(\"FREEZING\")\n",
    "            for param in child.parameters(): param.requires_grad = False\n",
    "        else:\n",
    "            for name,param in child.named_parameters(): \n",
    "                param.requires_grad = True\n",
    "    learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from optimizers import Novograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranger import Ranger\n",
    "from ralamb import Ralamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.opt_func = partial(torch.optim.Adam, betas=(0.9,0.99), eps=5e-4)\n",
    "#learner.opt_func = partial(RAdam,eps=1e-8)\n",
    "#learner.opt_func = partial(Novograd,eps=1e-8)\n",
    "learner.opt_func = partial(Ralamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learner.to_parallel()#.to_fp16()\n",
    "b = 0.2 #0.35\n",
    "data.batch_size = {1: int(b*(1/3)*4096//2), 2: int(b*(2/3)*4096//2), 3: int(b*4096//2)}[torch.cuda.device_count()]\n",
    "data.batch_size *= int(int(any([isinstance(cb, MixedPrecision) for cb in learner.callbacks]))*.8)+1 # 2x if fp16\n",
    "#data.batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real loss func. Need to test different auxiliary tasks weights: `magnetic_w`, `dipole_w`, `potential`, weights of indivial `lmae`s: `types_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.loss_func = LMAEMaskedLoss(types_w = [1] * 8)#,proxy_log=lambda x: x) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses, lrs = [], []\n",
    "wds = [None] #[1e-1,1e-2,1e-3,1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for wd in wds:\n",
    "    !rm models/bestmodel.pth\n",
    "    learner.lr_find(wd=wd)#num_it=800, stop_div=False)\n",
    "    learner.recorder.plot(suggestion=True)\n",
    "    losses.append(learner.recorder.losses)\n",
    "    lrs.append(learner.recorder.lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingPhase(length=519000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 300\n",
    "b_its = len(data.train_dl)\n",
    "ph1 = (TrainingPhase(epochs*b_its).schedule_hp('wd', (1e-5,1e-1)))\n",
    "gs = GeneralScheduler(learner, (ph1,))\n",
    "ph1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bestmodel_49\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='201' class='' max='250', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      80.40% [201/250 80:58:30<19:44:24]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>üëâüèªLMAEüëàüèª</th>\n",
       "      <th>0 1JHC</th>\n",
       "      <th>1 2JHH</th>\n",
       "      <th>2 1JHN</th>\n",
       "      <th>3 2JHN</th>\n",
       "      <th>4 2JHC</th>\n",
       "      <th>5 3JHH</th>\n",
       "      <th>6 3JHC</th>\n",
       "      <th>7 3JHN</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>-1.941227</td>\n",
       "      <td>-1.834383</td>\n",
       "      <td>-1.782793</td>\n",
       "      <td>-0.345874</td>\n",
       "      <td>-2.106712</td>\n",
       "      <td>-0.724459</td>\n",
       "      <td>-2.372476</td>\n",
       "      <td>-1.972014</td>\n",
       "      <td>-2.202477</td>\n",
       "      <td>-1.865117</td>\n",
       "      <td>-2.673219</td>\n",
       "      <td>23:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>-1.999355</td>\n",
       "      <td>-1.967216</td>\n",
       "      <td>-1.896709</td>\n",
       "      <td>-0.497966</td>\n",
       "      <td>-2.326706</td>\n",
       "      <td>-0.797102</td>\n",
       "      <td>-2.477362</td>\n",
       "      <td>-2.077526</td>\n",
       "      <td>-2.303958</td>\n",
       "      <td>-1.943194</td>\n",
       "      <td>-2.749860</td>\n",
       "      <td>23:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>-2.039129</td>\n",
       "      <td>-1.928958</td>\n",
       "      <td>-1.870467</td>\n",
       "      <td>-0.643280</td>\n",
       "      <td>-2.393408</td>\n",
       "      <td>-0.936794</td>\n",
       "      <td>-2.284434</td>\n",
       "      <td>-2.047473</td>\n",
       "      <td>-2.193491</td>\n",
       "      <td>-1.824450</td>\n",
       "      <td>-2.640408</td>\n",
       "      <td>23:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>-2.092899</td>\n",
       "      <td>-2.028335</td>\n",
       "      <td>-1.972866</td>\n",
       "      <td>-0.857991</td>\n",
       "      <td>-2.406072</td>\n",
       "      <td>-0.879458</td>\n",
       "      <td>-2.488930</td>\n",
       "      <td>-2.097606</td>\n",
       "      <td>-2.315978</td>\n",
       "      <td>-1.970246</td>\n",
       "      <td>-2.766650</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>-2.139467</td>\n",
       "      <td>-2.011097</td>\n",
       "      <td>-1.952036</td>\n",
       "      <td>-0.656689</td>\n",
       "      <td>-2.345375</td>\n",
       "      <td>-0.948157</td>\n",
       "      <td>-2.494774</td>\n",
       "      <td>-2.075543</td>\n",
       "      <td>-2.358957</td>\n",
       "      <td>-1.961684</td>\n",
       "      <td>-2.775106</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>-2.157303</td>\n",
       "      <td>-2.026816</td>\n",
       "      <td>-1.974220</td>\n",
       "      <td>-0.918562</td>\n",
       "      <td>-2.382319</td>\n",
       "      <td>-0.978413</td>\n",
       "      <td>-2.434853</td>\n",
       "      <td>-2.090604</td>\n",
       "      <td>-2.346374</td>\n",
       "      <td>-1.967258</td>\n",
       "      <td>-2.675380</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>-2.148622</td>\n",
       "      <td>-1.989623</td>\n",
       "      <td>-1.944444</td>\n",
       "      <td>-0.975779</td>\n",
       "      <td>-2.291133</td>\n",
       "      <td>-0.949448</td>\n",
       "      <td>-2.435213</td>\n",
       "      <td>-2.014188</td>\n",
       "      <td>-2.298755</td>\n",
       "      <td>-1.946026</td>\n",
       "      <td>-2.645012</td>\n",
       "      <td>23:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>-2.182971</td>\n",
       "      <td>-2.040275</td>\n",
       "      <td>-1.988839</td>\n",
       "      <td>-0.976502</td>\n",
       "      <td>-2.399496</td>\n",
       "      <td>-0.944304</td>\n",
       "      <td>-2.487610</td>\n",
       "      <td>-2.136223</td>\n",
       "      <td>-2.343214</td>\n",
       "      <td>-1.968388</td>\n",
       "      <td>-2.654974</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>-2.177977</td>\n",
       "      <td>-2.059753</td>\n",
       "      <td>-2.007333</td>\n",
       "      <td>-0.962092</td>\n",
       "      <td>-2.446563</td>\n",
       "      <td>-0.946963</td>\n",
       "      <td>-2.501626</td>\n",
       "      <td>-2.065511</td>\n",
       "      <td>-2.365331</td>\n",
       "      <td>-2.013248</td>\n",
       "      <td>-2.757331</td>\n",
       "      <td>23:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>-2.184012</td>\n",
       "      <td>-1.943537</td>\n",
       "      <td>-1.888844</td>\n",
       "      <td>-1.015979</td>\n",
       "      <td>-2.158828</td>\n",
       "      <td>-0.997078</td>\n",
       "      <td>-2.338098</td>\n",
       "      <td>-1.842026</td>\n",
       "      <td>-2.212333</td>\n",
       "      <td>-1.943332</td>\n",
       "      <td>-2.603076</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>-2.204272</td>\n",
       "      <td>-1.997205</td>\n",
       "      <td>-1.937943</td>\n",
       "      <td>-0.968376</td>\n",
       "      <td>-2.095295</td>\n",
       "      <td>-0.909487</td>\n",
       "      <td>-2.446422</td>\n",
       "      <td>-2.097612</td>\n",
       "      <td>-2.373447</td>\n",
       "      <td>-1.992770</td>\n",
       "      <td>-2.620137</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>-2.137762</td>\n",
       "      <td>-2.059860</td>\n",
       "      <td>-1.987737</td>\n",
       "      <td>-1.025872</td>\n",
       "      <td>-2.422518</td>\n",
       "      <td>-0.676886</td>\n",
       "      <td>-2.505782</td>\n",
       "      <td>-2.133898</td>\n",
       "      <td>-2.326051</td>\n",
       "      <td>-2.020735</td>\n",
       "      <td>-2.790152</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>-2.174698</td>\n",
       "      <td>-2.107640</td>\n",
       "      <td>-2.048239</td>\n",
       "      <td>-1.011209</td>\n",
       "      <td>-2.348186</td>\n",
       "      <td>-1.052108</td>\n",
       "      <td>-2.529100</td>\n",
       "      <td>-2.173522</td>\n",
       "      <td>-2.440243</td>\n",
       "      <td>-2.031837</td>\n",
       "      <td>-2.799705</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>-2.224411</td>\n",
       "      <td>-2.093073</td>\n",
       "      <td>-2.032079</td>\n",
       "      <td>-0.898178</td>\n",
       "      <td>-2.480727</td>\n",
       "      <td>-0.842841</td>\n",
       "      <td>-2.557336</td>\n",
       "      <td>-2.184901</td>\n",
       "      <td>-2.423837</td>\n",
       "      <td>-2.048681</td>\n",
       "      <td>-2.820135</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>-2.145775</td>\n",
       "      <td>-2.077864</td>\n",
       "      <td>-2.016101</td>\n",
       "      <td>-1.080304</td>\n",
       "      <td>-2.482627</td>\n",
       "      <td>-1.035662</td>\n",
       "      <td>-2.473862</td>\n",
       "      <td>-1.981850</td>\n",
       "      <td>-2.349119</td>\n",
       "      <td>-1.942669</td>\n",
       "      <td>-2.782717</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>-2.153490</td>\n",
       "      <td>-2.032305</td>\n",
       "      <td>-1.982996</td>\n",
       "      <td>-0.905694</td>\n",
       "      <td>-2.415990</td>\n",
       "      <td>-0.954703</td>\n",
       "      <td>-2.469506</td>\n",
       "      <td>-2.087150</td>\n",
       "      <td>-2.346702</td>\n",
       "      <td>-2.010582</td>\n",
       "      <td>-2.673644</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>-2.235407</td>\n",
       "      <td>-2.120943</td>\n",
       "      <td>-2.060710</td>\n",
       "      <td>-0.979622</td>\n",
       "      <td>-2.488245</td>\n",
       "      <td>-1.020004</td>\n",
       "      <td>-2.559719</td>\n",
       "      <td>-2.199493</td>\n",
       "      <td>-2.379394</td>\n",
       "      <td>-2.017539</td>\n",
       "      <td>-2.841665</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>-2.171542</td>\n",
       "      <td>-2.070037</td>\n",
       "      <td>-2.011464</td>\n",
       "      <td>-1.015407</td>\n",
       "      <td>-2.343573</td>\n",
       "      <td>-0.800956</td>\n",
       "      <td>-2.484492</td>\n",
       "      <td>-2.163019</td>\n",
       "      <td>-2.450624</td>\n",
       "      <td>-2.064575</td>\n",
       "      <td>-2.769064</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>-2.229492</td>\n",
       "      <td>-2.103618</td>\n",
       "      <td>-2.042743</td>\n",
       "      <td>-1.017218</td>\n",
       "      <td>-2.399280</td>\n",
       "      <td>-0.947521</td>\n",
       "      <td>-2.549212</td>\n",
       "      <td>-2.170905</td>\n",
       "      <td>-2.379367</td>\n",
       "      <td>-2.043335</td>\n",
       "      <td>-2.835105</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>-2.214920</td>\n",
       "      <td>-2.050359</td>\n",
       "      <td>-2.002436</td>\n",
       "      <td>-1.003851</td>\n",
       "      <td>-2.471699</td>\n",
       "      <td>-0.979408</td>\n",
       "      <td>-2.463802</td>\n",
       "      <td>-2.186993</td>\n",
       "      <td>-2.269502</td>\n",
       "      <td>-1.973994</td>\n",
       "      <td>-2.670240</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>-2.229372</td>\n",
       "      <td>-1.980177</td>\n",
       "      <td>-1.943054</td>\n",
       "      <td>-1.062798</td>\n",
       "      <td>-2.378947</td>\n",
       "      <td>-0.964845</td>\n",
       "      <td>-2.353252</td>\n",
       "      <td>-1.884049</td>\n",
       "      <td>-2.350711</td>\n",
       "      <td>-2.008280</td>\n",
       "      <td>-2.541550</td>\n",
       "      <td>24:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>-2.244234</td>\n",
       "      <td>-2.147727</td>\n",
       "      <td>-2.083211</td>\n",
       "      <td>-1.075367</td>\n",
       "      <td>-2.387837</td>\n",
       "      <td>-1.040671</td>\n",
       "      <td>-2.568253</td>\n",
       "      <td>-2.199901</td>\n",
       "      <td>-2.445738</td>\n",
       "      <td>-2.100883</td>\n",
       "      <td>-2.847039</td>\n",
       "      <td>24:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>-2.206306</td>\n",
       "      <td>-2.131660</td>\n",
       "      <td>-2.066263</td>\n",
       "      <td>-1.097932</td>\n",
       "      <td>-2.244039</td>\n",
       "      <td>-0.981345</td>\n",
       "      <td>-2.573049</td>\n",
       "      <td>-2.184461</td>\n",
       "      <td>-2.456611</td>\n",
       "      <td>-2.100274</td>\n",
       "      <td>-2.892395</td>\n",
       "      <td>24:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>-2.210800</td>\n",
       "      <td>-2.021849</td>\n",
       "      <td>-1.963127</td>\n",
       "      <td>-1.081964</td>\n",
       "      <td>-2.017209</td>\n",
       "      <td>-0.869492</td>\n",
       "      <td>-2.498863</td>\n",
       "      <td>-2.123334</td>\n",
       "      <td>-2.322858</td>\n",
       "      <td>-1.991318</td>\n",
       "      <td>-2.799977</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>-2.257297</td>\n",
       "      <td>-2.131563</td>\n",
       "      <td>-2.061011</td>\n",
       "      <td>-1.006508</td>\n",
       "      <td>-2.290430</td>\n",
       "      <td>-0.973826</td>\n",
       "      <td>-2.590475</td>\n",
       "      <td>-2.193618</td>\n",
       "      <td>-2.470959</td>\n",
       "      <td>-2.121948</td>\n",
       "      <td>-2.840327</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>-2.232378</td>\n",
       "      <td>-2.059321</td>\n",
       "      <td>-2.001303</td>\n",
       "      <td>-1.067750</td>\n",
       "      <td>-2.522676</td>\n",
       "      <td>-1.060704</td>\n",
       "      <td>-2.426889</td>\n",
       "      <td>-1.992621</td>\n",
       "      <td>-2.357348</td>\n",
       "      <td>-2.034475</td>\n",
       "      <td>-2.547961</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>-2.286398</td>\n",
       "      <td>-2.083610</td>\n",
       "      <td>-2.044237</td>\n",
       "      <td>-1.089865</td>\n",
       "      <td>-2.264625</td>\n",
       "      <td>-0.716708</td>\n",
       "      <td>-2.570469</td>\n",
       "      <td>-2.238427</td>\n",
       "      <td>-2.519507</td>\n",
       "      <td>-2.102190</td>\n",
       "      <td>-2.852105</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>-2.204442</td>\n",
       "      <td>-2.071515</td>\n",
       "      <td>-2.024627</td>\n",
       "      <td>-1.044251</td>\n",
       "      <td>-2.503682</td>\n",
       "      <td>-1.050400</td>\n",
       "      <td>-2.459839</td>\n",
       "      <td>-2.038749</td>\n",
       "      <td>-2.286057</td>\n",
       "      <td>-2.048554</td>\n",
       "      <td>-2.765484</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>-2.237230</td>\n",
       "      <td>-2.088058</td>\n",
       "      <td>-2.033135</td>\n",
       "      <td>-1.056100</td>\n",
       "      <td>-2.400541</td>\n",
       "      <td>-1.077982</td>\n",
       "      <td>-2.349006</td>\n",
       "      <td>-2.213468</td>\n",
       "      <td>-2.425111</td>\n",
       "      <td>-2.084833</td>\n",
       "      <td>-2.658035</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>-2.321278</td>\n",
       "      <td>-2.188685</td>\n",
       "      <td>-2.131053</td>\n",
       "      <td>-1.155490</td>\n",
       "      <td>-2.527766</td>\n",
       "      <td>-1.076704</td>\n",
       "      <td>-2.575073</td>\n",
       "      <td>-2.209712</td>\n",
       "      <td>-2.512217</td>\n",
       "      <td>-2.135508</td>\n",
       "      <td>-2.855954</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>-2.312600</td>\n",
       "      <td>-2.066935</td>\n",
       "      <td>-2.016592</td>\n",
       "      <td>-1.168530</td>\n",
       "      <td>-2.169249</td>\n",
       "      <td>-1.062506</td>\n",
       "      <td>-2.501201</td>\n",
       "      <td>-2.199297</td>\n",
       "      <td>-2.356874</td>\n",
       "      <td>-1.977775</td>\n",
       "      <td>-2.697300</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>-2.296098</td>\n",
       "      <td>-2.196438</td>\n",
       "      <td>-2.133025</td>\n",
       "      <td>-1.119448</td>\n",
       "      <td>-2.464934</td>\n",
       "      <td>-1.069366</td>\n",
       "      <td>-2.632448</td>\n",
       "      <td>-2.269588</td>\n",
       "      <td>-2.510348</td>\n",
       "      <td>-2.125065</td>\n",
       "      <td>-2.873003</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>-2.269356</td>\n",
       "      <td>-2.060786</td>\n",
       "      <td>-2.005211</td>\n",
       "      <td>-1.133543</td>\n",
       "      <td>-2.350886</td>\n",
       "      <td>-1.055610</td>\n",
       "      <td>-2.397668</td>\n",
       "      <td>-2.110425</td>\n",
       "      <td>-2.294240</td>\n",
       "      <td>-2.035651</td>\n",
       "      <td>-2.663663</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>-2.336694</td>\n",
       "      <td>-2.117833</td>\n",
       "      <td>-2.062526</td>\n",
       "      <td>-0.662047</td>\n",
       "      <td>-2.548268</td>\n",
       "      <td>-1.026811</td>\n",
       "      <td>-2.541347</td>\n",
       "      <td>-2.235897</td>\n",
       "      <td>-2.482954</td>\n",
       "      <td>-2.112597</td>\n",
       "      <td>-2.890289</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>-2.354968</td>\n",
       "      <td>-2.187024</td>\n",
       "      <td>-2.134117</td>\n",
       "      <td>-1.154062</td>\n",
       "      <td>-2.487843</td>\n",
       "      <td>-1.033571</td>\n",
       "      <td>-2.578030</td>\n",
       "      <td>-2.226922</td>\n",
       "      <td>-2.547419</td>\n",
       "      <td>-2.163606</td>\n",
       "      <td>-2.881484</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>-2.369909</td>\n",
       "      <td>-2.239003</td>\n",
       "      <td>-2.159625</td>\n",
       "      <td>-1.211020</td>\n",
       "      <td>-2.603702</td>\n",
       "      <td>-0.907018</td>\n",
       "      <td>-2.654800</td>\n",
       "      <td>-2.271109</td>\n",
       "      <td>-2.561835</td>\n",
       "      <td>-2.160580</td>\n",
       "      <td>-2.906934</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>-2.350842</td>\n",
       "      <td>-2.168594</td>\n",
       "      <td>-2.101647</td>\n",
       "      <td>-1.179175</td>\n",
       "      <td>-2.365417</td>\n",
       "      <td>-1.106006</td>\n",
       "      <td>-2.556554</td>\n",
       "      <td>-2.228022</td>\n",
       "      <td>-2.431730</td>\n",
       "      <td>-2.103968</td>\n",
       "      <td>-2.842305</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>-2.303843</td>\n",
       "      <td>-2.159598</td>\n",
       "      <td>-2.100041</td>\n",
       "      <td>-1.157495</td>\n",
       "      <td>-2.525964</td>\n",
       "      <td>-1.039031</td>\n",
       "      <td>-2.542210</td>\n",
       "      <td>-2.189100</td>\n",
       "      <td>-2.484381</td>\n",
       "      <td>-2.033115</td>\n",
       "      <td>-2.829032</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>-2.398606</td>\n",
       "      <td>-2.205107</td>\n",
       "      <td>-2.146850</td>\n",
       "      <td>-1.133444</td>\n",
       "      <td>-2.603328</td>\n",
       "      <td>-1.124368</td>\n",
       "      <td>-2.558707</td>\n",
       "      <td>-2.197645</td>\n",
       "      <td>-2.521766</td>\n",
       "      <td>-2.172514</td>\n",
       "      <td>-2.863028</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>-2.398975</td>\n",
       "      <td>-2.233366</td>\n",
       "      <td>-2.167760</td>\n",
       "      <td>-1.168367</td>\n",
       "      <td>-2.550787</td>\n",
       "      <td>-1.080488</td>\n",
       "      <td>-2.659817</td>\n",
       "      <td>-2.300461</td>\n",
       "      <td>-2.554577</td>\n",
       "      <td>-2.175114</td>\n",
       "      <td>-2.852467</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>-2.419935</td>\n",
       "      <td>-2.263170</td>\n",
       "      <td>-2.186699</td>\n",
       "      <td>-1.177743</td>\n",
       "      <td>-2.635946</td>\n",
       "      <td>-1.015500</td>\n",
       "      <td>-2.637805</td>\n",
       "      <td>-2.302322</td>\n",
       "      <td>-2.547252</td>\n",
       "      <td>-2.219992</td>\n",
       "      <td>-2.957034</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>-2.428290</td>\n",
       "      <td>-2.222930</td>\n",
       "      <td>-2.156477</td>\n",
       "      <td>-1.096947</td>\n",
       "      <td>-2.584741</td>\n",
       "      <td>-1.101717</td>\n",
       "      <td>-2.631896</td>\n",
       "      <td>-2.281075</td>\n",
       "      <td>-2.489784</td>\n",
       "      <td>-2.188212</td>\n",
       "      <td>-2.877445</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>-2.411438</td>\n",
       "      <td>-2.212358</td>\n",
       "      <td>-2.150173</td>\n",
       "      <td>-1.219250</td>\n",
       "      <td>-2.649529</td>\n",
       "      <td>-1.128878</td>\n",
       "      <td>-2.565663</td>\n",
       "      <td>-2.265973</td>\n",
       "      <td>-2.391095</td>\n",
       "      <td>-2.175936</td>\n",
       "      <td>-2.805063</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>-2.429271</td>\n",
       "      <td>-2.279752</td>\n",
       "      <td>-2.212847</td>\n",
       "      <td>-1.242761</td>\n",
       "      <td>-2.438481</td>\n",
       "      <td>-1.178065</td>\n",
       "      <td>-2.707682</td>\n",
       "      <td>-2.338285</td>\n",
       "      <td>-2.612435</td>\n",
       "      <td>-2.225209</td>\n",
       "      <td>-2.959862</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>-2.418628</td>\n",
       "      <td>-2.138943</td>\n",
       "      <td>-2.092245</td>\n",
       "      <td>-0.822928</td>\n",
       "      <td>-2.427222</td>\n",
       "      <td>-1.042116</td>\n",
       "      <td>-2.576020</td>\n",
       "      <td>-2.264556</td>\n",
       "      <td>-2.571002</td>\n",
       "      <td>-2.178916</td>\n",
       "      <td>-2.855201</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>-2.415613</td>\n",
       "      <td>-2.176639</td>\n",
       "      <td>-2.121867</td>\n",
       "      <td>-1.202503</td>\n",
       "      <td>-2.450539</td>\n",
       "      <td>-1.059950</td>\n",
       "      <td>-2.647185</td>\n",
       "      <td>-2.149589</td>\n",
       "      <td>-2.540902</td>\n",
       "      <td>-2.222590</td>\n",
       "      <td>-2.701682</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>-2.409225</td>\n",
       "      <td>-2.275245</td>\n",
       "      <td>-2.204087</td>\n",
       "      <td>-0.929851</td>\n",
       "      <td>-2.625492</td>\n",
       "      <td>-1.183029</td>\n",
       "      <td>-2.664039</td>\n",
       "      <td>-2.371694</td>\n",
       "      <td>-2.640646</td>\n",
       "      <td>-2.250751</td>\n",
       "      <td>-2.967191</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>-2.497123</td>\n",
       "      <td>-2.293762</td>\n",
       "      <td>-2.228618</td>\n",
       "      <td>-1.223230</td>\n",
       "      <td>-2.676104</td>\n",
       "      <td>-1.201954</td>\n",
       "      <td>-2.669426</td>\n",
       "      <td>-2.293970</td>\n",
       "      <td>-2.604848</td>\n",
       "      <td>-2.237735</td>\n",
       "      <td>-2.921677</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>-2.494128</td>\n",
       "      <td>-2.330723</td>\n",
       "      <td>-2.263289</td>\n",
       "      <td>-1.280869</td>\n",
       "      <td>-2.646762</td>\n",
       "      <td>-1.192238</td>\n",
       "      <td>-2.720213</td>\n",
       "      <td>-2.351094</td>\n",
       "      <td>-2.662394</td>\n",
       "      <td>-2.266592</td>\n",
       "      <td>-2.986146</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>-2.509300</td>\n",
       "      <td>-2.284210</td>\n",
       "      <td>-2.210526</td>\n",
       "      <td>-1.193601</td>\n",
       "      <td>-2.657753</td>\n",
       "      <td>-0.956644</td>\n",
       "      <td>-2.726141</td>\n",
       "      <td>-2.349113</td>\n",
       "      <td>-2.606098</td>\n",
       "      <td>-2.200145</td>\n",
       "      <td>-2.994714</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>-2.505100</td>\n",
       "      <td>-2.330287</td>\n",
       "      <td>-2.259385</td>\n",
       "      <td>-1.274268</td>\n",
       "      <td>-2.649404</td>\n",
       "      <td>-1.208854</td>\n",
       "      <td>-2.713385</td>\n",
       "      <td>-2.389776</td>\n",
       "      <td>-2.655113</td>\n",
       "      <td>-2.269236</td>\n",
       "      <td>-2.915045</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>-2.514015</td>\n",
       "      <td>-2.305126</td>\n",
       "      <td>-2.233503</td>\n",
       "      <td>-1.263777</td>\n",
       "      <td>-2.679776</td>\n",
       "      <td>-1.060830</td>\n",
       "      <td>-2.730034</td>\n",
       "      <td>-2.316014</td>\n",
       "      <td>-2.594635</td>\n",
       "      <td>-2.264380</td>\n",
       "      <td>-2.958579</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>-2.535667</td>\n",
       "      <td>-2.264067</td>\n",
       "      <td>-2.206403</td>\n",
       "      <td>-1.294402</td>\n",
       "      <td>-2.625763</td>\n",
       "      <td>-1.094241</td>\n",
       "      <td>-2.668435</td>\n",
       "      <td>-2.333759</td>\n",
       "      <td>-2.584418</td>\n",
       "      <td>-2.239273</td>\n",
       "      <td>-2.810938</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>-2.515337</td>\n",
       "      <td>-2.322587</td>\n",
       "      <td>-2.249089</td>\n",
       "      <td>-1.236830</td>\n",
       "      <td>-2.652127</td>\n",
       "      <td>-1.144181</td>\n",
       "      <td>-2.702503</td>\n",
       "      <td>-2.395712</td>\n",
       "      <td>-2.634666</td>\n",
       "      <td>-2.259656</td>\n",
       "      <td>-2.967041</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>-2.491257</td>\n",
       "      <td>-2.291399</td>\n",
       "      <td>-2.223856</td>\n",
       "      <td>-1.308164</td>\n",
       "      <td>-2.666445</td>\n",
       "      <td>-1.239293</td>\n",
       "      <td>-2.610696</td>\n",
       "      <td>-2.330790</td>\n",
       "      <td>-2.576623</td>\n",
       "      <td>-2.213677</td>\n",
       "      <td>-2.845165</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>-2.524421</td>\n",
       "      <td>-2.313673</td>\n",
       "      <td>-2.245098</td>\n",
       "      <td>-1.305194</td>\n",
       "      <td>-2.651415</td>\n",
       "      <td>-1.117256</td>\n",
       "      <td>-2.664693</td>\n",
       "      <td>-2.396037</td>\n",
       "      <td>-2.651009</td>\n",
       "      <td>-2.274304</td>\n",
       "      <td>-2.900876</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>-2.560217</td>\n",
       "      <td>-2.337214</td>\n",
       "      <td>-2.274666</td>\n",
       "      <td>-1.296567</td>\n",
       "      <td>-2.672622</td>\n",
       "      <td>-1.162098</td>\n",
       "      <td>-2.748418</td>\n",
       "      <td>-2.405835</td>\n",
       "      <td>-2.618362</td>\n",
       "      <td>-2.299976</td>\n",
       "      <td>-2.993451</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>-2.585567</td>\n",
       "      <td>-2.356685</td>\n",
       "      <td>-2.285339</td>\n",
       "      <td>-1.275314</td>\n",
       "      <td>-2.701494</td>\n",
       "      <td>-1.227317</td>\n",
       "      <td>-2.730620</td>\n",
       "      <td>-2.381468</td>\n",
       "      <td>-2.650029</td>\n",
       "      <td>-2.310541</td>\n",
       "      <td>-3.005932</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>-2.596939</td>\n",
       "      <td>-2.348920</td>\n",
       "      <td>-2.270026</td>\n",
       "      <td>-1.291820</td>\n",
       "      <td>-2.534134</td>\n",
       "      <td>-1.139449</td>\n",
       "      <td>-2.731703</td>\n",
       "      <td>-2.400953</td>\n",
       "      <td>-2.703999</td>\n",
       "      <td>-2.337169</td>\n",
       "      <td>-3.020983</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>-2.600149</td>\n",
       "      <td>-2.330139</td>\n",
       "      <td>-2.264798</td>\n",
       "      <td>-1.267186</td>\n",
       "      <td>-2.719502</td>\n",
       "      <td>-1.050391</td>\n",
       "      <td>-2.703783</td>\n",
       "      <td>-2.414086</td>\n",
       "      <td>-2.697574</td>\n",
       "      <td>-2.302794</td>\n",
       "      <td>-2.963072</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>-2.619060</td>\n",
       "      <td>-2.352239</td>\n",
       "      <td>-2.284634</td>\n",
       "      <td>-1.352014</td>\n",
       "      <td>-2.728993</td>\n",
       "      <td>-1.131579</td>\n",
       "      <td>-2.720826</td>\n",
       "      <td>-2.424618</td>\n",
       "      <td>-2.647931</td>\n",
       "      <td>-2.287206</td>\n",
       "      <td>-2.983905</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>-2.588251</td>\n",
       "      <td>-2.362782</td>\n",
       "      <td>-2.288079</td>\n",
       "      <td>-1.340942</td>\n",
       "      <td>-2.743959</td>\n",
       "      <td>-1.033506</td>\n",
       "      <td>-2.688038</td>\n",
       "      <td>-2.417247</td>\n",
       "      <td>-2.726770</td>\n",
       "      <td>-2.344267</td>\n",
       "      <td>-3.009901</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>-2.629120</td>\n",
       "      <td>-2.415274</td>\n",
       "      <td>-2.342343</td>\n",
       "      <td>-1.372076</td>\n",
       "      <td>-2.746633</td>\n",
       "      <td>-1.296897</td>\n",
       "      <td>-2.796491</td>\n",
       "      <td>-2.432932</td>\n",
       "      <td>-2.733032</td>\n",
       "      <td>-2.344000</td>\n",
       "      <td>-3.016682</td>\n",
       "      <td>24:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>-2.599506</td>\n",
       "      <td>-2.366012</td>\n",
       "      <td>-2.293158</td>\n",
       "      <td>-1.208038</td>\n",
       "      <td>-2.586337</td>\n",
       "      <td>-1.263765</td>\n",
       "      <td>-2.766568</td>\n",
       "      <td>-2.446920</td>\n",
       "      <td>-2.731588</td>\n",
       "      <td>-2.330015</td>\n",
       "      <td>-3.012033</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>-2.636452</td>\n",
       "      <td>-2.385170</td>\n",
       "      <td>-2.311254</td>\n",
       "      <td>-1.367965</td>\n",
       "      <td>-2.744726</td>\n",
       "      <td>-1.185665</td>\n",
       "      <td>-2.757996</td>\n",
       "      <td>-2.418697</td>\n",
       "      <td>-2.675871</td>\n",
       "      <td>-2.328808</td>\n",
       "      <td>-3.010300</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>-2.649441</td>\n",
       "      <td>-2.404433</td>\n",
       "      <td>-2.322889</td>\n",
       "      <td>-1.368322</td>\n",
       "      <td>-2.704767</td>\n",
       "      <td>-1.152479</td>\n",
       "      <td>-2.762550</td>\n",
       "      <td>-2.420724</td>\n",
       "      <td>-2.758617</td>\n",
       "      <td>-2.357183</td>\n",
       "      <td>-3.058469</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>-2.668711</td>\n",
       "      <td>-2.368526</td>\n",
       "      <td>-2.305506</td>\n",
       "      <td>-1.309129</td>\n",
       "      <td>-2.758514</td>\n",
       "      <td>-1.121626</td>\n",
       "      <td>-2.741780</td>\n",
       "      <td>-2.418430</td>\n",
       "      <td>-2.724844</td>\n",
       "      <td>-2.352977</td>\n",
       "      <td>-3.016748</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>-2.674366</td>\n",
       "      <td>-2.390153</td>\n",
       "      <td>-2.318357</td>\n",
       "      <td>-1.361654</td>\n",
       "      <td>-2.740654</td>\n",
       "      <td>-1.294295</td>\n",
       "      <td>-2.723320</td>\n",
       "      <td>-2.427065</td>\n",
       "      <td>-2.707530</td>\n",
       "      <td>-2.337083</td>\n",
       "      <td>-2.955254</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>-2.642281</td>\n",
       "      <td>-2.389044</td>\n",
       "      <td>-2.323253</td>\n",
       "      <td>-1.392382</td>\n",
       "      <td>-2.757170</td>\n",
       "      <td>-1.148429</td>\n",
       "      <td>-2.738605</td>\n",
       "      <td>-2.426905</td>\n",
       "      <td>-2.749766</td>\n",
       "      <td>-2.352865</td>\n",
       "      <td>-3.019902</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>-2.660734</td>\n",
       "      <td>-2.412329</td>\n",
       "      <td>-2.329638</td>\n",
       "      <td>-1.306088</td>\n",
       "      <td>-2.760318</td>\n",
       "      <td>-1.109600</td>\n",
       "      <td>-2.775799</td>\n",
       "      <td>-2.455343</td>\n",
       "      <td>-2.780542</td>\n",
       "      <td>-2.376631</td>\n",
       "      <td>-3.072788</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>-2.677819</td>\n",
       "      <td>-2.390423</td>\n",
       "      <td>-2.315920</td>\n",
       "      <td>-1.241116</td>\n",
       "      <td>-2.747227</td>\n",
       "      <td>-1.115050</td>\n",
       "      <td>-2.800870</td>\n",
       "      <td>-2.453058</td>\n",
       "      <td>-2.727777</td>\n",
       "      <td>-2.353119</td>\n",
       "      <td>-3.089147</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>-2.691221</td>\n",
       "      <td>-2.422896</td>\n",
       "      <td>-2.351151</td>\n",
       "      <td>-1.318203</td>\n",
       "      <td>-2.706980</td>\n",
       "      <td>-1.248622</td>\n",
       "      <td>-2.775264</td>\n",
       "      <td>-2.468691</td>\n",
       "      <td>-2.803680</td>\n",
       "      <td>-2.400525</td>\n",
       "      <td>-3.087240</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>-2.698761</td>\n",
       "      <td>-2.414085</td>\n",
       "      <td>-2.341513</td>\n",
       "      <td>-1.367513</td>\n",
       "      <td>-2.737237</td>\n",
       "      <td>-1.261284</td>\n",
       "      <td>-2.778097</td>\n",
       "      <td>-2.466148</td>\n",
       "      <td>-2.678579</td>\n",
       "      <td>-2.378171</td>\n",
       "      <td>-3.065072</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>-2.688289</td>\n",
       "      <td>-2.385411</td>\n",
       "      <td>-2.321659</td>\n",
       "      <td>-1.272062</td>\n",
       "      <td>-2.648532</td>\n",
       "      <td>-1.188268</td>\n",
       "      <td>-2.797754</td>\n",
       "      <td>-2.444103</td>\n",
       "      <td>-2.807091</td>\n",
       "      <td>-2.390671</td>\n",
       "      <td>-3.024796</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>-2.723333</td>\n",
       "      <td>-2.426093</td>\n",
       "      <td>-2.347905</td>\n",
       "      <td>-1.340796</td>\n",
       "      <td>-2.776556</td>\n",
       "      <td>-1.177084</td>\n",
       "      <td>-2.816309</td>\n",
       "      <td>-2.489746</td>\n",
       "      <td>-2.761102</td>\n",
       "      <td>-2.379972</td>\n",
       "      <td>-3.041673</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>-2.721808</td>\n",
       "      <td>-2.418360</td>\n",
       "      <td>-2.338228</td>\n",
       "      <td>-1.329975</td>\n",
       "      <td>-2.795597</td>\n",
       "      <td>-1.208789</td>\n",
       "      <td>-2.782752</td>\n",
       "      <td>-2.468491</td>\n",
       "      <td>-2.705379</td>\n",
       "      <td>-2.389532</td>\n",
       "      <td>-3.025305</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>-2.744661</td>\n",
       "      <td>-2.459269</td>\n",
       "      <td>-2.377563</td>\n",
       "      <td>-1.409596</td>\n",
       "      <td>-2.774522</td>\n",
       "      <td>-1.161329</td>\n",
       "      <td>-2.849620</td>\n",
       "      <td>-2.486911</td>\n",
       "      <td>-2.813677</td>\n",
       "      <td>-2.410055</td>\n",
       "      <td>-3.114793</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>-2.751785</td>\n",
       "      <td>-2.486485</td>\n",
       "      <td>-2.402638</td>\n",
       "      <td>-1.428236</td>\n",
       "      <td>-2.790506</td>\n",
       "      <td>-1.294665</td>\n",
       "      <td>-2.815166</td>\n",
       "      <td>-2.513690</td>\n",
       "      <td>-2.844817</td>\n",
       "      <td>-2.405895</td>\n",
       "      <td>-3.128132</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>-2.751869</td>\n",
       "      <td>-2.472810</td>\n",
       "      <td>-2.391104</td>\n",
       "      <td>-1.413522</td>\n",
       "      <td>-2.833925</td>\n",
       "      <td>-1.223182</td>\n",
       "      <td>-2.835078</td>\n",
       "      <td>-2.501865</td>\n",
       "      <td>-2.819144</td>\n",
       "      <td>-2.412300</td>\n",
       "      <td>-3.089815</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>-2.785610</td>\n",
       "      <td>-2.460314</td>\n",
       "      <td>-2.375121</td>\n",
       "      <td>-1.334149</td>\n",
       "      <td>-2.801178</td>\n",
       "      <td>-1.198213</td>\n",
       "      <td>-2.847244</td>\n",
       "      <td>-2.500829</td>\n",
       "      <td>-2.807857</td>\n",
       "      <td>-2.416959</td>\n",
       "      <td>-3.094538</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>-2.762291</td>\n",
       "      <td>-2.418640</td>\n",
       "      <td>-2.340004</td>\n",
       "      <td>-1.406195</td>\n",
       "      <td>-2.341303</td>\n",
       "      <td>-1.280266</td>\n",
       "      <td>-2.830550</td>\n",
       "      <td>-2.510634</td>\n",
       "      <td>-2.832602</td>\n",
       "      <td>-2.416265</td>\n",
       "      <td>-3.102217</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>-2.789397</td>\n",
       "      <td>-2.467069</td>\n",
       "      <td>-2.387242</td>\n",
       "      <td>-1.354508</td>\n",
       "      <td>-2.829803</td>\n",
       "      <td>-1.310837</td>\n",
       "      <td>-2.836647</td>\n",
       "      <td>-2.502239</td>\n",
       "      <td>-2.804040</td>\n",
       "      <td>-2.377940</td>\n",
       "      <td>-3.081919</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>-2.766764</td>\n",
       "      <td>-2.488039</td>\n",
       "      <td>-2.411725</td>\n",
       "      <td>-1.388929</td>\n",
       "      <td>-2.820010</td>\n",
       "      <td>-1.345624</td>\n",
       "      <td>-2.845652</td>\n",
       "      <td>-2.529502</td>\n",
       "      <td>-2.826726</td>\n",
       "      <td>-2.414889</td>\n",
       "      <td>-3.122463</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>-2.798758</td>\n",
       "      <td>-2.476612</td>\n",
       "      <td>-2.393954</td>\n",
       "      <td>-1.407621</td>\n",
       "      <td>-2.804160</td>\n",
       "      <td>-1.291636</td>\n",
       "      <td>-2.800399</td>\n",
       "      <td>-2.514391</td>\n",
       "      <td>-2.784649</td>\n",
       "      <td>-2.414491</td>\n",
       "      <td>-3.134283</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>-2.803947</td>\n",
       "      <td>-2.501960</td>\n",
       "      <td>-2.421593</td>\n",
       "      <td>-1.447196</td>\n",
       "      <td>-2.811608</td>\n",
       "      <td>-1.312126</td>\n",
       "      <td>-2.868323</td>\n",
       "      <td>-2.544249</td>\n",
       "      <td>-2.837536</td>\n",
       "      <td>-2.439413</td>\n",
       "      <td>-3.112288</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>-2.807242</td>\n",
       "      <td>-2.469712</td>\n",
       "      <td>-2.392768</td>\n",
       "      <td>-1.365984</td>\n",
       "      <td>-2.863632</td>\n",
       "      <td>-1.253942</td>\n",
       "      <td>-2.828446</td>\n",
       "      <td>-2.525882</td>\n",
       "      <td>-2.835376</td>\n",
       "      <td>-2.392109</td>\n",
       "      <td>-3.076776</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>-2.815160</td>\n",
       "      <td>-2.498277</td>\n",
       "      <td>-2.415007</td>\n",
       "      <td>-1.450937</td>\n",
       "      <td>-2.849390</td>\n",
       "      <td>-1.318934</td>\n",
       "      <td>-2.832626</td>\n",
       "      <td>-2.499322</td>\n",
       "      <td>-2.816797</td>\n",
       "      <td>-2.443438</td>\n",
       "      <td>-3.108612</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>-2.848837</td>\n",
       "      <td>-2.491243</td>\n",
       "      <td>-2.415036</td>\n",
       "      <td>-1.394687</td>\n",
       "      <td>-2.775386</td>\n",
       "      <td>-1.305692</td>\n",
       "      <td>-2.847786</td>\n",
       "      <td>-2.548898</td>\n",
       "      <td>-2.853063</td>\n",
       "      <td>-2.456735</td>\n",
       "      <td>-3.138042</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>-2.803488</td>\n",
       "      <td>-2.518101</td>\n",
       "      <td>-2.438196</td>\n",
       "      <td>-1.419201</td>\n",
       "      <td>-2.838905</td>\n",
       "      <td>-1.294362</td>\n",
       "      <td>-2.901827</td>\n",
       "      <td>-2.547893</td>\n",
       "      <td>-2.888877</td>\n",
       "      <td>-2.474705</td>\n",
       "      <td>-3.139798</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>-2.850957</td>\n",
       "      <td>-2.517725</td>\n",
       "      <td>-2.433598</td>\n",
       "      <td>-1.474851</td>\n",
       "      <td>-2.856267</td>\n",
       "      <td>-1.284250</td>\n",
       "      <td>-2.877252</td>\n",
       "      <td>-2.555824</td>\n",
       "      <td>-2.836985</td>\n",
       "      <td>-2.454163</td>\n",
       "      <td>-3.129194</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>-2.858458</td>\n",
       "      <td>-2.505112</td>\n",
       "      <td>-2.425834</td>\n",
       "      <td>-1.412493</td>\n",
       "      <td>-2.850857</td>\n",
       "      <td>-1.327672</td>\n",
       "      <td>-2.847011</td>\n",
       "      <td>-2.535707</td>\n",
       "      <td>-2.901330</td>\n",
       "      <td>-2.472652</td>\n",
       "      <td>-3.058950</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>-2.845262</td>\n",
       "      <td>-2.512957</td>\n",
       "      <td>-2.428133</td>\n",
       "      <td>-1.376245</td>\n",
       "      <td>-2.771482</td>\n",
       "      <td>-1.328928</td>\n",
       "      <td>-2.871797</td>\n",
       "      <td>-2.544717</td>\n",
       "      <td>-2.899223</td>\n",
       "      <td>-2.484233</td>\n",
       "      <td>-3.148436</td>\n",
       "      <td>24:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>-2.832006</td>\n",
       "      <td>-2.471847</td>\n",
       "      <td>-2.397560</td>\n",
       "      <td>-1.319555</td>\n",
       "      <td>-2.863330</td>\n",
       "      <td>-1.309427</td>\n",
       "      <td>-2.803351</td>\n",
       "      <td>-2.549222</td>\n",
       "      <td>-2.807506</td>\n",
       "      <td>-2.430234</td>\n",
       "      <td>-3.097855</td>\n",
       "      <td>24:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>-2.876266</td>\n",
       "      <td>-2.526401</td>\n",
       "      <td>-2.440781</td>\n",
       "      <td>-1.403740</td>\n",
       "      <td>-2.895110</td>\n",
       "      <td>-1.274668</td>\n",
       "      <td>-2.908062</td>\n",
       "      <td>-2.551071</td>\n",
       "      <td>-2.875747</td>\n",
       "      <td>-2.465072</td>\n",
       "      <td>-3.152780</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>-2.881344</td>\n",
       "      <td>-2.520908</td>\n",
       "      <td>-2.435889</td>\n",
       "      <td>-1.478866</td>\n",
       "      <td>-2.855672</td>\n",
       "      <td>-1.306940</td>\n",
       "      <td>-2.873646</td>\n",
       "      <td>-2.497218</td>\n",
       "      <td>-2.875262</td>\n",
       "      <td>-2.480264</td>\n",
       "      <td>-3.119246</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>-2.892761</td>\n",
       "      <td>-2.559160</td>\n",
       "      <td>-2.476511</td>\n",
       "      <td>-1.494252</td>\n",
       "      <td>-2.914198</td>\n",
       "      <td>-1.360666</td>\n",
       "      <td>-2.885627</td>\n",
       "      <td>-2.580604</td>\n",
       "      <td>-2.913158</td>\n",
       "      <td>-2.498998</td>\n",
       "      <td>-3.164586</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>-2.889518</td>\n",
       "      <td>-2.535915</td>\n",
       "      <td>-2.454730</td>\n",
       "      <td>-1.494672</td>\n",
       "      <td>-2.827488</td>\n",
       "      <td>-1.343916</td>\n",
       "      <td>-2.875175</td>\n",
       "      <td>-2.574602</td>\n",
       "      <td>-2.864832</td>\n",
       "      <td>-2.488051</td>\n",
       "      <td>-3.169107</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>-2.886243</td>\n",
       "      <td>-2.537867</td>\n",
       "      <td>-2.454622</td>\n",
       "      <td>-1.449912</td>\n",
       "      <td>-2.780440</td>\n",
       "      <td>-1.392990</td>\n",
       "      <td>-2.898153</td>\n",
       "      <td>-2.578726</td>\n",
       "      <td>-2.871006</td>\n",
       "      <td>-2.506301</td>\n",
       "      <td>-3.159447</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>-2.939842</td>\n",
       "      <td>-2.550184</td>\n",
       "      <td>-2.465303</td>\n",
       "      <td>-1.494641</td>\n",
       "      <td>-2.865271</td>\n",
       "      <td>-1.381739</td>\n",
       "      <td>-2.908250</td>\n",
       "      <td>-2.566573</td>\n",
       "      <td>-2.865285</td>\n",
       "      <td>-2.482519</td>\n",
       "      <td>-3.158144</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>-2.926675</td>\n",
       "      <td>-2.541516</td>\n",
       "      <td>-2.451971</td>\n",
       "      <td>-1.466411</td>\n",
       "      <td>-2.837897</td>\n",
       "      <td>-1.347074</td>\n",
       "      <td>-2.901448</td>\n",
       "      <td>-2.572798</td>\n",
       "      <td>-2.878589</td>\n",
       "      <td>-2.471461</td>\n",
       "      <td>-3.140091</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>-2.946070</td>\n",
       "      <td>-2.506557</td>\n",
       "      <td>-2.431224</td>\n",
       "      <td>-1.339160</td>\n",
       "      <td>-2.866542</td>\n",
       "      <td>-1.264262</td>\n",
       "      <td>-2.880176</td>\n",
       "      <td>-2.591655</td>\n",
       "      <td>-2.862968</td>\n",
       "      <td>-2.478051</td>\n",
       "      <td>-3.166976</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>-2.922211</td>\n",
       "      <td>-2.520290</td>\n",
       "      <td>-2.440611</td>\n",
       "      <td>-1.367101</td>\n",
       "      <td>-2.909891</td>\n",
       "      <td>-1.385567</td>\n",
       "      <td>-2.855728</td>\n",
       "      <td>-2.568466</td>\n",
       "      <td>-2.901400</td>\n",
       "      <td>-2.487520</td>\n",
       "      <td>-3.049212</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>-2.944293</td>\n",
       "      <td>-2.578108</td>\n",
       "      <td>-2.486085</td>\n",
       "      <td>-1.495133</td>\n",
       "      <td>-2.922470</td>\n",
       "      <td>-1.366186</td>\n",
       "      <td>-2.905681</td>\n",
       "      <td>-2.603364</td>\n",
       "      <td>-2.900341</td>\n",
       "      <td>-2.515553</td>\n",
       "      <td>-3.179953</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>-2.975453</td>\n",
       "      <td>-2.537302</td>\n",
       "      <td>-2.449746</td>\n",
       "      <td>-1.373771</td>\n",
       "      <td>-2.862767</td>\n",
       "      <td>-1.202476</td>\n",
       "      <td>-2.932485</td>\n",
       "      <td>-2.589921</td>\n",
       "      <td>-2.936080</td>\n",
       "      <td>-2.518492</td>\n",
       "      <td>-3.181980</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>-2.991725</td>\n",
       "      <td>-2.579119</td>\n",
       "      <td>-2.487384</td>\n",
       "      <td>-1.433565</td>\n",
       "      <td>-2.917643</td>\n",
       "      <td>-1.353926</td>\n",
       "      <td>-2.909509</td>\n",
       "      <td>-2.610378</td>\n",
       "      <td>-2.941466</td>\n",
       "      <td>-2.542737</td>\n",
       "      <td>-3.189845</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>-2.977481</td>\n",
       "      <td>-2.570444</td>\n",
       "      <td>-2.480957</td>\n",
       "      <td>-1.426034</td>\n",
       "      <td>-2.817314</td>\n",
       "      <td>-1.351527</td>\n",
       "      <td>-2.951471</td>\n",
       "      <td>-2.593673</td>\n",
       "      <td>-2.957827</td>\n",
       "      <td>-2.544832</td>\n",
       "      <td>-3.204978</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>-2.976741</td>\n",
       "      <td>-2.576018</td>\n",
       "      <td>-2.486081</td>\n",
       "      <td>-1.539881</td>\n",
       "      <td>-2.851217</td>\n",
       "      <td>-1.337745</td>\n",
       "      <td>-2.922424</td>\n",
       "      <td>-2.586278</td>\n",
       "      <td>-2.944265</td>\n",
       "      <td>-2.536367</td>\n",
       "      <td>-3.170469</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>-3.009031</td>\n",
       "      <td>-2.608071</td>\n",
       "      <td>-2.512455</td>\n",
       "      <td>-1.548238</td>\n",
       "      <td>-2.957976</td>\n",
       "      <td>-1.322464</td>\n",
       "      <td>-2.929839</td>\n",
       "      <td>-2.614269</td>\n",
       "      <td>-2.979405</td>\n",
       "      <td>-2.547575</td>\n",
       "      <td>-3.199877</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>-3.016070</td>\n",
       "      <td>-2.575599</td>\n",
       "      <td>-2.491465</td>\n",
       "      <td>-1.461021</td>\n",
       "      <td>-2.844256</td>\n",
       "      <td>-1.400527</td>\n",
       "      <td>-2.925391</td>\n",
       "      <td>-2.601826</td>\n",
       "      <td>-2.957355</td>\n",
       "      <td>-2.536653</td>\n",
       "      <td>-3.204690</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>-3.018826</td>\n",
       "      <td>-2.599022</td>\n",
       "      <td>-2.510911</td>\n",
       "      <td>-1.537526</td>\n",
       "      <td>-2.903540</td>\n",
       "      <td>-1.391819</td>\n",
       "      <td>-2.928595</td>\n",
       "      <td>-2.640574</td>\n",
       "      <td>-2.935350</td>\n",
       "      <td>-2.550050</td>\n",
       "      <td>-3.199834</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>-3.023865</td>\n",
       "      <td>-2.573475</td>\n",
       "      <td>-2.484567</td>\n",
       "      <td>-1.562744</td>\n",
       "      <td>-2.958591</td>\n",
       "      <td>-1.289249</td>\n",
       "      <td>-2.878503</td>\n",
       "      <td>-2.614137</td>\n",
       "      <td>-2.910514</td>\n",
       "      <td>-2.532130</td>\n",
       "      <td>-3.130664</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>-3.045490</td>\n",
       "      <td>-2.545980</td>\n",
       "      <td>-2.467584</td>\n",
       "      <td>-1.551603</td>\n",
       "      <td>-2.892396</td>\n",
       "      <td>-1.406772</td>\n",
       "      <td>-2.847792</td>\n",
       "      <td>-2.620590</td>\n",
       "      <td>-2.890662</td>\n",
       "      <td>-2.487542</td>\n",
       "      <td>-3.043314</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>-3.052038</td>\n",
       "      <td>-2.629085</td>\n",
       "      <td>-2.534111</td>\n",
       "      <td>-1.581913</td>\n",
       "      <td>-2.973937</td>\n",
       "      <td>-1.352020</td>\n",
       "      <td>-2.963987</td>\n",
       "      <td>-2.641079</td>\n",
       "      <td>-2.972510</td>\n",
       "      <td>-2.562470</td>\n",
       "      <td>-3.224973</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>-3.054380</td>\n",
       "      <td>-2.613433</td>\n",
       "      <td>-2.521049</td>\n",
       "      <td>-1.540191</td>\n",
       "      <td>-2.977076</td>\n",
       "      <td>-1.271251</td>\n",
       "      <td>-2.944747</td>\n",
       "      <td>-2.657880</td>\n",
       "      <td>-2.965950</td>\n",
       "      <td>-2.576248</td>\n",
       "      <td>-3.235050</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>-3.076631</td>\n",
       "      <td>-2.613350</td>\n",
       "      <td>-2.515797</td>\n",
       "      <td>-1.564954</td>\n",
       "      <td>-2.956259</td>\n",
       "      <td>-1.344741</td>\n",
       "      <td>-2.945791</td>\n",
       "      <td>-2.608691</td>\n",
       "      <td>-2.945668</td>\n",
       "      <td>-2.555890</td>\n",
       "      <td>-3.204380</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>-3.053207</td>\n",
       "      <td>-2.624552</td>\n",
       "      <td>-2.526287</td>\n",
       "      <td>-1.571627</td>\n",
       "      <td>-2.901593</td>\n",
       "      <td>-1.365937</td>\n",
       "      <td>-2.930504</td>\n",
       "      <td>-2.644942</td>\n",
       "      <td>-2.988871</td>\n",
       "      <td>-2.580338</td>\n",
       "      <td>-3.226485</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>-3.087494</td>\n",
       "      <td>-2.572523</td>\n",
       "      <td>-2.496818</td>\n",
       "      <td>-1.568023</td>\n",
       "      <td>-2.940641</td>\n",
       "      <td>-1.174915</td>\n",
       "      <td>-2.915086</td>\n",
       "      <td>-2.618368</td>\n",
       "      <td>-2.993845</td>\n",
       "      <td>-2.584270</td>\n",
       "      <td>-3.179393</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>-3.090013</td>\n",
       "      <td>-2.540932</td>\n",
       "      <td>-2.462891</td>\n",
       "      <td>-1.549985</td>\n",
       "      <td>-2.822969</td>\n",
       "      <td>-1.352420</td>\n",
       "      <td>-2.889010</td>\n",
       "      <td>-2.555074</td>\n",
       "      <td>-2.918082</td>\n",
       "      <td>-2.529202</td>\n",
       "      <td>-3.086389</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>-3.120723</td>\n",
       "      <td>-2.604172</td>\n",
       "      <td>-2.509449</td>\n",
       "      <td>-1.343811</td>\n",
       "      <td>-2.937976</td>\n",
       "      <td>-1.360053</td>\n",
       "      <td>-2.975558</td>\n",
       "      <td>-2.651579</td>\n",
       "      <td>-2.975206</td>\n",
       "      <td>-2.584253</td>\n",
       "      <td>-3.247153</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>-3.108978</td>\n",
       "      <td>-2.641528</td>\n",
       "      <td>-2.545616</td>\n",
       "      <td>-1.595094</td>\n",
       "      <td>-2.965684</td>\n",
       "      <td>-1.336029</td>\n",
       "      <td>-2.986708</td>\n",
       "      <td>-2.643371</td>\n",
       "      <td>-3.007648</td>\n",
       "      <td>-2.587616</td>\n",
       "      <td>-3.242779</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>-3.123174</td>\n",
       "      <td>-2.626734</td>\n",
       "      <td>-2.530234</td>\n",
       "      <td>-1.499334</td>\n",
       "      <td>-2.972640</td>\n",
       "      <td>-1.340753</td>\n",
       "      <td>-2.957853</td>\n",
       "      <td>-2.657135</td>\n",
       "      <td>-3.003288</td>\n",
       "      <td>-2.576289</td>\n",
       "      <td>-3.234582</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>-3.140232</td>\n",
       "      <td>-2.607609</td>\n",
       "      <td>-2.513531</td>\n",
       "      <td>-1.561663</td>\n",
       "      <td>-2.805526</td>\n",
       "      <td>-1.373161</td>\n",
       "      <td>-2.964334</td>\n",
       "      <td>-2.658663</td>\n",
       "      <td>-2.967225</td>\n",
       "      <td>-2.555015</td>\n",
       "      <td>-3.222666</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>-3.148816</td>\n",
       "      <td>-2.671544</td>\n",
       "      <td>-2.571362</td>\n",
       "      <td>-1.613323</td>\n",
       "      <td>-2.983884</td>\n",
       "      <td>-1.409280</td>\n",
       "      <td>-2.971404</td>\n",
       "      <td>-2.692989</td>\n",
       "      <td>-3.033265</td>\n",
       "      <td>-2.607479</td>\n",
       "      <td>-3.259277</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>-3.167458</td>\n",
       "      <td>-2.634896</td>\n",
       "      <td>-2.538946</td>\n",
       "      <td>-1.412472</td>\n",
       "      <td>-3.021084</td>\n",
       "      <td>-1.410750</td>\n",
       "      <td>-2.966135</td>\n",
       "      <td>-2.672255</td>\n",
       "      <td>-2.990514</td>\n",
       "      <td>-2.590318</td>\n",
       "      <td>-3.248044</td>\n",
       "      <td>24:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>-3.186349</td>\n",
       "      <td>-2.665493</td>\n",
       "      <td>-2.566345</td>\n",
       "      <td>-1.621258</td>\n",
       "      <td>-3.015658</td>\n",
       "      <td>-1.381360</td>\n",
       "      <td>-2.978213</td>\n",
       "      <td>-2.675162</td>\n",
       "      <td>-3.011587</td>\n",
       "      <td>-2.600805</td>\n",
       "      <td>-3.246715</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>-3.163713</td>\n",
       "      <td>-2.640441</td>\n",
       "      <td>-2.551862</td>\n",
       "      <td>-1.617740</td>\n",
       "      <td>-3.004232</td>\n",
       "      <td>-1.428316</td>\n",
       "      <td>-2.930829</td>\n",
       "      <td>-2.677007</td>\n",
       "      <td>-2.970138</td>\n",
       "      <td>-2.576785</td>\n",
       "      <td>-3.209848</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>-3.199841</td>\n",
       "      <td>-2.674072</td>\n",
       "      <td>-2.576794</td>\n",
       "      <td>-1.492971</td>\n",
       "      <td>-3.024576</td>\n",
       "      <td>-1.478886</td>\n",
       "      <td>-2.996440</td>\n",
       "      <td>-2.693060</td>\n",
       "      <td>-3.058125</td>\n",
       "      <td>-2.613389</td>\n",
       "      <td>-3.256903</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>-3.209108</td>\n",
       "      <td>-2.680510</td>\n",
       "      <td>-2.577305</td>\n",
       "      <td>-1.626521</td>\n",
       "      <td>-2.996698</td>\n",
       "      <td>-1.393143</td>\n",
       "      <td>-3.002979</td>\n",
       "      <td>-2.696095</td>\n",
       "      <td>-3.019404</td>\n",
       "      <td>-2.624742</td>\n",
       "      <td>-3.258854</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>-3.234496</td>\n",
       "      <td>-2.662611</td>\n",
       "      <td>-2.570839</td>\n",
       "      <td>-1.633796</td>\n",
       "      <td>-2.965306</td>\n",
       "      <td>-1.449427</td>\n",
       "      <td>-2.975263</td>\n",
       "      <td>-2.668038</td>\n",
       "      <td>-3.020687</td>\n",
       "      <td>-2.617056</td>\n",
       "      <td>-3.237136</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>-3.214289</td>\n",
       "      <td>-2.648523</td>\n",
       "      <td>-2.555088</td>\n",
       "      <td>-1.535338</td>\n",
       "      <td>-3.022437</td>\n",
       "      <td>-1.347764</td>\n",
       "      <td>-2.985004</td>\n",
       "      <td>-2.663323</td>\n",
       "      <td>-3.021445</td>\n",
       "      <td>-2.618369</td>\n",
       "      <td>-3.247022</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>-3.233582</td>\n",
       "      <td>-2.697504</td>\n",
       "      <td>-2.592500</td>\n",
       "      <td>-1.640672</td>\n",
       "      <td>-3.030605</td>\n",
       "      <td>-1.415113</td>\n",
       "      <td>-2.984754</td>\n",
       "      <td>-2.692712</td>\n",
       "      <td>-3.069356</td>\n",
       "      <td>-2.642880</td>\n",
       "      <td>-3.263910</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>-3.262470</td>\n",
       "      <td>-2.708667</td>\n",
       "      <td>-2.604960</td>\n",
       "      <td>-1.610234</td>\n",
       "      <td>-3.014179</td>\n",
       "      <td>-1.501306</td>\n",
       "      <td>-3.017623</td>\n",
       "      <td>-2.711602</td>\n",
       "      <td>-3.072371</td>\n",
       "      <td>-2.637103</td>\n",
       "      <td>-3.275259</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>-3.263903</td>\n",
       "      <td>-2.697946</td>\n",
       "      <td>-2.596520</td>\n",
       "      <td>-1.638959</td>\n",
       "      <td>-3.028763</td>\n",
       "      <td>-1.466064</td>\n",
       "      <td>-3.002947</td>\n",
       "      <td>-2.718755</td>\n",
       "      <td>-3.059647</td>\n",
       "      <td>-2.632363</td>\n",
       "      <td>-3.224663</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>-3.281563</td>\n",
       "      <td>-2.707922</td>\n",
       "      <td>-2.607008</td>\n",
       "      <td>-1.663297</td>\n",
       "      <td>-3.042972</td>\n",
       "      <td>-1.425679</td>\n",
       "      <td>-3.014467</td>\n",
       "      <td>-2.719429</td>\n",
       "      <td>-3.061873</td>\n",
       "      <td>-2.654056</td>\n",
       "      <td>-3.274288</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>-3.297157</td>\n",
       "      <td>-2.679779</td>\n",
       "      <td>-2.583029</td>\n",
       "      <td>-1.608038</td>\n",
       "      <td>-3.047430</td>\n",
       "      <td>-1.378660</td>\n",
       "      <td>-2.982167</td>\n",
       "      <td>-2.704372</td>\n",
       "      <td>-3.072212</td>\n",
       "      <td>-2.603972</td>\n",
       "      <td>-3.267378</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>-3.310192</td>\n",
       "      <td>-2.720308</td>\n",
       "      <td>-2.611367</td>\n",
       "      <td>-1.674163</td>\n",
       "      <td>-3.024961</td>\n",
       "      <td>-1.427173</td>\n",
       "      <td>-3.005793</td>\n",
       "      <td>-2.730967</td>\n",
       "      <td>-3.090002</td>\n",
       "      <td>-2.655969</td>\n",
       "      <td>-3.281910</td>\n",
       "      <td>24:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>-3.330929</td>\n",
       "      <td>-2.715400</td>\n",
       "      <td>-2.611001</td>\n",
       "      <td>-1.652042</td>\n",
       "      <td>-3.054362</td>\n",
       "      <td>-1.457919</td>\n",
       "      <td>-3.017229</td>\n",
       "      <td>-2.717576</td>\n",
       "      <td>-3.069032</td>\n",
       "      <td>-2.663191</td>\n",
       "      <td>-3.256659</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>-3.336947</td>\n",
       "      <td>-2.721148</td>\n",
       "      <td>-2.616647</td>\n",
       "      <td>-1.658477</td>\n",
       "      <td>-3.068911</td>\n",
       "      <td>-1.383538</td>\n",
       "      <td>-3.026693</td>\n",
       "      <td>-2.735128</td>\n",
       "      <td>-3.104663</td>\n",
       "      <td>-2.658645</td>\n",
       "      <td>-3.297125</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>-3.342389</td>\n",
       "      <td>-2.725754</td>\n",
       "      <td>-2.618379</td>\n",
       "      <td>-1.570060</td>\n",
       "      <td>-3.073117</td>\n",
       "      <td>-1.486757</td>\n",
       "      <td>-3.023196</td>\n",
       "      <td>-2.733825</td>\n",
       "      <td>-3.098109</td>\n",
       "      <td>-2.673030</td>\n",
       "      <td>-3.288933</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>-3.366430</td>\n",
       "      <td>-2.750005</td>\n",
       "      <td>-2.640211</td>\n",
       "      <td>-1.686934</td>\n",
       "      <td>-3.083555</td>\n",
       "      <td>-1.462462</td>\n",
       "      <td>-3.044088</td>\n",
       "      <td>-2.746993</td>\n",
       "      <td>-3.120192</td>\n",
       "      <td>-2.677223</td>\n",
       "      <td>-3.300242</td>\n",
       "      <td>24:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>-3.376562</td>\n",
       "      <td>-2.724443</td>\n",
       "      <td>-2.621391</td>\n",
       "      <td>-1.625537</td>\n",
       "      <td>-3.015395</td>\n",
       "      <td>-1.507596</td>\n",
       "      <td>-3.029068</td>\n",
       "      <td>-2.741904</td>\n",
       "      <td>-3.101109</td>\n",
       "      <td>-2.673725</td>\n",
       "      <td>-3.276794</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>-3.377625</td>\n",
       "      <td>-2.728103</td>\n",
       "      <td>-2.620506</td>\n",
       "      <td>-1.592628</td>\n",
       "      <td>-3.091899</td>\n",
       "      <td>-1.475123</td>\n",
       "      <td>-3.033668</td>\n",
       "      <td>-2.739368</td>\n",
       "      <td>-3.077740</td>\n",
       "      <td>-2.668540</td>\n",
       "      <td>-3.285081</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>-3.390485</td>\n",
       "      <td>-2.728038</td>\n",
       "      <td>-2.624332</td>\n",
       "      <td>-1.677372</td>\n",
       "      <td>-3.061332</td>\n",
       "      <td>-1.500038</td>\n",
       "      <td>-3.023759</td>\n",
       "      <td>-2.736887</td>\n",
       "      <td>-3.060276</td>\n",
       "      <td>-2.656577</td>\n",
       "      <td>-3.278415</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>-3.399067</td>\n",
       "      <td>-2.746555</td>\n",
       "      <td>-2.640813</td>\n",
       "      <td>-1.684525</td>\n",
       "      <td>-3.078677</td>\n",
       "      <td>-1.506213</td>\n",
       "      <td>-3.017365</td>\n",
       "      <td>-2.751378</td>\n",
       "      <td>-3.113930</td>\n",
       "      <td>-2.689538</td>\n",
       "      <td>-3.284877</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>-3.429144</td>\n",
       "      <td>-2.741251</td>\n",
       "      <td>-2.635846</td>\n",
       "      <td>-1.675300</td>\n",
       "      <td>-3.059546</td>\n",
       "      <td>-1.495822</td>\n",
       "      <td>-3.034169</td>\n",
       "      <td>-2.748401</td>\n",
       "      <td>-3.099955</td>\n",
       "      <td>-2.685514</td>\n",
       "      <td>-3.288060</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>-3.447714</td>\n",
       "      <td>-2.729749</td>\n",
       "      <td>-2.627675</td>\n",
       "      <td>-1.671745</td>\n",
       "      <td>-3.039544</td>\n",
       "      <td>-1.481324</td>\n",
       "      <td>-3.014282</td>\n",
       "      <td>-2.751733</td>\n",
       "      <td>-3.117435</td>\n",
       "      <td>-2.658700</td>\n",
       "      <td>-3.286633</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>-3.463927</td>\n",
       "      <td>-2.748785</td>\n",
       "      <td>-2.640429</td>\n",
       "      <td>-1.688977</td>\n",
       "      <td>-3.038168</td>\n",
       "      <td>-1.470287</td>\n",
       "      <td>-3.041057</td>\n",
       "      <td>-2.754322</td>\n",
       "      <td>-3.129496</td>\n",
       "      <td>-2.696446</td>\n",
       "      <td>-3.304676</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>-3.484649</td>\n",
       "      <td>-2.763219</td>\n",
       "      <td>-2.654841</td>\n",
       "      <td>-1.670993</td>\n",
       "      <td>-3.099472</td>\n",
       "      <td>-1.487208</td>\n",
       "      <td>-3.053749</td>\n",
       "      <td>-2.759709</td>\n",
       "      <td>-3.145347</td>\n",
       "      <td>-2.704289</td>\n",
       "      <td>-3.317965</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>-3.470264</td>\n",
       "      <td>-2.754364</td>\n",
       "      <td>-2.647652</td>\n",
       "      <td>-1.633709</td>\n",
       "      <td>-3.106103</td>\n",
       "      <td>-1.489582</td>\n",
       "      <td>-3.040697</td>\n",
       "      <td>-2.765274</td>\n",
       "      <td>-3.112434</td>\n",
       "      <td>-2.708027</td>\n",
       "      <td>-3.325390</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>-3.498634</td>\n",
       "      <td>-2.777899</td>\n",
       "      <td>-2.665660</td>\n",
       "      <td>-1.717703</td>\n",
       "      <td>-3.106157</td>\n",
       "      <td>-1.489464</td>\n",
       "      <td>-3.056974</td>\n",
       "      <td>-2.775402</td>\n",
       "      <td>-3.145727</td>\n",
       "      <td>-2.699308</td>\n",
       "      <td>-3.334549</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-3.524358</td>\n",
       "      <td>-2.779062</td>\n",
       "      <td>-2.665976</td>\n",
       "      <td>-1.700383</td>\n",
       "      <td>-3.102879</td>\n",
       "      <td>-1.511399</td>\n",
       "      <td>-3.049989</td>\n",
       "      <td>-2.774983</td>\n",
       "      <td>-3.146098</td>\n",
       "      <td>-2.714436</td>\n",
       "      <td>-3.327643</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>-3.528999</td>\n",
       "      <td>-2.783566</td>\n",
       "      <td>-2.673421</td>\n",
       "      <td>-1.729000</td>\n",
       "      <td>-3.116796</td>\n",
       "      <td>-1.525244</td>\n",
       "      <td>-3.050988</td>\n",
       "      <td>-2.786685</td>\n",
       "      <td>-3.148961</td>\n",
       "      <td>-2.699470</td>\n",
       "      <td>-3.330227</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>-3.510907</td>\n",
       "      <td>-2.771380</td>\n",
       "      <td>-2.661070</td>\n",
       "      <td>-1.698235</td>\n",
       "      <td>-3.114248</td>\n",
       "      <td>-1.481971</td>\n",
       "      <td>-3.058973</td>\n",
       "      <td>-2.785053</td>\n",
       "      <td>-3.125787</td>\n",
       "      <td>-2.714657</td>\n",
       "      <td>-3.309636</td>\n",
       "      <td>24:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>-3.536337</td>\n",
       "      <td>-2.782305</td>\n",
       "      <td>-2.668254</td>\n",
       "      <td>-1.724769</td>\n",
       "      <td>-3.095029</td>\n",
       "      <td>-1.480554</td>\n",
       "      <td>-3.084009</td>\n",
       "      <td>-2.789533</td>\n",
       "      <td>-3.144680</td>\n",
       "      <td>-2.708140</td>\n",
       "      <td>-3.319320</td>\n",
       "      <td>26:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>-3.579187</td>\n",
       "      <td>-2.795812</td>\n",
       "      <td>-2.681358</td>\n",
       "      <td>-1.725529</td>\n",
       "      <td>-3.130307</td>\n",
       "      <td>-1.521754</td>\n",
       "      <td>-3.058314</td>\n",
       "      <td>-2.793965</td>\n",
       "      <td>-3.173229</td>\n",
       "      <td>-2.720212</td>\n",
       "      <td>-3.327558</td>\n",
       "      <td>24:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>-3.579671</td>\n",
       "      <td>-2.788933</td>\n",
       "      <td>-2.675586</td>\n",
       "      <td>-1.706269</td>\n",
       "      <td>-3.075121</td>\n",
       "      <td>-1.512255</td>\n",
       "      <td>-3.076337</td>\n",
       "      <td>-2.789824</td>\n",
       "      <td>-3.176711</td>\n",
       "      <td>-2.735681</td>\n",
       "      <td>-3.332492</td>\n",
       "      <td>25:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>-3.611345</td>\n",
       "      <td>-2.785928</td>\n",
       "      <td>-2.676104</td>\n",
       "      <td>-1.726449</td>\n",
       "      <td>-3.098796</td>\n",
       "      <td>-1.517555</td>\n",
       "      <td>-3.057952</td>\n",
       "      <td>-2.795224</td>\n",
       "      <td>-3.147592</td>\n",
       "      <td>-2.725957</td>\n",
       "      <td>-3.339308</td>\n",
       "      <td>24:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>-3.611669</td>\n",
       "      <td>-2.793661</td>\n",
       "      <td>-2.680012</td>\n",
       "      <td>-1.734024</td>\n",
       "      <td>-3.120558</td>\n",
       "      <td>-1.517205</td>\n",
       "      <td>-3.047423</td>\n",
       "      <td>-2.789975</td>\n",
       "      <td>-3.157612</td>\n",
       "      <td>-2.738816</td>\n",
       "      <td>-3.334486</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>-3.611865</td>\n",
       "      <td>-2.795734</td>\n",
       "      <td>-2.682445</td>\n",
       "      <td>-1.734230</td>\n",
       "      <td>-3.137507</td>\n",
       "      <td>-1.524617</td>\n",
       "      <td>-3.062099</td>\n",
       "      <td>-2.796036</td>\n",
       "      <td>-3.139085</td>\n",
       "      <td>-2.725779</td>\n",
       "      <td>-3.340207</td>\n",
       "      <td>24:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>-3.640631</td>\n",
       "      <td>-2.804236</td>\n",
       "      <td>-2.691032</td>\n",
       "      <td>-1.747635</td>\n",
       "      <td>-3.112746</td>\n",
       "      <td>-1.524162</td>\n",
       "      <td>-3.078815</td>\n",
       "      <td>-2.806307</td>\n",
       "      <td>-3.191413</td>\n",
       "      <td>-2.741724</td>\n",
       "      <td>-3.325457</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>-3.645743</td>\n",
       "      <td>-2.800950</td>\n",
       "      <td>-2.686885</td>\n",
       "      <td>-1.710660</td>\n",
       "      <td>-3.115466</td>\n",
       "      <td>-1.536530</td>\n",
       "      <td>-3.084726</td>\n",
       "      <td>-2.771708</td>\n",
       "      <td>-3.192068</td>\n",
       "      <td>-2.746426</td>\n",
       "      <td>-3.337495</td>\n",
       "      <td>24:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>-3.665498</td>\n",
       "      <td>-2.812028</td>\n",
       "      <td>-2.695868</td>\n",
       "      <td>-1.751233</td>\n",
       "      <td>-3.091591</td>\n",
       "      <td>-1.551265</td>\n",
       "      <td>-3.086647</td>\n",
       "      <td>-2.805597</td>\n",
       "      <td>-3.198156</td>\n",
       "      <td>-2.741600</td>\n",
       "      <td>-3.340860</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>-3.681913</td>\n",
       "      <td>-2.820322</td>\n",
       "      <td>-2.700439</td>\n",
       "      <td>-1.740584</td>\n",
       "      <td>-3.134766</td>\n",
       "      <td>-1.522939</td>\n",
       "      <td>-3.075127</td>\n",
       "      <td>-2.819891</td>\n",
       "      <td>-3.203687</td>\n",
       "      <td>-2.756108</td>\n",
       "      <td>-3.350408</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>-3.706971</td>\n",
       "      <td>-2.829581</td>\n",
       "      <td>-2.708993</td>\n",
       "      <td>-1.755769</td>\n",
       "      <td>-3.148401</td>\n",
       "      <td>-1.537578</td>\n",
       "      <td>-3.095850</td>\n",
       "      <td>-2.822098</td>\n",
       "      <td>-3.198086</td>\n",
       "      <td>-2.757595</td>\n",
       "      <td>-3.356571</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>-3.712521</td>\n",
       "      <td>-2.823844</td>\n",
       "      <td>-2.705250</td>\n",
       "      <td>-1.764119</td>\n",
       "      <td>-3.140083</td>\n",
       "      <td>-1.518713</td>\n",
       "      <td>-3.089878</td>\n",
       "      <td>-2.818132</td>\n",
       "      <td>-3.200197</td>\n",
       "      <td>-2.756845</td>\n",
       "      <td>-3.354030</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>-3.746148</td>\n",
       "      <td>-2.828039</td>\n",
       "      <td>-2.709671</td>\n",
       "      <td>-1.776554</td>\n",
       "      <td>-3.147223</td>\n",
       "      <td>-1.535236</td>\n",
       "      <td>-3.086112</td>\n",
       "      <td>-2.825674</td>\n",
       "      <td>-3.195399</td>\n",
       "      <td>-2.753227</td>\n",
       "      <td>-3.357945</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>-3.771105</td>\n",
       "      <td>-2.825876</td>\n",
       "      <td>-2.708497</td>\n",
       "      <td>-1.775759</td>\n",
       "      <td>-3.152625</td>\n",
       "      <td>-1.551192</td>\n",
       "      <td>-3.076025</td>\n",
       "      <td>-2.814263</td>\n",
       "      <td>-3.213557</td>\n",
       "      <td>-2.764948</td>\n",
       "      <td>-3.319605</td>\n",
       "      <td>24:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>-3.778578</td>\n",
       "      <td>-2.838781</td>\n",
       "      <td>-2.717693</td>\n",
       "      <td>-1.768858</td>\n",
       "      <td>-3.162602</td>\n",
       "      <td>-1.520030</td>\n",
       "      <td>-3.101948</td>\n",
       "      <td>-2.836457</td>\n",
       "      <td>-3.221500</td>\n",
       "      <td>-2.767963</td>\n",
       "      <td>-3.362189</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>-3.786456</td>\n",
       "      <td>-2.824016</td>\n",
       "      <td>-2.706037</td>\n",
       "      <td>-1.741212</td>\n",
       "      <td>-3.157229</td>\n",
       "      <td>-1.526355</td>\n",
       "      <td>-3.087732</td>\n",
       "      <td>-2.823772</td>\n",
       "      <td>-3.196325</td>\n",
       "      <td>-2.762459</td>\n",
       "      <td>-3.353209</td>\n",
       "      <td>24:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>-3.819635</td>\n",
       "      <td>-2.838093</td>\n",
       "      <td>-2.717434</td>\n",
       "      <td>-1.773205</td>\n",
       "      <td>-3.141151</td>\n",
       "      <td>-1.531946</td>\n",
       "      <td>-3.106000</td>\n",
       "      <td>-2.833107</td>\n",
       "      <td>-3.221883</td>\n",
       "      <td>-2.767414</td>\n",
       "      <td>-3.364766</td>\n",
       "      <td>24:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>-3.841115</td>\n",
       "      <td>-2.843491</td>\n",
       "      <td>-2.723076</td>\n",
       "      <td>-1.769378</td>\n",
       "      <td>-3.178355</td>\n",
       "      <td>-1.547391</td>\n",
       "      <td>-3.098248</td>\n",
       "      <td>-2.839963</td>\n",
       "      <td>-3.219421</td>\n",
       "      <td>-2.771544</td>\n",
       "      <td>-3.360303</td>\n",
       "      <td>24:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>-3.847722</td>\n",
       "      <td>-2.844691</td>\n",
       "      <td>-2.720989</td>\n",
       "      <td>-1.764576</td>\n",
       "      <td>-3.160449</td>\n",
       "      <td>-1.518022</td>\n",
       "      <td>-3.112075</td>\n",
       "      <td>-2.833745</td>\n",
       "      <td>-3.233390</td>\n",
       "      <td>-2.777799</td>\n",
       "      <td>-3.367858</td>\n",
       "      <td>24:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>-3.863830</td>\n",
       "      <td>-2.845570</td>\n",
       "      <td>-2.725775</td>\n",
       "      <td>-1.756519</td>\n",
       "      <td>-3.179397</td>\n",
       "      <td>-1.549508</td>\n",
       "      <td>-3.099320</td>\n",
       "      <td>-2.839698</td>\n",
       "      <td>-3.232088</td>\n",
       "      <td>-2.777084</td>\n",
       "      <td>-3.372591</td>\n",
       "      <td>24:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>-3.909702</td>\n",
       "      <td>-2.852569</td>\n",
       "      <td>-2.728740</td>\n",
       "      <td>-1.760183</td>\n",
       "      <td>-3.176253</td>\n",
       "      <td>-1.550454</td>\n",
       "      <td>-3.102982</td>\n",
       "      <td>-2.845654</td>\n",
       "      <td>-3.233261</td>\n",
       "      <td>-2.782820</td>\n",
       "      <td>-3.378316</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>-3.899172</td>\n",
       "      <td>-2.858920</td>\n",
       "      <td>-2.731332</td>\n",
       "      <td>-1.787550</td>\n",
       "      <td>-3.172967</td>\n",
       "      <td>-1.525627</td>\n",
       "      <td>-3.112803</td>\n",
       "      <td>-2.852818</td>\n",
       "      <td>-3.233136</td>\n",
       "      <td>-2.789347</td>\n",
       "      <td>-3.376409</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>-3.934157</td>\n",
       "      <td>-2.867330</td>\n",
       "      <td>-2.741663</td>\n",
       "      <td>-1.789305</td>\n",
       "      <td>-3.177750</td>\n",
       "      <td>-1.563689</td>\n",
       "      <td>-3.124787</td>\n",
       "      <td>-2.850881</td>\n",
       "      <td>-3.253447</td>\n",
       "      <td>-2.792722</td>\n",
       "      <td>-3.380719</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>-3.969914</td>\n",
       "      <td>-2.862046</td>\n",
       "      <td>-2.735919</td>\n",
       "      <td>-1.780218</td>\n",
       "      <td>-3.186797</td>\n",
       "      <td>-1.543035</td>\n",
       "      <td>-3.114923</td>\n",
       "      <td>-2.856070</td>\n",
       "      <td>-3.242039</td>\n",
       "      <td>-2.786931</td>\n",
       "      <td>-3.377345</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>-3.980917</td>\n",
       "      <td>-2.871226</td>\n",
       "      <td>-2.744692</td>\n",
       "      <td>-1.798973</td>\n",
       "      <td>-3.189687</td>\n",
       "      <td>-1.560938</td>\n",
       "      <td>-3.121820</td>\n",
       "      <td>-2.855757</td>\n",
       "      <td>-3.255031</td>\n",
       "      <td>-2.792807</td>\n",
       "      <td>-3.382521</td>\n",
       "      <td>24:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>-4.000115</td>\n",
       "      <td>-2.877249</td>\n",
       "      <td>-2.747906</td>\n",
       "      <td>-1.806929</td>\n",
       "      <td>-3.193937</td>\n",
       "      <td>-1.557621</td>\n",
       "      <td>-3.127756</td>\n",
       "      <td>-2.856198</td>\n",
       "      <td>-3.264030</td>\n",
       "      <td>-2.794945</td>\n",
       "      <td>-3.381832</td>\n",
       "      <td>24:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>-4.025322</td>\n",
       "      <td>-2.871997</td>\n",
       "      <td>-2.742511</td>\n",
       "      <td>-1.803485</td>\n",
       "      <td>-3.200574</td>\n",
       "      <td>-1.547429</td>\n",
       "      <td>-3.118839</td>\n",
       "      <td>-2.856649</td>\n",
       "      <td>-3.240599</td>\n",
       "      <td>-2.792477</td>\n",
       "      <td>-3.380039</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>-4.040103</td>\n",
       "      <td>-2.871097</td>\n",
       "      <td>-2.745279</td>\n",
       "      <td>-1.797473</td>\n",
       "      <td>-3.202721</td>\n",
       "      <td>-1.561584</td>\n",
       "      <td>-3.125654</td>\n",
       "      <td>-2.862113</td>\n",
       "      <td>-3.234870</td>\n",
       "      <td>-2.790891</td>\n",
       "      <td>-3.386923</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>-4.062837</td>\n",
       "      <td>-2.878364</td>\n",
       "      <td>-2.749654</td>\n",
       "      <td>-1.782876</td>\n",
       "      <td>-3.193371</td>\n",
       "      <td>-1.568243</td>\n",
       "      <td>-3.129679</td>\n",
       "      <td>-2.866998</td>\n",
       "      <td>-3.267945</td>\n",
       "      <td>-2.804314</td>\n",
       "      <td>-3.383808</td>\n",
       "      <td>25:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>-4.087267</td>\n",
       "      <td>-2.876245</td>\n",
       "      <td>-2.750206</td>\n",
       "      <td>-1.813280</td>\n",
       "      <td>-3.202407</td>\n",
       "      <td>-1.559017</td>\n",
       "      <td>-3.124482</td>\n",
       "      <td>-2.854403</td>\n",
       "      <td>-3.260710</td>\n",
       "      <td>-2.804210</td>\n",
       "      <td>-3.383141</td>\n",
       "      <td>24:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>-4.107983</td>\n",
       "      <td>-2.889164</td>\n",
       "      <td>-2.757777</td>\n",
       "      <td>-1.815737</td>\n",
       "      <td>-3.207238</td>\n",
       "      <td>-1.571351</td>\n",
       "      <td>-3.127650</td>\n",
       "      <td>-2.868882</td>\n",
       "      <td>-3.267485</td>\n",
       "      <td>-2.808052</td>\n",
       "      <td>-3.395820</td>\n",
       "      <td>24:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>-4.137262</td>\n",
       "      <td>-2.886802</td>\n",
       "      <td>-2.756346</td>\n",
       "      <td>-1.796982</td>\n",
       "      <td>-3.208330</td>\n",
       "      <td>-1.572008</td>\n",
       "      <td>-3.136733</td>\n",
       "      <td>-2.870672</td>\n",
       "      <td>-3.262703</td>\n",
       "      <td>-2.810048</td>\n",
       "      <td>-3.393291</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>-4.163132</td>\n",
       "      <td>-2.884052</td>\n",
       "      <td>-2.755767</td>\n",
       "      <td>-1.794577</td>\n",
       "      <td>-3.210801</td>\n",
       "      <td>-1.565370</td>\n",
       "      <td>-3.129004</td>\n",
       "      <td>-2.869916</td>\n",
       "      <td>-3.278097</td>\n",
       "      <td>-2.812875</td>\n",
       "      <td>-3.385493</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>-4.194174</td>\n",
       "      <td>-2.888140</td>\n",
       "      <td>-2.757312</td>\n",
       "      <td>-1.805784</td>\n",
       "      <td>-3.201123</td>\n",
       "      <td>-1.570182</td>\n",
       "      <td>-3.133929</td>\n",
       "      <td>-2.875131</td>\n",
       "      <td>-3.271234</td>\n",
       "      <td>-2.808876</td>\n",
       "      <td>-3.392241</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>-4.212748</td>\n",
       "      <td>-2.895315</td>\n",
       "      <td>-2.763297</td>\n",
       "      <td>-1.827215</td>\n",
       "      <td>-3.197741</td>\n",
       "      <td>-1.563233</td>\n",
       "      <td>-3.142026</td>\n",
       "      <td>-2.875109</td>\n",
       "      <td>-3.285369</td>\n",
       "      <td>-2.816735</td>\n",
       "      <td>-3.398949</td>\n",
       "      <td>24:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>-4.245491</td>\n",
       "      <td>-2.895845</td>\n",
       "      <td>-2.762500</td>\n",
       "      <td>-1.810633</td>\n",
       "      <td>-3.215881</td>\n",
       "      <td>-1.562743</td>\n",
       "      <td>-3.146116</td>\n",
       "      <td>-2.867844</td>\n",
       "      <td>-3.279886</td>\n",
       "      <td>-2.819339</td>\n",
       "      <td>-3.397554</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>-4.257604</td>\n",
       "      <td>-2.899872</td>\n",
       "      <td>-2.768243</td>\n",
       "      <td>-1.829076</td>\n",
       "      <td>-3.217714</td>\n",
       "      <td>-1.567502</td>\n",
       "      <td>-3.142104</td>\n",
       "      <td>-2.881188</td>\n",
       "      <td>-3.287208</td>\n",
       "      <td>-2.817704</td>\n",
       "      <td>-3.403449</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>-4.287841</td>\n",
       "      <td>-2.906291</td>\n",
       "      <td>-2.770930</td>\n",
       "      <td>-1.825716</td>\n",
       "      <td>-3.220191</td>\n",
       "      <td>-1.576225</td>\n",
       "      <td>-3.145858</td>\n",
       "      <td>-2.879891</td>\n",
       "      <td>-3.291909</td>\n",
       "      <td>-2.823257</td>\n",
       "      <td>-3.404394</td>\n",
       "      <td>24:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>-4.324856</td>\n",
       "      <td>-2.897858</td>\n",
       "      <td>-2.765199</td>\n",
       "      <td>-1.807590</td>\n",
       "      <td>-3.222612</td>\n",
       "      <td>-1.578731</td>\n",
       "      <td>-3.143227</td>\n",
       "      <td>-2.875151</td>\n",
       "      <td>-3.280234</td>\n",
       "      <td>-2.817383</td>\n",
       "      <td>-3.396667</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>-4.349561</td>\n",
       "      <td>-2.901417</td>\n",
       "      <td>-2.768836</td>\n",
       "      <td>-1.833489</td>\n",
       "      <td>-3.203151</td>\n",
       "      <td>-1.574273</td>\n",
       "      <td>-3.142594</td>\n",
       "      <td>-2.876581</td>\n",
       "      <td>-3.292863</td>\n",
       "      <td>-2.824514</td>\n",
       "      <td>-3.403223</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>-4.374071</td>\n",
       "      <td>-2.914406</td>\n",
       "      <td>-2.777554</td>\n",
       "      <td>-1.837358</td>\n",
       "      <td>-3.229682</td>\n",
       "      <td>-1.583502</td>\n",
       "      <td>-3.146631</td>\n",
       "      <td>-2.886320</td>\n",
       "      <td>-3.299308</td>\n",
       "      <td>-2.828188</td>\n",
       "      <td>-3.409442</td>\n",
       "      <td>24:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>-4.400084</td>\n",
       "      <td>-2.907985</td>\n",
       "      <td>-2.774410</td>\n",
       "      <td>-1.830227</td>\n",
       "      <td>-3.223995</td>\n",
       "      <td>-1.584337</td>\n",
       "      <td>-3.144791</td>\n",
       "      <td>-2.886171</td>\n",
       "      <td>-3.296440</td>\n",
       "      <td>-2.828013</td>\n",
       "      <td>-3.401305</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>-4.439099</td>\n",
       "      <td>-2.912474</td>\n",
       "      <td>-2.776679</td>\n",
       "      <td>-1.838349</td>\n",
       "      <td>-3.225408</td>\n",
       "      <td>-1.580165</td>\n",
       "      <td>-3.147600</td>\n",
       "      <td>-2.889801</td>\n",
       "      <td>-3.295571</td>\n",
       "      <td>-2.827890</td>\n",
       "      <td>-3.408645</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>-4.472975</td>\n",
       "      <td>-2.916737</td>\n",
       "      <td>-2.781339</td>\n",
       "      <td>-1.832901</td>\n",
       "      <td>-3.233186</td>\n",
       "      <td>-1.596438</td>\n",
       "      <td>-3.155297</td>\n",
       "      <td>-2.889158</td>\n",
       "      <td>-3.297359</td>\n",
       "      <td>-2.832763</td>\n",
       "      <td>-3.413608</td>\n",
       "      <td>24:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>-4.502078</td>\n",
       "      <td>-2.915583</td>\n",
       "      <td>-2.779507</td>\n",
       "      <td>-1.840847</td>\n",
       "      <td>-3.233021</td>\n",
       "      <td>-1.589512</td>\n",
       "      <td>-3.143378</td>\n",
       "      <td>-2.890112</td>\n",
       "      <td>-3.300222</td>\n",
       "      <td>-2.832315</td>\n",
       "      <td>-3.406648</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>-4.505734</td>\n",
       "      <td>-2.913046</td>\n",
       "      <td>-2.777517</td>\n",
       "      <td>-1.813284</td>\n",
       "      <td>-3.228627</td>\n",
       "      <td>-1.592906</td>\n",
       "      <td>-3.152945</td>\n",
       "      <td>-2.888261</td>\n",
       "      <td>-3.303355</td>\n",
       "      <td>-2.833150</td>\n",
       "      <td>-3.407605</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>-4.533860</td>\n",
       "      <td>-2.914223</td>\n",
       "      <td>-2.779168</td>\n",
       "      <td>-1.836353</td>\n",
       "      <td>-3.232304</td>\n",
       "      <td>-1.576981</td>\n",
       "      <td>-3.150037</td>\n",
       "      <td>-2.888388</td>\n",
       "      <td>-3.306311</td>\n",
       "      <td>-2.835337</td>\n",
       "      <td>-3.407633</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>-4.586464</td>\n",
       "      <td>-2.914975</td>\n",
       "      <td>-2.779347</td>\n",
       "      <td>-1.834861</td>\n",
       "      <td>-3.228456</td>\n",
       "      <td>-1.588842</td>\n",
       "      <td>-3.151539</td>\n",
       "      <td>-2.888778</td>\n",
       "      <td>-3.304706</td>\n",
       "      <td>-2.831374</td>\n",
       "      <td>-3.406220</td>\n",
       "      <td>24:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='952' class='' max='1730', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      55.03% [952/1730 12:41<10:22 -4.5998]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 50 with üëâüèªLMAEüëàüèª value: -1.782793402671814.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 51 with üëâüèªLMAEüëàüèª value: -1.8967092037200928.\n",
      "Better model found at epoch 53 with üëâüèªLMAEüëàüèª value: -1.972866415977478.\n",
      "Better model found at epoch 55 with üëâüèªLMAEüëàüèª value: -1.9742203950881958.\n",
      "Better model found at epoch 57 with üëâüèªLMAEüëàüèª value: -1.9888389110565186.\n",
      "Better model found at epoch 58 with üëâüèªLMAEüëàüèª value: -2.0073330402374268.\n",
      "Better model found at epoch 62 with üëâüèªLMAEüëàüèª value: -2.048238754272461.\n",
      "Better model found at epoch 66 with üëâüèªLMAEüëàüèª value: -2.0607099533081055.\n",
      "Better model found at epoch 71 with üëâüèªLMAEüëàüèª value: -2.0832114219665527.\n",
      "Better model found at epoch 79 with üëâüèªLMAEüëàüèª value: -2.1310529708862305.\n",
      "Better model found at epoch 81 with üëâüèªLMAEüëàüèª value: -2.1330249309539795.\n",
      "Better model found at epoch 84 with üëâüèªLMAEüëàüèª value: -2.1341171264648438.\n",
      "Better model found at epoch 85 with üëâüèªLMAEüëàüèª value: -2.1596245765686035.\n",
      "Better model found at epoch 89 with üëâüèªLMAEüëàüèª value: -2.167759895324707.\n",
      "Better model found at epoch 90 with üëâüèªLMAEüëàüèª value: -2.1866989135742188.\n",
      "Better model found at epoch 93 with üëâüèªLMAEüëàüèª value: -2.2128474712371826.\n",
      "Better model found at epoch 97 with üëâüèªLMAEüëàüèª value: -2.2286181449890137.\n",
      "Better model found at epoch 98 with üëâüèªLMAEüëàüèª value: -2.263288736343384.\n",
      "Better model found at epoch 106 with üëâüèªLMAEüëàüèª value: -2.2746660709381104.\n",
      "Better model found at epoch 107 with üëâüèªLMAEüëàüèª value: -2.28533935546875.\n",
      "Better model found at epoch 111 with üëâüèªLMAEüëàüèª value: -2.288078784942627.\n",
      "Better model found at epoch 112 with üëâüèªLMAEüëàüèª value: -2.3423428535461426.\n",
      "Better model found at epoch 121 with üëâüèªLMAEüëàüèª value: -2.3511505126953125.\n",
      "Better model found at epoch 126 with üëâüèªLMAEüëàüèª value: -2.3775625228881836.\n",
      "Better model found at epoch 127 with üëâüèªLMAEüëàüèª value: -2.4026384353637695.\n",
      "Better model found at epoch 132 with üëâüèªLMAEüëàüèª value: -2.41172456741333.\n",
      "Better model found at epoch 134 with üëâüèªLMAEüëàüèª value: -2.4215927124023438.\n",
      "Better model found at epoch 138 with üëâüèªLMAEüëàüèª value: -2.4381957054138184.\n",
      "Better model found at epoch 143 with üëâüèªLMAEüëàüèª value: -2.440781354904175.\n",
      "Better model found at epoch 145 with üëâüèªLMAEüëàüèª value: -2.476511001586914.\n",
      "Better model found at epoch 152 with üëâüèªLMAEüëàüèª value: -2.4860851764678955.\n",
      "Better model found at epoch 154 with üëâüèªLMAEüëàüèª value: -2.4873836040496826.\n",
      "Better model found at epoch 157 with üëâüèªLMAEüëàüèª value: -2.5124552249908447.\n",
      "Better model found at epoch 162 with üëâüèªLMAEüëàüèª value: -2.534111261367798.\n",
      "Better model found at epoch 169 with üëâüèªLMAEüëàüèª value: -2.5456161499023438.\n",
      "Better model found at epoch 172 with üëâüèªLMAEüëàüèª value: -2.5713624954223633.\n",
      "Better model found at epoch 176 with üëâüèªLMAEüëàüèª value: -2.576793670654297.\n",
      "Better model found at epoch 177 with üëâüèªLMAEüëàüèª value: -2.5773046016693115.\n",
      "Better model found at epoch 180 with üëâüèªLMAEüëàüèª value: -2.5925002098083496.\n",
      "Better model found at epoch 181 with üëâüèªLMAEüëàüèª value: -2.604959726333618.\n",
      "Better model found at epoch 183 with üëâüèªLMAEüëàüèª value: -2.6070075035095215.\n",
      "Better model found at epoch 185 with üëâüèªLMAEüëàüèª value: -2.6113672256469727.\n",
      "Better model found at epoch 187 with üëâüèªLMAEüëàüèª value: -2.616647481918335.\n",
      "Better model found at epoch 188 with üëâüèªLMAEüëàüèª value: -2.6183786392211914.\n",
      "Better model found at epoch 189 with üëâüèªLMAEüëàüèª value: -2.6402108669281006.\n",
      "Better model found at epoch 193 with üëâüèªLMAEüëàüèª value: -2.640812873840332.\n",
      "Better model found at epoch 197 with üëâüèªLMAEüëàüèª value: -2.654841423034668.\n",
      "Better model found at epoch 199 with üëâüèªLMAEüëàüèª value: -2.6656603813171387.\n",
      "Better model found at epoch 200 with üëâüèªLMAEüëàüèª value: -2.6659762859344482.\n",
      "Better model found at epoch 201 with üëâüèªLMAEüëàüèª value: -2.6734213829040527.\n",
      "Better model found at epoch 204 with üëâüèªLMAEüëàüèª value: -2.6813583374023438.\n",
      "Better model found at epoch 208 with üëâüèªLMAEüëàüèª value: -2.6824448108673096.\n",
      "Better model found at epoch 209 with üëâüèªLMAEüëàüèª value: -2.6910324096679688.\n",
      "Better model found at epoch 211 with üëâüèªLMAEüëàüèª value: -2.695868492126465.\n",
      "Better model found at epoch 212 with üëâüèªLMAEüëàüèª value: -2.7004387378692627.\n",
      "Better model found at epoch 213 with üëâüèªLMAEüëàüèª value: -2.708993434906006.\n",
      "Better model found at epoch 215 with üëâüèªLMAEüëàüèª value: -2.7096712589263916.\n",
      "Better model found at epoch 217 with üëâüèªLMAEüëàüèª value: -2.717693328857422.\n",
      "Better model found at epoch 220 with üëâüèªLMAEüëàüèª value: -2.7230756282806396.\n",
      "Better model found at epoch 222 with üëâüèªLMAEüëàüèª value: -2.7257754802703857.\n",
      "Better model found at epoch 223 with üëâüèªLMAEüëàüèª value: -2.7287404537200928.\n",
      "Better model found at epoch 224 with üëâüèªLMAEüëàüèª value: -2.731332302093506.\n",
      "Better model found at epoch 225 with üëâüèªLMAEüëàüèª value: -2.7416625022888184.\n",
      "Better model found at epoch 227 with üëâüèªLMAEüëàüèª value: -2.7446916103363037.\n",
      "Better model found at epoch 228 with üëâüèªLMAEüëàüèª value: -2.747905969619751.\n",
      "Better model found at epoch 231 with üëâüèªLMAEüëàüèª value: -2.7496542930603027.\n",
      "Better model found at epoch 232 with üëâüèªLMAEüëàüèª value: -2.7502059936523438.\n",
      "Better model found at epoch 233 with üëâüèªLMAEüëàüèª value: -2.7577767372131348.\n",
      "Better model found at epoch 237 with üëâüèªLMAEüëàüèª value: -2.7632970809936523.\n",
      "Better model found at epoch 239 with üëâüèªLMAEüëàüèª value: -2.768242835998535.\n",
      "Better model found at epoch 240 with üëâüèªLMAEüëàüèª value: -2.770930051803589.\n",
      "Better model found at epoch 243 with üëâüèªLMAEüëàüèª value: -2.7775537967681885.\n",
      "Better model found at epoch 246 with üëâüèªLMAEüëàüèª value: -2.781338930130005.\n"
     ]
    }
   ],
   "source": [
    "learner.fit_one_cycle(epochs,4e-4, pct_start=0.3, start_epoch=50)#,callbacks=gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.save(\"bestmodel_flatlineddd_dropout2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import warnings\n",
    "\n",
    "class Lookahead(torch.optim.Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Fine tune regular fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.callbacks.append(\n",
    "    ReduceLROnPlateauCallback(learner, monitor='train_loss', mode='min', factor=0.2, patience=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    base_opt\n",
    "except:\n",
    "    base_opt = learner.opt.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookahead = Lookahead(base_opt, k=5, alpha=0.5) # Initialize Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.opt = OptimWrapper(lookahead)\n",
    "learner.opt_func = lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(50, 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(50, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(50, 8e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 6e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 8e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.to_fp32()\n",
    "val = learner.validate()[1]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sub_fname = f'd_model{d_model}decoder_dropout{decoder_dropout}n_layers{n_layers}n_heads{n_heads}d_inner{d_inner}loss{learner.recorder.losses[-1]:.04f}val{val:.04f}'\n",
    "except:\n",
    "    sub_fname = f'd_model{d_model}val{val:.04f}'\n",
    "learner.save(sub_fname)\n",
    "print(sub_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Make sure `tranforms` are activated to test set otherwise TTA > 1 will be as TTA =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile  = np.load(fname_ext(test_fname, ext))\n",
    "    xt_xyz   = npzfile['x_xyz']\n",
    "    xt_type  = npzfile['x_type']\n",
    "    xt_ext   = npzfile['x_ext']\n",
    "    xt_atom  = npzfile['x_atom']\n",
    "    mt = npzfile['m']\n",
    "    xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    xt_xyz,xt_type,xt_ext,xt_atom,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index=types,ext=ext)\n",
    "    np.savez(fname_ext('_'+test_fname, ext), \n",
    "             x_xyz  = xt_xyz,\n",
    "             x_type = xt_type,\n",
    "             x_ext  = xt_ext,\n",
    "             x_atom = xt_atom,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt_qm9_mulliken = load_fn(f'xt_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[v.shape for v in [xt_xyz,xt_type,xt_ext,xt_atom, xt_qm9_mulliken,xt_ids, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.data.add_test(ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                              enumerate(zip(xt_xyz,xt_type,xt_ext,xt_atom,xt_qm9_mulliken)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTA_N = 1\n",
    "learner.data.test_ds.tfms = tta_tfms if TTA_N > 1 else tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = learner.model.module\n",
    "#data.batch_size = 4096*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = nn.DataParallel(learner.model)\n",
    "old_bs = data.batch_size\n",
    "data.batch_size *= 2\n",
    "\n",
    "sub = defaultdict(int)\n",
    "xt_ids_not_extended = (xt_ids!=0) & (xt_ids<=7163688) # TODO\n",
    "ids = xt_ids[xt_ids_not_extended]\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n",
    "        _, _, preds_,_,_,_ = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "        preds_ = preds_.sum(dim=1)\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "    preds = test_preds[xt_ids_not_extended]\n",
    "    for k in range(len(ids)):\n",
    "        sub[int(ids[k])] += preds[k]\n",
    "    \n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = sub[int(ids[k])]/TTA_N\n",
    "\n",
    "learner.model = learner.model.module\n",
    "data.batch_size = old_bs\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_fname = f'{sub_fname}_tta{TTA_N}.csv'\n",
    "sub_df.to_csv(csv_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = 'champs-scalar-coupling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c {comp} -f {csv_fname} -m 'QM9 tta {TTA_N} {ext}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "!kaggle competitions submissions -c {comp} -v > submissions-{comp}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(f'submissions-{comp}.csv')\n",
    "submissions.iloc[0].publicScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root)",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
